{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.set() # apply the seaborn defaults to plotted figures (e.g. theme, scaling, color palette), instead of matplotlib's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from time import time, strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "strftime('%X')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Fashion-MNIST dataset\n",
    "[Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) is a dataset of Zalando’s article images, consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28×28 grayscale image, associated with a label from 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** you should have the .csv files in the directory listed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that you have fashion-mnist_train.csv and fashion-mnist_test.csv (unzipped)\n",
    "!ls ../fashion-mnist-dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = os.path.join(\"..\", \"fashion-mnist-dataset\")\n",
    "\n",
    "# the datasets have the label on the first column and the image pixels on the following 784\n",
    "train_dataset_csv = pd.read_csv(os.path.join(PATH_TO_DATA, \"fashion-mnist_train.csv\"), delimiter=',')\n",
    "test_dataset_csv = pd.read_csv(os.path.join(PATH_TO_DATA, \"fashion-mnist_test.csv\"), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = IMG_HEIGHT = 28\n",
    "IMG_SHAPE = (IMG_HEIGHT, IMG_WIDTH) # grayscale images\n",
    "IMG_FLAT_SHAPE = (IMG_HEIGHT*IMG_WIDTH, )\n",
    "\n",
    "CLASS_NAME = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "              'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "CLASS_COUNT = len(CLASS_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train into train + validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed for splitting the data deterministically\n",
    "RANDOM_SEED = 886\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_dataset, validation_dataset = train_test_split(train_dataset_csv, train_size=0.8, shuffle=True, random_state=RANDOM_SEED)\n",
    "print(f\"train:      {train_dataset.shape}\") # 80%\n",
    "print(f\"validation: {validation_dataset.shape}\") # 20%\n",
    "\n",
    "test_dataset = test_dataset_csv\n",
    "print(f\"test:       {test_dataset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate labels and images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_train = train_dataset[\"label\"].values\n",
    "ys_val   = validation_dataset[\"label\"].values\n",
    "ys_test  = test_dataset[\"label\"].values\n",
    "\n",
    "print(ys_train.shape, \"min:\", ys_train.min(), \"max:\", ys_train.max())\n",
    "print(ys_val.shape,   \"min:\", ys_val.min(),   \"max:\", ys_val.max())\n",
    "print(ys_test.shape,  \"min:\", ys_test.min(),  \"max:\", ys_test.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_train = train_dataset.loc[:, train_dataset.columns != \"label\"].values\n",
    "xs_val   = validation_dataset.loc[:, validation_dataset.columns != \"label\"].values\n",
    "xs_test  = test_dataset.loc[:, test_dataset.columns != \"label\"].values\n",
    "\n",
    "print(xs_train.shape, \"min:\", xs_train.min(), \"max:\", xs_train.max())\n",
    "print(xs_val.shape,   \"min:\", xs_val.min(),   \"max:\", xs_val.max())\n",
    "print(xs_test.shape,  \"min:\", xs_test.min(),  \"max:\", xs_test.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def each_count(array):\n",
    "    count = np.bincount(array)\n",
    "    nonzero_count = np.nonzero(count)[0]\n",
    "    return zip(nonzero_count, count[nonzero_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count_train = list(each_count(ys_train))\n",
    "class_count_val   = list(each_count(ys_val))\n",
    "class_count_test  = list(each_count(ys_test))\n",
    "print(\"              Train         Validation    Test\")\n",
    "for label in range(CLASS_COUNT):\n",
    "    print(f\"{(CLASS_NAME[label] + ':'):<12}  \"\n",
    "          f\"{class_count_train[label][1]} ({class_count_train[label][1] / ys_train.shape[0]:.2f}%)  \"\n",
    "          f\"{class_count_val[label][1]} ({class_count_val[label][1] / ys_val.shape[0]:.2f}%)  \"\n",
    "          f\"{class_count_test[label][1]} ({class_count_test[label][1] / ys_test.shape[0]:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize images and one-hot encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_train = xs_train.astype('float32') / 255.0\n",
    "xs_val = xs_val.astype('float32') / 255.0\n",
    "xs_test = xs_test.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's forget about the test set for now..** just assume it doesn't exist (but don't worry, we'll come back to it at the end)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"xs_train: mean={xs_train.mean():>8.4f}, stddev={xs_train.std():>7.4f}\")\n",
    "\n",
    "# NOTE that we must use stats from train data to normalize the val and test sets aswell\n",
    "mean, std = xs_train.mean(), xs_train.std()\n",
    "X_train = (xs_train - mean) / std\n",
    "\n",
    "print(f\"X_train:  mean={X_train.mean():>8.4f}, stddev={X_train.std():>7.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = (xs_val - mean) / std\n",
    "print(f\"xs_val: mean={xs_val.mean():>8.4f}, stddev={xs_val.std():>7.4f}\")\n",
    "print(f\"X_val:  mean={X_val.mean():>8.4f}, stddev={X_val.std():>7.4f}\") # mean should be close to 0 and stddev close to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode(ys):\n",
    "    n_examples, *_ = ys.shape\n",
    "    onehot = np.zeros(shape=(n_examples, CLASS_COUNT))\n",
    "    onehot[np.arange(n_examples), ys] = 1\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = onehot_encode(ys_train)\n",
    "print(ys_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_val = onehot_encode(ys_val)\n",
    "print(ys_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, model=None, n_epochs=None, batch_size=None, title_prefix=None):\n",
    "    plot_title = (\"\" if title_prefix is None else (title_prefix + \"\\n\")) + \"Train history\"\n",
    "    info = []\n",
    "    if n_epochs is not None:\n",
    "        info.append(f\"{n_epochs} epochs\")\n",
    "    if batch_size is not None:\n",
    "        info.append(f\"{batch_size} batch size\")\n",
    "    if model is not None:\n",
    "        info.append(f\"{model.optimizer.get_config()['name']} optimizer\")\n",
    "    if len(info) > 0:\n",
    "        plot_title += f\" ({', '.join(info)})\"\n",
    "    \n",
    "    # leave only \"loss\" and \"val_loss\" for plotting\n",
    "    loss_data = pd.DataFrame({k: v for k, v in history.items() if (k == \"loss\" or k == \"val_loss\")})\n",
    "    loss_data.index += 1 # start counting the epochs at 1, not 0\n",
    "    lineplot = sns.lineplot(data=loss_data)\n",
    "    lineplot.set_title(plot_title)\n",
    "    lineplot.set_xlabel(\"epoch\")\n",
    "    lineplot.set_ylabel(\"Loss\" + f\" ({model.loss})\" if model is not None else \"\")\n",
    "    plt.show()\n",
    "\n",
    "    # leave only \"accuracy\" and \"val_accuracy\" for plotting\n",
    "    acc_data = pd.DataFrame({k: v for k, v in history.items() if (k == \"accuracy\" or k == \"val_accuracy\")})\n",
    "    acc_data.index += 1 # start counting the epochs at 1, not 0\n",
    "    lineplot = sns.lineplot(data=acc_data)\n",
    "    lineplot.set_title(plot_title)\n",
    "    lineplot.set_xlabel(\"epoch\")\n",
    "    lineplot.set_ylabel(\"Accuracy\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, Y_train, X_val, Y_val, n_epochs=64, batch_size=1024, \n",
    "          min_delta=0.003, patience=3, early_stopping=True, plot_title=None, plot=True):\n",
    "    best_model_path = os.path.join(\"models\", f\"best_model_{model.name}.h5\")\n",
    "    if not os.path.exists(os.path.dirname(best_model_path)):\n",
    "        os.makedirs(os.path.dirname(best_model_path))\n",
    "    \n",
    "    callbacks = [keras.callbacks.ModelCheckpoint(monitor='val_loss', filepath=best_model_path, save_best_only=True)]\n",
    "    if early_stopping:\n",
    "        callbacks.append(keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=min_delta, patience=patience))\n",
    "    \n",
    "    print(\"Starting to train...\")\n",
    "    start = time()\n",
    "    hist = model.fit(\n",
    "        x=X_train, y=Y_train,\n",
    "        validation_data=(X_val, Y_val),\n",
    "        epochs=n_epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks\n",
    "    ).history\n",
    "    end = time()\n",
    "    print(f\"\\nDone.\\nTraining took {(end - start):.2f}s\")\n",
    "    \n",
    "    if os.path.exists(best_model_path):\n",
    "        model = keras.models.load_model(best_model_path)\n",
    "    \n",
    "    if plot:\n",
    "        plot_history(hist, model, n_epochs, batch_size, plot_title)\n",
    "    \n",
    "    return hist, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_with_prediction(xs, ys, ys_pred, num_rows=5, num_cols=3):\n",
    "    # ref.: https://www.tensorflow.org/tutorials/keras/classification\n",
    "    \n",
    "    def plot_image(i, predictions_array, true_label, img):\n",
    "        true_label, img = true_label[i], img[i]\n",
    "        plt.grid(False)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.imshow(img.reshape(IMG_SHAPE), cmap=plt.cm.binary)\n",
    "        predicted_label = np.argmax(predictions_array)\n",
    "        color = 'blue' if predicted_label == true_label else 'red'\n",
    "        plt.xlabel(\"{} {:2.0f}% ({})\".format(CLASS_NAME[predicted_label],\n",
    "                                             100*np.max(predictions_array),\n",
    "                                             CLASS_NAME[true_label]),\n",
    "                                             color=color)\n",
    "\n",
    "    def plot_value_array(i, predictions_array, true_label):\n",
    "        true_label = true_label[i]\n",
    "        plt.grid(False)\n",
    "        plt.xticks(range(10))\n",
    "        plt.yticks([])\n",
    "        thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
    "        plt.ylim([0, 1])\n",
    "        predicted_label = np.argmax(predictions_array)\n",
    "        thisplot[predicted_label].set_color('red')\n",
    "        thisplot[true_label].set_color('blue')\n",
    "    \n",
    "    # plot the first few test images, their predicted labels, and the true labels\n",
    "    # color correct predictions in blue and incorrect predictions in red    \n",
    "    num_images = num_rows * num_cols\n",
    "    plt.figure(figsize=(4 * num_cols, 2 * num_rows))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(num_rows, 2 * num_cols, 2 * i + 1)\n",
    "        plot_image(i, ys_pred[i], ys, xs)\n",
    "        plt.subplot(num_rows, 2 * num_cols, 2 * i + 2)\n",
    "        plot_value_array(i, ys_pred[i], ys)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cm(ys, ys_pred, class_names, figsize=(10, 9), fontsize=12):\n",
    "    #cm = confusion_matrix(ys, ys_pred)\n",
    "    #df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "    cm = np.flip(confusion_matrix(ys, ys_pred), axis=0)\n",
    "    df_cm = pd.DataFrame(cm, index=class_names[::-1], columns=class_names)\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    heatmap.set_ylim(*heatmap.get_xlim())\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baseline\n",
    "Explore Neural Networks with Fashion-MNIST. What is the accuracy? Describe your Neural Network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Reshape\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    Input(IMG_FLAT_SHAPE), # 28 * 28 = 784 pixels\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax', kernel_regularizer=l2(0.001))\n",
    "], \"MLP_baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE use 'categorical_crossentropy' since `labels` is a one-hot encoded vector\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "strftime('%X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history, best_model = train(model, X_train, Y_train, X_val, Y_val, \n",
    "                            n_epochs=64, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "strftime('%X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y_val_pred = best_model.predict(X_val)\n",
    "plot_image_with_prediction(X_val, ys_val, Y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_cm(ys_val, np.argmax(Y_val_pred, axis=1), CLASS_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs.: padding='valid' by default (i.e. no padding)\n",
    "conv_model = keras.Sequential([\n",
    "    Reshape((28, 28, 1), input_shape=(784,)),\n",
    "    Conv2D(32, (5, 5), activation='relu', padding='same', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D((2, 2)), # 28x28 -> 14x14 feature map\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2, 2)), # 14x14 -> 7x7 feature map\n",
    "    Flatten(), # 7*7*64 = 3136\n",
    "    Dense(10, activation='softmax', kernel_regularizer=l2(0.001))\n",
    "], \"CNN_baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model.compile(optimizer='adam',\n",
    "                   loss='categorical_crossentropy',\n",
    "                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "conv_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conv_model.optimizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "strftime('%X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "conv_history, best_conv_model = train(conv_model, X_train, Y_train, X_val, Y_val, \n",
    "                                      n_epochs=64, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "strftime('%X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_val_conv_pred = best_conv_model.predict(X_val)\n",
    "plot_image_with_prediction(X_val, ys_val, Y_val_conv_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_cm(ys_val, np.argmax(Y_val_conv_pred, axis=1), CLASS_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. CNN with batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME (check the confusion matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout, BatchNormalization, ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs.: padding='valid' by default (i.e. no padding)\n",
    "conv_v2_model = keras.Sequential([\n",
    "    Reshape((28, 28, 1), input_shape=(784,)),\n",
    "    Conv2D(32, (5, 5), padding='same', input_shape=(28, 28, 1)),\n",
    "    ReLU(),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)), # 28x28 -> 14x14 feature map\n",
    "    Conv2D(64, (3, 3), padding='same'),\n",
    "    ReLU(),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)), # 14x14 -> 7x7 feature map\n",
    "    Flatten(), # 7*7*64 = 3136\n",
    "    Dense(10, activation='softmax', kernel_regularizer=l2(0.001))\n",
    "], \"CNN_v2_baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_v2_model.compile(optimizer='adam',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_v2_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_v2_model.optimizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "strftime('%X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "conv_v2_history, best_conv_v2_model = train(conv_v2_model, X_train, Y_train, X_val, Y_val, \n",
    "                                            n_epochs=64, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "strftime('%X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_val_conv_v2_pred = best_conv_v2_model.predict(X_val)\n",
    "plot_image_with_prediction(X_val, ys_val, Y_val_conv_v2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_cm(ys_val, np.argmax(Y_val_conv_v2_pred, axis=1), CLASS_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using PCA\n",
    "Re-do the ﬁrst experiment considering the PCA dimensionality reduction. Consider three diﬀerent energies (variance) for reducing the image dimensionality. What are the conclusions when using PCA in this problem? Does the accuracy improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_info(array, name):\n",
    "    print(f\"{name+':':<9} {array.dtype.name:<7} {str(array.shape):<12} \"\n",
    "          f\"| min: {array.min():>5.2f} | max: {array.max():>5.2f} \"\n",
    "          f\"| mean: {array.mean():>5.3f} | std: {array.std():>5.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_info(X_train, \"X_train\")\n",
    "print_info(xs_train, \"xs_train\")\n",
    "print_info(Y_train, \"Y_train\")\n",
    "print_info(ys_train, \"ys_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_As we'll be reducing the amount of features (dimensions) of our input data after applying PCA, it makes more sense to treat it as a vector, instead of as an image.\n",
    "Thus, we'll use our MLP baseline model instead of our CNN, even though the latter showed better results._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(min_explained_variance, model_name):\n",
    "    pca = PCA(min_explained_variance)\n",
    "    pca.fit(X_train) # fit on train data (and then use it to transform both train and validation)\n",
    "    print(pca)\n",
    "    print()\n",
    "    \n",
    "    explained_variance = pca.explained_variance_ratio_.sum()\n",
    "    print(f\"{pca.n_components_} principal components → {explained_variance} explained variance\")\n",
    "    print(f\"{pca.n_components_} / {X_train.shape[1]} = {(100 * pca.n_components_ / X_train.shape[1]):.2f}% of components used\")\n",
    "    print()\n",
    "    \n",
    "    model_pca = keras.Sequential([\n",
    "        Input(shape=(pca.n_components_, )), # number of components used to keep min_explained_variance\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(10, activation='softmax', kernel_regularizer=l2(0.001))\n",
    "    ], model_name)\n",
    "    \n",
    "    model_pca.compile(optimizer='adam',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "    \n",
    "    print(model_pca.summary())\n",
    "    print()\n",
    "    \n",
    "    history_pca, best_model_pca = train(model_pca, \n",
    "                                        pca.transform(X_train), Y_train, \n",
    "                                        pca.transform(X_val), Y_val, \n",
    "                                        n_epochs=64, batch_size=512)\n",
    "    \n",
    "    # using ys_val below, instead of np.argmax(Y_val, axis=1), simply for convinence\n",
    "    \n",
    "    Y_val_pca_pred = best_model_pca.predict(pca.transform(X_val))\n",
    "    plot_image_with_prediction(X_val, ys_val, Y_val_pca_pred)\n",
    "    \n",
    "    print_cm(ys_val, np.argmax(Y_val_pca_pred, axis=1), CLASS_NAME)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pca_model(min_explained_variance, model_name):\n",
    "    pca = PCA(min_explained_variance)\n",
    "    pca.fit(X_train) # fit on train data (and then use it to transform both train and validation)\n",
    "    print(pca)\n",
    "    print()\n",
    "    \n",
    "    explained_variance = pca.explained_variance_ratio_.sum()\n",
    "    print(f\"{pca.n_components_} principal components → {explained_variance} explained variance\")\n",
    "    print(f\"{pca.n_components_} / {X_train.shape[1]} = {(100 * pca.n_components_ / X_train.shape[1]):.2f}% of components used\")\n",
    "    \n",
    "    model_pca = keras.Sequential([\n",
    "        Input(shape=(pca.n_components_, )), # number of components used to keep min_explained_variance\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(10, activation='softmax', kernel_regularizer=l2(0.001))\n",
    "    ], model_name)\n",
    "    \n",
    "    model_pca.compile(optimizer='adam',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "    \n",
    "    return model_pca, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pca_model(model_pca, pca):\n",
    "    history_pca, best_model_pca = train(model_pca, \n",
    "                                        pca.transform(X_train), Y_train, \n",
    "                                        pca.transform(X_val), Y_val, \n",
    "                                        n_epochs=64, batch_size=512)\n",
    "\n",
    "    # using ys_val below, instead of np.argmax(Y_val, axis=1), simply for convinence\n",
    "\n",
    "    Y_val_pca_pred = best_model_pca.predict(pca.transform(X_val))\n",
    "    plot_image_with_prediction(X_val, ys_val, Y_val_pca_pred)\n",
    "\n",
    "    print_cm(ys_val, np.argmax(Y_val_pca_pred, axis=1), CLASS_NAME)\n",
    "    \n",
    "    return Y_val_pca_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. 99% explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_99var, pca_99var = build_pca_model(min_explained_variance=0.99, model_name=\"MLP_99var\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y_val_99var_pred = train_pca_model(model_99var, pca_99var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. 95% explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_95var, pca_95var = build_pca_model(min_explained_variance=0.95, model_name=\"MLP_95var\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y_val_95var_pred = train_pca_model(model_95var, pca_95var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. 90% explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_90var, pca_90var = build_pca_model(min_explained_variance=0.90, model_name=\"MLP_90var\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_val_90var_pred = train_pca_model(model_90var, pca_90var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. F1-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_f1_scores(ys, ys_pred, class_name_dict=CLASS_NAME):\n",
    "    # ref.: https://github.com/erikperillo/ml-2sem2019/\n",
    "    def get_metrics(conf_mtx):\n",
    "        metrics = []\n",
    "        mtx_sum = conf_mtx.sum()\n",
    "        for i in range(conf_mtx.shape[0]):\n",
    "            col_sum = conf_mtx[:, i].sum()\n",
    "            row_sum = conf_mtx[i].sum()        \n",
    "            tp = conf_mtx[i][i]\n",
    "            fp = col_sum - tp\n",
    "            tn = mtx_sum - (row_sum + col_sum - tp)\n",
    "            fn = row_sum - tp\n",
    "            metrics.append((tp, fp, tn, fn))\n",
    "        return metrics\n",
    "    def get_f1_score(tp, fp, tn, fn, eps=1e-18):\n",
    "        precision = tp/max(tp + fp, eps)\n",
    "        recall = tp/max(tp + fn, eps)\n",
    "        f1 = 2*precision*recall/(precision + recall)\n",
    "        return f1\n",
    "    def get_f1_scores(conf_mtx):\n",
    "        metrics = get_metrics(conf_mtx)\n",
    "        return [get_f1_score(*m) for m in metrics]\n",
    "    def harmonic_mean_of_positive_values(values, eps=1e-18):\n",
    "        return len(values)/sum(1/max(v, eps) for v in values)\n",
    "    \n",
    "    cm = confusion_matrix(ys, ys_pred)\n",
    "    f1_scores = get_f1_scores(cm)\n",
    "    print(\"F1 scores:\")\n",
    "    for i in range(cm.shape[0]):\n",
    "        print(f\"Class {i} ({class_name_dict[i]}): {f1_scores[i]:.3f}\")\n",
    "    print()\n",
    "    print(f\"Combined F1 score: {harmonic_mean_of_positive_values(f1_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP baseline\n",
    "print_f1_scores(ys_val, np.argmax(Y_val_pred, axis=1), CLASS_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN baseline\n",
    "print_f1_scores(ys_val, np.argmax(Y_val_conv_pred, axis=1), CLASS_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 99% explained variance\n",
    "print_f1_scores(ys_val, np.argmax(Y_val_99var_pred, axis=1), CLASS_NAME) # FIXME why is this worse than 95% ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95% explained variance\n",
    "print_f1_scores(ys_val, np.argmax(Y_val_95var_pred, axis=1), CLASS_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 90% explained variance\n",
    "print_f1_scores(ys_val, np.argmax(Y_val_90var_pred, axis=1), CLASS_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. First 3 principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_3pc = PCA(n_components=3) # min_explained_variance_if_gt_0_and_lt_1_else_n_of_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_3pc_result = pca_3pc.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Explained variance per principal component:\")\n",
    "for i, (var, cumsum) in enumerate(zip(pca_3pc.explained_variance_ratio_, np.cumsum(pca_3pc.explained_variance_ratio_))):\n",
    "    print(f\"PC {i+1}: {var:.4f} (cumulative sum: {cumsum:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "sns.scatterplot(\n",
    "    x=pca_3pc_result[..., 0], # PC 1\n",
    "    y=pca_3pc_result[..., 1], # PC 2\n",
    "    hue=\"label\",\n",
    "    palette=sns.color_palette(\"hls\", 10),\n",
    "    data=train_dataset,\n",
    "    legend=\"full\",\n",
    "    alpha=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run '%matplotlib notebook' for an interactive (but laggy) visualization\n",
    "ax = plt.figure(figsize=(16, 10)).gca(projection='3d')\n",
    "ax.scatter(\n",
    "    xs=pca_3pc_result[..., 0], # PC 1\n",
    "    ys=pca_3pc_result[..., 1], # PC 2\n",
    "    zs=pca_3pc_result[..., 2], # PC 3\n",
    "    c=ys_train, \n",
    "    cmap='tab10'\n",
    ")\n",
    "ax.set_xlabel('pca-one')\n",
    "ax.set_ylabel('pca-two')\n",
    "ax.set_zlabel('pca-three')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. 100% variance explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO only apply PCA but don't reduce dimensionality (this way we can test the CNN model aswell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Autoencoders\n",
    "Re-do the ﬁrst experiment considering Autoencoders for reducing the image dimensionality. Consider two diﬀerent latent vector size for reducing the image dimensionality. What are the conclusions when using Autoencoders in this problem? Does accuracy improve?\n",
    "\n",
    "Autoencoders are a branch of neural network which attempt to compress the information of the input variables into a reduced dimensional space and then recreate the input data set. Typically the autoencoder is trained over a number of iterations using gradient descent, minimizing the mean squared error. The key component is the “bottleneck” hidden layer. This is where the information from the input has been compressed. By extracting this layer from the model, each node can now be treated as a variable in the same way each chosen principal component is used as a variable in following models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using clustering techniques\n",
    "Explore two clustering algorithms using the reduced features (PCA or Autoencoders). Do the clusters make sense? Check the validity/quality of your clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
