{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Atari.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laurelkeys/machine-learning/blob/master/assignment-4/Atari.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oG8Y52Lv4z1F"
      },
      "source": [
        "# Install dependencies for video displaying\n",
        "_obs.:_ This may require a runtime restart"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I7PJOBGVXFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get update                               > /dev/null 2>&1\n",
        "!apt-get install cmake                        > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get install x11-utils                    > /dev/null 2>&1 # fixes 'xdpyinfo was not found' when importing Display from pyvirtualdisplay\n",
        "\n",
        "# !pip install --upgrade setuptools 2>&1\n",
        "!pip install gym[atari] pyvirtualdisplay ez_setup > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOJnuONdvopa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Colab has stable-baselines version 2.2.1, but the 'reset_num_timesteps' argument of model.learn() was introduced in v2.4.1\n",
        "!pip install --upgrade stable-baselines > /dev/null 2>&1\n",
        "!pip list | grep stable-baselines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JGh7DuPB4z1L"
      },
      "source": [
        "# Mount Drive\n",
        "Remeber to mount your drive on your academic account for more storage ;)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRIt1QEXaEx6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Audio, display\n",
        "display(Audio(url='https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav', autoplay=True))\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prXRYYviFKJk",
        "colab_type": "text"
      },
      "source": [
        "# Variables to set\n",
        "> `ALGORITHM` : [RL Algorithms](https://stable-baselines.readthedocs.io/en/master/guide/algos.html) | `POLICY` : [Policy Networks](https://stable-baselines.readthedocs.io/en/master/modules/policies.html) | `ENVIRONMENT`: [Table of environments](https://github.com/openai/gym/wiki/Table-of-environments) \\( [Atari Environments](https://gym.openai.com/envs/#atari) \\)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFDQhtBZFIOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://stable-baselines.readthedocs.io/en/master/guide/algos.html\n",
        "ALGORITHM = \"PPO2\"\n",
        "POLICY = \"CnnPolicy\"\n",
        "\n",
        "# https://gym.openai.com/envs/#atari\n",
        "ENVIRONMENT = \"PongNoFrameskip-v4\"\n",
        "\n",
        "# NOTE we're not using RAM envs, but if you do, remember not to use a CNN policy ;)\n",
        "# also, you probably should be using the NoFrameskip-v4 environments (instead of the -v0 or -v4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hdPOQmJD4z1Q"
      },
      "source": [
        "**Remeber** to:\n",
        "* change the `ENVIRONMENT`\n",
        "* set `INITIAL_EPOCH_` if `CONTINUE_TRAINING = True`\n",
        "* set `MAX_EPOCHS` if `TRAIN_FOREVER = False`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ7Qa6BsFgBu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CONTINUE_TRAINING = False  # Set this if want to continue training from a saved model, and don't forget to set INITIAL_EPOCH_\n",
        "TRAIN_FOREVER = True       # Set this if you want to train until the notebook disconnects\n",
        "\n",
        "INITIAL_EPOCH_ = 0         # If CONTINUE_TRAINING set this to use the model trained for this many epochs with the other set parameters\n",
        "EPOCHS = 25000             # Number of epochs between model saving (checkpoint)\n",
        "MAX_EPOCHS = None          # If not TRAIN_FOREVER set this to halt after this many epochs (the value of INITIAL_EPOCH_ doesn't interfere)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiUQSHnCaqzT",
        "colab_type": "text"
      },
      "source": [
        "Please don't forget to **mount your drive**, otherwise it will wait forever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNfIP1VBFTaU",
        "colab_type": "text"
      },
      "source": [
        "# Import libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCPb9PsLZbEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Stable baselines only support TF 1.x for now ####\n",
        "try:\n",
        "    # Colab only\n",
        "    # %tensorflow_version 2.x\n",
        "    %tensorflow_version 1.x\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL8eYtmNZxld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import stable_baselines\n",
        "\n",
        "from stable_baselines.common.vec_env import DummyVecEnv, SubprocVecEnv, VecFrameStack, VecVideoRecorder\n",
        "from stable_baselines.bench import Monitor as SBMonitor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0i0LUUNZ05V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "from time import time, strftime, sleep"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGv0nTbMaDhR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "strftime(\"%X\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dinyDs4ud-Bv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "PATH_TO_DATA = os.path.join(\"drive\", \"My Drive\", \"unicamp\", \"MC886\", \"atari\")\n",
        "os.makedirs(PATH_TO_DATA, exist_ok=True)\n",
        "\n",
        "!ls drive/My\\ Drive/unicamp/MC886/atari/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRDKKNj6Ie2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH_PREFIX = os.path.join(PATH_TO_DATA, ENVIRONMENT)\n",
        "os.makedirs(PATH_PREFIX, exist_ok=True)\n",
        "\n",
        "FILE_NAME_ = f\"{ALGORITHM}_{POLICY}_ep{INITIAL_EPOCH_ + EPOCHS}\"\n",
        "\n",
        "SAVE_PATH_ = os.path.join(PATH_PREFIX, FILE_NAME_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4aiKF3WELp8",
        "colab_type": "text"
      },
      "source": [
        "## Setup for video displaying\n",
        "[Rendering OpenAi Gym in Google Colaboratory](https://star-ai.github.io/Rendering-OpenAi-Gym-in-Colaboratory/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs5CDXLJEUot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym.wrappers import Monitor as GymMonitor\n",
        "from gym import logger as gymlogger\n",
        "gymlogger.set_level(40) # error only\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import math, glob, io, base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(210, 160)) # images from Atari are 210x160 RGB\n",
        "#display = Display(visible=0, size=(640, 480))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bm8SO4MEccv",
        "colab_type": "text"
      },
      "source": [
        "# Functions to replay videos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q2kRFavEU6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH_VIDEO = os.path.join(PATH_PREFIX, \"video\")\n",
        "os.makedirs(PATH_VIDEO, exist_ok=True)\n",
        "\n",
        "# Create log dir\n",
        "LOG_PATH = \"tmp/\"\n",
        "os.makedirs(LOG_PATH, exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g1Z5irLEU4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_video(video_folder=PATH_VIDEO):\n",
        "    mp4list = glob.glob(video_folder + '/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = max(mp4list, key=os.path.getctime) # get the latest file\n",
        "        video = io.open(mp4, 'rb').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" \n",
        "                    controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "                  </video>'''.format(encoded.decode())))\n",
        "    else: \n",
        "        print(f\"Could not find any videos on folder '{video_folder}/*.mp4'\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AisZAxsFElJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VIDEO_LENGTH = 1000\n",
        "\n",
        "def wrap_env(env, video_name_prefix=None):\n",
        "    env = VecVideoRecorder(env, \n",
        "                           video_folder=PATH_VIDEO, \n",
        "                           record_video_trigger=lambda curr_step: curr_step == 0, # defines when to start recording\n",
        "                           video_length=VIDEO_LENGTH,\n",
        "                           name_prefix=f\"random-agent-{ENVIRONMENT}\" if video_name_prefix is None else video_name_prefix)\n",
        "    return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQBvHmjrGZqV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_and_show(model, env, show_info=False):\n",
        "    observation = env.reset()\n",
        "\n",
        "    steps = 0\n",
        "    while True:\n",
        "        env.render()\n",
        "        action = model.predict(observation)\n",
        "        observation, reward, done, info = env.step(action)\n",
        "        steps += 1\n",
        "        if show_info:\n",
        "            print(f\"[step {steps}] reward: {reward} | info: {info}\", end=\"\\r\")\n",
        "        if done:\n",
        "            print(f\"Done after {steps} steps\")\n",
        "            break\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    sleep(1)\n",
        "    show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqhGDS2Sy8Fp",
        "colab_type": "text"
      },
      "source": [
        "## Callback for model.learn()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi8bQmuXy-Q0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import traceback\n",
        "from stable_baselines.results_plotter import load_results, ts2xy\n",
        "from stable_baselines import results_plotter\n",
        "\n",
        "BEST_MEAN_REWARD_, N_STEPS_ = -np.inf, 0 # Copied code, pls don't judge\n",
        "\n",
        "def callback(_locals, _globals):\n",
        "    \"\"\"\n",
        "    Callback called at each step (for DQN and others) or after n steps (see ACER or PPO2)\n",
        "    :param _locals: (dict)\n",
        "    :param _globals: (dict)\n",
        "    \"\"\"\n",
        "    global N_STEPS_, BEST_MEAN_REWARD_\n",
        "    # Print stats every 1000 calls\n",
        "    if (N_STEPS_ + 1) % 1000 == 0:\n",
        "        # Evaluate policy training performance\n",
        "        try:\n",
        "            x, y = ts2xy(load_results(LOG_PATH), 'timesteps')\n",
        "            if len(x) > 0:\n",
        "                mean_reward = np.mean(y[-100:])\n",
        "                print(x[-1], 'timesteps')\n",
        "                print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(BEST_MEAN_REWARD_, mean_reward))\n",
        "\n",
        "                # New best model, you could save the agent here\n",
        "                if mean_reward >= BEST_MEAN_REWARD_:\n",
        "                    BEST_MEAN_REWARD_ = mean_reward\n",
        "                    # Example for saving best model\n",
        "                    print(f\"Saving new best model (at N_STEPS_={N_STEPS_})\")\n",
        "                    _locals['self'].save(os.path.join(LOG_PATH, 'best_model.pkl'))\n",
        "\n",
        "                print(strftime(\"%X\"))\n",
        "        except Exception as e:\n",
        "            print(f\"Exception raised:\")\n",
        "            traceback.print_exception(type(e), e, e.__traceback__)\n",
        "    N_STEPS_ += 1\n",
        "    return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOE-rQCAaGEv",
        "colab_type": "text"
      },
      "source": [
        "# Trem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5IFuMJDy3dt",
        "colab_type": "text"
      },
      "source": [
        "## Copying `make_atari_env` code to avoid error with latest `gym version`\n",
        "[Issue #51](https://github.com/araffin/rl-baselines-zoo/issues/51)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y__JXMp8x6Yv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from stable_baselines.common.atari_wrappers import make_atari, wrap_deepmind\n",
        "from stable_baselines.common import set_global_seeds\n",
        "from stable_baselines import logger\n",
        "\n",
        "def make_atari_env(env_id, num_env, seed, wrapper_kwargs=None,\n",
        "                   start_index=0, allow_early_resets=True, start_method=None):\n",
        "    \"\"\"\n",
        "    Create a wrapped, monitored SubprocVecEnv for Atari.\n",
        "\n",
        "    :param env_id: (str) the environment ID\n",
        "    :param num_env: (int) the number of environment you wish to have in subprocesses\n",
        "    :param seed: (int) the inital seed for RNG\n",
        "    :param wrapper_kwargs: (dict) the parameters for wrap_deepmind function\n",
        "    :param start_index: (int) start rank index\n",
        "    :param allow_early_resets: (bool) allows early reset of the environment\n",
        "    :return: (Gym Environment) The atari environment\n",
        "    :param start_method: (str) method used to start the subprocesses.\n",
        "        See SubprocVecEnv doc for more information\n",
        "    \"\"\"\n",
        "    if wrapper_kwargs is None:\n",
        "        wrapper_kwargs = {}\n",
        "\n",
        "    def make_env(rank):\n",
        "        def _thunk():\n",
        "            env = make_atari(env_id)\n",
        "            env.seed(seed + rank)\n",
        "            env = SBMonitor(env, logger.get_dir() and os.path.join(logger.get_dir(), str(rank)),\n",
        "                            allow_early_resets=allow_early_resets)\n",
        "            return wrap_deepmind(env, **wrapper_kwargs)\n",
        "        return _thunk\n",
        "    set_global_seeds(seed)\n",
        "\n",
        "    # When using one environment, no need to start subprocesses\n",
        "    if num_env == 1:\n",
        "        return DummyVecEnv([make_env(0)])\n",
        "\n",
        "    return SubprocVecEnv([make_env(i + start_index) for i in range(num_env)],\n",
        "                         start_method=start_method)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oeUambAGF1n",
        "colab_type": "text"
      },
      "source": [
        "## Model setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "72G5nvRZ4z1v",
        "colab": {}
      },
      "source": [
        "def make_env():    \n",
        "    env = make_atari_env(env_id=ENVIRONMENT, num_env=1, seed=0)\n",
        "    env = VecFrameStack(env, n_stack=4) # Frame-stacking with 4 frames\n",
        "    return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAZNOhIifzkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = make_env()\n",
        "\n",
        "# Wrapped env for recording\n",
        "env_record = wrap_env(make_env(), video_name_prefix=FILE_NAME_)\n",
        "\n",
        "# check out the action space, if both aren't the same something will probably go wrong\n",
        "envs_are_eq = env.action_space == env_record.action_space\n",
        "print(f\"{env.action_space} == {env_record.action_space}? {envs_are_eq}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dMbhpDub4z1y",
        "colab": {}
      },
      "source": [
        "from stable_baselines import PPO2, ACER, ACKTR\n",
        "IMPL = {\n",
        "    'PPO2': PPO2,\n",
        "    'ACER': ACER,\n",
        "    'ACKTR': ACKTR,\n",
        "}\n",
        "VERBOSITY = 0 # 0 none, 1 training information, 2 tensorflow debug"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vytZYM62sgJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "PATH_SAVED = os.path.join(PATH_TO_DATA, ENVIRONMENT, f\"{ALGORITHM}_{POLICY}_ep{INITIAL_EPOCH_}.pkl\")\n",
        "\n",
        "if CONTINUE_TRAINING and os.path.exists(PATH_SAVED):\n",
        "    model = IMPL[ALGORITHM].load(PATH_SAVED, env=env, verbose=VERBOSITY) # Load the trained agent\n",
        "    clear_output() # clear tf warnings\n",
        "    print(\"Model loaded from:\", PATH_SAVED)\n",
        "else:\n",
        "    model = (IMPL[ALGORITHM](POLICY, env, verbose=VERBOSITY) if ALGORITHM != 'PPO2'\n",
        "        else IMPL[ALGORITHM](POLICY, env, verbose=VERBOSITY, nminibatches=1))\n",
        "    clear_output() # clear tf warnings\n",
        "    if CONTINUE_TRAINING:\n",
        "        print(\"Model not found, training from scratch\")\n",
        "    else:\n",
        "        print(\"obs.: CONTINUE_TRAINING is set to False\")\n",
        "        if os.path.exists(PATH_SAVED):\n",
        "            print(f\"      but a file at '{PATH_SAVED}' exists !!\")\n",
        "    INITIAL_EPOCH_ = 0\n",
        "    print(\"INITIAL_EPOCH_ set to 0\\nModel created\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLEFnedaTSkR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TENSORBOARD_LOG = True\n",
        "# https://stable-baselines.readthedocs.io/en/master/guide/tensorboard.html\n",
        "\n",
        "if TENSORBOARD_LOG:\n",
        "    model.tensorboard_log = os.path.join(LOG_PATH, f\"tb_logs\")\n",
        "    print(f\"Adding TensorBoard logs to '{model.tensorboard_log}/'\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8enCSG_B4z12",
        "colab": {}
      },
      "source": [
        "if CONTINUE_TRAINING:\n",
        "    play_and_show(model, env_record) # Show video of model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjFzGXtK_Qve",
        "colab_type": "text"
      },
      "source": [
        "## Constants\n",
        "Just printing out the constants we use to make sure they're right 😅"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32mNK-zr_Xnn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"ALGORITHM:         {ALGORITHM}\")\n",
        "print(f\"POLICY:            {POLICY}\")\n",
        "print(f\"ENVIRONMENT:       {ENVIRONMENT}\")\n",
        "print()\n",
        "print(f\"CONTINUE_TRAINING: {CONTINUE_TRAINING}\")\n",
        "print(f\"TRAIN_FOREVER:     {TRAIN_FOREVER}\")\n",
        "print(f\"INITIAL_EPOCH_:    {INITIAL_EPOCH_}\")\n",
        "print(f\"EPOCHS:            {EPOCHS}\")\n",
        "print(f\"MAX_EPOCHS:        {MAX_EPOCHS}\")\n",
        "print()\n",
        "print(f\"PATH_PREFIX:       {PATH_PREFIX}\")\n",
        "print(f\"FILE_NAME_:        {FILE_NAME_}\")\n",
        "print(f\"SAVE_PATH_:        {SAVE_PATH_}\")\n",
        "print()\n",
        "print(f\"PATH_VIDEO:        {PATH_VIDEO}\")\n",
        "print(f\"LOG_PATH:          {LOG_PATH}\")\n",
        "print()\n",
        "print(f\"VERBOSITY:         {VERBOSITY}\")\n",
        "print(f\"PATH_SAVED:        {PATH_SAVED}\")\n",
        "print(f\"TENSORBOARD_LOG:   {TENSORBOARD_LOG}\")\n",
        "print()\n",
        "print(f\"obs.: variables ending in '_' may have their values changed\") # they're not really constants"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj8FLMg8GLk4",
        "colab_type": "text"
      },
      "source": [
        "## Learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "96M5yX114z16",
        "colab": {}
      },
      "source": [
        "strftime(\"%X\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0SjhkYLjiqZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if TENSORBOARD_LOG:\n",
        "    %load_ext tensorboard\n",
        "    %tensorboard --logdir tmp/tb_logs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkD3oJnTg2VG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FIRST_EPOCH = INITIAL_EPOCH_ # saves the value of INITIAL_EPOCH_ before we start training\n",
        "while True:\n",
        "    start = time()\n",
        "    # Pass reset_num_timesteps=False to continue the training curve in tensorboard\n",
        "    model = model.learn(EPOCHS, callback=callback, reset_num_timesteps=(INITIAL_EPOCH_ == FIRST_EPOCH))\n",
        "    dt = time() - start\n",
        "    print(f\"\\nTraining took {dt:.2f}s (~{(dt//60):.0f} minute{((dt//60) != 1) * 's'})\")\n",
        "\n",
        "    # Save model\n",
        "    FILE_NAME_ = f\"{ALGORITHM}_{POLICY}_ep{INITIAL_EPOCH_ + EPOCHS}\"\n",
        "    SAVE_PATH_ = os.path.join(PATH_PREFIX, FILE_NAME_)\n",
        "    print(\"Saving to\", SAVE_PATH_)\n",
        "    model.save(SAVE_PATH_)\n",
        "    INITIAL_EPOCH_ += EPOCHS\n",
        "\n",
        "    # Wrapped env for recording\n",
        "    env_record = wrap_env(make_env(), video_name_prefix=FILE_NAME_)\n",
        "\n",
        "    # Show video of model\n",
        "    play_and_show(model, env_record) # set show_info=True to prints each step's info and reward\n",
        "\n",
        "    dt = time() - start\n",
        "    print(f\"The iteration took {dt:.2f}s (~{(dt//60):.0f} minute{((dt//60) != 1) * 's'})\")\n",
        "\n",
        "    if not TRAIN_FOREVER:\n",
        "        reached_max_epochs = MAX_EPOCHS is None or (INITIAL_EPOCH_ - FIRST_EPOCH) >= MAX_EPOCHS\n",
        "        if reached_max_epochs:\n",
        "            print(f\"\\nStop training (at epoch {INITIAL_EPOCH_}, started at {FIRST_EPOCH})\")\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vgi8k2qI-z_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "strftime(\"%X\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOP_rnnY4zde",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}