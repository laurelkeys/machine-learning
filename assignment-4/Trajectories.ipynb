{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trajectories.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ANURED63lzYB",
        "_AmTVEEaTk3Q"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laurelkeys/machine-learning/blob/master/assignment-4/Trajectories.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8D0TEl9vLJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount=True)\n",
        "# PATH_TO_DATA = os.path.join(\"drive\", \"My Drive\", \"unicamp\", \"MC886\", \"atari\")\n",
        "\n",
        "import os\n",
        "PATH_TO_DATA = \"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSrCsl0Av6RV",
        "colab_type": "code",
        "outputId": "79437bac-6b84-4f6e-ceda-c379f59633fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "SAVE_DIR = os.path.join(PATH_TO_DATA, \"data\")\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "SAVE_DIR # where the trajectories for each game will be saved to"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'data'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zUPRZG8TOTv",
        "colab_type": "code",
        "outputId": "468f5def-5652-481e-de2d-cae695ff923a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "LOG_DIR = os.path.join(PATH_TO_DATA, \"data\", \"results\")\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "\n",
        "LOG_DIR # where the stats for each game will be saved to"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'data/results'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6rL58MEZRQh",
        "colab_type": "text"
      },
      "source": [
        "## Generate a dataset of trajectories from pre-trained RL agents on [Atari](https://gym.openai.com/envs/#atari) [environments](https://github.com/openai/gym/wiki/Table-of-environments).\n",
        "That is, by the end of this notebook we will have $observation \\rightarrow action$ mappings, where $observation$s are images of shape `IMG_SHAPE` and $action$s are integer values in the range $[0, 18)$, meaning:\n",
        "\n",
        "| 0 | 1 | 2 | 3 | 4 | 5 |\n",
        "| --- | --- | --- | --- | --- | --- |\n",
        "| NOOP | FIRE | UP | RIGHT | LEFT | DOWN |\n",
        "\n",
        "\n",
        "| 6 | 7 | 8 | 9 |\n",
        "| --- | --- | --- | --- |\n",
        "| UPRIGHT | UPLEFT | DOWNRIGHT | DOWNLEFT |\n",
        "\n",
        "\n",
        "| 10 | 11 | 12 | 13 |\n",
        "| --- | --- | --- | --- |\n",
        "| UPFIRE | RIGHTFIRE | LEFTFIRE | DOWNFIRE |\n",
        "\n",
        "\n",
        "| 14 | 15 | 16 | 17 |\n",
        "| --- | --- | --- | --- |\n",
        "| UPRIGHTFIRE | UPLEFTFIRE | DOWNRIGHTFIRE | DOWNLEFTFIRE |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxiF53dtcx5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of trajectories to generate\n",
        "N_OF_TRAJECTORIES = 2\n",
        "\n",
        "# number of steps per trajectory\n",
        "N_OF_STEPS = 20\n",
        "\n",
        "# list of string tuples in the format (RL Algorithm, Game Environment)\n",
        "GAMES = [\n",
        "    (\"PPO2\", \"BreakoutNoFrameskip-v4\"),\n",
        "    (\"PPO2\", \"PongNoFrameskip-v4\"),\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xniuUnC1sohU",
        "colab_type": "code",
        "outputId": "4761f762-9dfe-4e1e-bd6f-5f74487dcd83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "[env_id for algo, env_id in GAMES]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['BreakoutNoFrameskip-v4', 'PongNoFrameskip-v4']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANURED63lzYB",
        "colab_type": "text"
      },
      "source": [
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOvHHxnVn5rp",
        "colab_type": "text"
      },
      "source": [
        "Note that we're not installing [MPI](https://mpi4py.readthedocs.io/en/stable/), so the following algorithms will probably not work: `DDPG`, `GAIL`, `PPO1`, `TRPO`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmjBQE8sl8CP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get update                                                  > /dev/null 2>&1\n",
        "!apt-get install swig cmake zlib1g-dev ffmpeg freeglut3-dev xvfb > /dev/null 2>&1\n",
        "!pip install pytablewriter                                       > /dev/null 2>&1\n",
        "# !pip install pytablewriter pyyaml optuna scikit-optimize         > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_p5-8pYmEsa",
        "colab_type": "code",
        "outputId": "a86873ce-a258-49d7-e9c0-3dc6bbbe8e20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#### Stable Baselines only supports TF 1.x for now ####\n",
        "try:\n",
        "    # Colab only\n",
        "    %tensorflow_version 1.x\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYDsdxKLulZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from time import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# NOTE use tqdm.write() instead of print() inside of tqdm wrapped loops\n",
        "from tqdm import tqdm\n",
        "\n",
        "import gym\n",
        "from gym.envs.atari.atari_env import ACTION_MEANING"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6d58ZxSm7BR",
        "colab_type": "text"
      },
      "source": [
        "### Update [Stable Baselines](https://github.com/hill-a/stable-baselines) and clone [RL Zoo Baselines](https://github.com/araffin/rl-baselines-zoo)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Mx9fivzl8KW",
        "colab_type": "code",
        "outputId": "39f1e67c-ab6a-4fba-dcf9-87eb5d1394bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!pip list | grep baselines"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "stable-baselines         2.2.1      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPOfKIrIm3Ef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!yes | pip uninstall stable-baselines                           > /dev/null 2>&1\n",
        "!pip install git+https://github.com/hill-a/stable-baselines.git > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZG914mxnHN_",
        "colab_type": "code",
        "outputId": "c33ad248-3ed9-40f5-dde0-d57049359614",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!pip list | grep baselines"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "stable-baselines         2.9.0a0    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULku9CT6ovu2",
        "colab_type": "code",
        "outputId": "ab83d244-59ec-42ec-8548-558a496e413b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "from stable_baselines.common.cmd_util import make_atari_env\n",
        "from stable_baselines.common.vec_env import VecFrameStack, DummyVecEnv\n",
        "\n",
        "# HACK to save logs\n",
        "from stable_baselines import logger\n",
        "os.environ[\"OPENAI_LOG_FORMAT\"] = 'csv' # 'stdout,log,csv,tensorboard'\n",
        "os.environ[\"OPENAI_LOGDIR\"] = os.path.abspath(LOG_DIR)\n",
        "logger.configure()\n",
        "\n",
        "# NOTE add more algorithms here if you want to use them\n",
        "from stable_baselines import PPO2, ACER, ACKTR\n",
        "ALGO_IMPL = {\n",
        "    'PPO2': PPO2,\n",
        "    'ACER': ACER,\n",
        "    'ACKTR': ACKTR,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBZSNLAGnpkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/araffin/rl-baselines-zoo.git      > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZQhTCzfwAZT",
        "colab_type": "text"
      },
      "source": [
        "## Load the pre-trained agents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BukuKWcxxaed",
        "colab_type": "code",
        "outputId": "c19429c8-f016-4473-82a3-aa12a42b2dec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls rl-baselines-zoo/trained_agents/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a2c  acer  acktr  ddpg\tdqn  her  ppo2\tsac  td3  trpo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhFU5ULCxYEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH_TO_AGENTS = os.path.join(\"rl-baselines-zoo\", \"trained_agents\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGcL8fuWUdHT",
        "colab_type": "code",
        "outputId": "c21b14f5-2e91-4dca-c949-6886e0b27c7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "ext = \"NoFrameskip-v4.pkl\"\n",
        "# check the available pre-trained models\n",
        "algorithms = [\"PPO2\"]\n",
        "for algo in algorithms:\n",
        "    algo_path = os.path.join(PATH_TO_AGENTS, algo.lower())\n",
        "    print(algo_path + '/')\n",
        "    for f in sorted(os.listdir(algo_path), key=lambda x: x[::-1]):\n",
        "        # sort by the reverse filename, so env types get grouped together\n",
        "        if f.endswith(ext):\n",
        "            print(\"├──\", f)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rl-baselines-zoo/trained_agents/ppo2/\n",
            "├── PongNoFrameskip-v4.pkl\n",
            "├── MsPacmanNoFrameskip-v4.pkl\n",
            "├── EnduroNoFrameskip-v4.pkl\n",
            "├── BeamRiderNoFrameskip-v4.pkl\n",
            "├── SpaceInvadersNoFrameskip-v4.pkl\n",
            "├── QbertNoFrameskip-v4.pkl\n",
            "├── SeaquestNoFrameskip-v4.pkl\n",
            "├── BreakoutNoFrameskip-v4.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5ye3CG3NzJo",
        "colab_type": "code",
        "outputId": "006fc050-010d-4263-fd3b-f692e5aa78e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "for i in range(2):\n",
        "    clear_output() # HACK to remove TensorFlow warnings\n",
        "    for algo, env_id in GAMES:\n",
        "        print(f\"('{algo}', '{env_id}')\")\n",
        "        agent_path = os.path.join(PATH_TO_AGENTS, algo.lower(), env_id + '.pkl')\n",
        "        model = ALGO_IMPL[algo].load(agent_path, verbose=0)\n",
        "        print(\"observation_space:\", model.observation_space)\n",
        "        print(\"action_space:\", model.action_space)\n",
        "        print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('PPO2', 'BreakoutNoFrameskip-v4')\n",
            "observation_space: Box(84, 84, 4)\n",
            "action_space: Discrete(4)\n",
            "\n",
            "('PPO2', 'PongNoFrameskip-v4')\n",
            "observation_space: Box(84, 84, 4)\n",
            "action_space: Discrete(6)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwdzhBHua5ad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VERBOSE = 2 # 0, 1 or 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFyZqGsRP7SG",
        "colab_type": "code",
        "outputId": "c7e118c6-603e-40d2-b99f-69bff310668b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print(\"N_OF_STEPS:\", N_OF_STEPS)\n",
        "print(\"N_OF_TRAJECTORIES:\", N_OF_TRAJECTORIES)\n",
        "print(N_OF_STEPS, \"*\", N_OF_TRAJECTORIES, \"=\", N_OF_STEPS * N_OF_TRAJECTORIES)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "N_OF_STEPS: 20\n",
            "N_OF_TRAJECTORIES: 2\n",
            "20 * 2 = 40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkmS5sW_M76d",
        "colab_type": "text"
      },
      "source": [
        "## Use Stable Baseline's `generate_expert_traj`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krB3TyHwT1dd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAJ_IMAGES_FOLDER = \"images\" # name of the folder in which to save the observations\n",
        "TRAJ_FILE_NAME = \"trajectory\" # name of the .npz trajectory file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM3TWb5sgZlD",
        "colab_type": "code",
        "outputId": "551aa02a-edbc-41f6-a815-39dcfdb8da44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(LOG_DIR)\n",
        "print(logger.get_dir())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/results\n",
            "/content/data/results\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81YyG8d-NCV-",
        "colab_type": "code",
        "outputId": "cdf84b0f-ed7d-479e-a2fc-5dfe943b48bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        }
      },
      "source": [
        "from stable_baselines.gail import generate_expert_traj\n",
        "\n",
        "time_start = time()\n",
        "print(\"================\")\n",
        "for algo, env_id in GAMES:\n",
        "    time_start_env = time()\n",
        "\n",
        "    env = make_atari_env(env_id, num_env=1, seed=0)\n",
        "    env = VecFrameStack(env, n_stack=4) # Frame-stacking with 4 frames\n",
        "    agent_path = os.path.join(PATH_TO_AGENTS, algo.lower(), env_id + '.pkl')\n",
        "    \n",
        "    print(f\"('{algo}', '{env_id}')\")\n",
        "    print(f\"Getting pre-trained agent from: '{agent_path}'\")\n",
        "    if VERBOSE > 1:\n",
        "        print(f\"env.envs: {env.envs}\")\n",
        "    print()\n",
        "    \n",
        "    model = ALGO_IMPL[algo].load(agent_path, env)\n",
        "    \n",
        "    traj_name = f\"{env_id}_{algo}_{N_OF_STEPS}s\"\n",
        "    generate_expert_traj(model, env=env, \n",
        "                         n_episodes=N_OF_TRAJECTORIES, \n",
        "                         save_path=os.path.join(SAVE_DIR, traj_name, TRAJ_FILE_NAME), \n",
        "                         image_folder=TRAJ_IMAGES_FOLDER)\n",
        "\n",
        "    env.close()\n",
        "    print(f\"\\nΔt = {(time() - time_start_env):.2f}s\")\n",
        "    print(\"================\")\n",
        "\n",
        "print(f\"\\nTotal Δt = {(time() - time_start):.2f}s\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================\n",
            "('PPO2', 'BreakoutNoFrameskip-v4')\n",
            "Getting pre-trained agent from: 'rl-baselines-zoo/trained_agents/ppo2/BreakoutNoFrameskip-v4.pkl'\n",
            "env.envs: [<ClipRewardEnv<WarpFrame<FireResetEnv<EpisodicLifeEnv<Monitor<MaxAndSkipEnv<NoopResetEnv<TimeLimit<AtariEnv<BreakoutNoFrameskip-v4>>>>>>>>>>]\n",
            "\n",
            "==========\n",
            "Images will be recorded to data/BreakoutNoFrameskip-v4_PPO2_20s/images/\n",
            "Image shape: (84, 84, 4)\n",
            "==========\n",
            "actions (2182, 1)\n",
            "obs (2182,)\n",
            "rewards (2182, 1)\n",
            "episode_returns (2,)\n",
            "episode_starts (2182,)\n",
            "\n",
            "Δt = 8.31s\n",
            "================\n",
            "('PPO2', 'PongNoFrameskip-v4')\n",
            "Getting pre-trained agent from: 'rl-baselines-zoo/trained_agents/ppo2/PongNoFrameskip-v4.pkl'\n",
            "env.envs: [<ClipRewardEnv<WarpFrame<FireResetEnv<EpisodicLifeEnv<Monitor<MaxAndSkipEnv<NoopResetEnv<TimeLimit<AtariEnv<PongNoFrameskip-v4>>>>>>>>>>]\n",
            "\n",
            "==========\n",
            "Images will be recorded to data/PongNoFrameskip-v4_PPO2_20s/images/\n",
            "Image shape: (84, 84, 4)\n",
            "==========\n",
            "actions (3400, 1)\n",
            "obs (3400,)\n",
            "rewards (3400, 1)\n",
            "episode_returns (2,)\n",
            "episode_starts (3400,)\n",
            "\n",
            "Δt = 11.97s\n",
            "================\n",
            "\n",
            "Total Δt = 20.28s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AmTVEEaTk3Q",
        "colab_type": "text"
      },
      "source": [
        "## Download generated dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FFwBqmxTnnH",
        "colab_type": "code",
        "outputId": "e8191d60-e464-4530-be01-51f574a19d6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "trajectories = []\n",
        "image_folders = []\n",
        "for save_folder in os.listdir(SAVE_DIR):\n",
        "    for x in os.listdir(os.path.join(SAVE_DIR, save_folder)):\n",
        "        path = os.path.join(SAVE_DIR, save_folder, x)\n",
        "        if x == TRAJ_IMAGES_FOLDER:\n",
        "            image_folders.append(path)\n",
        "        elif os.path.splitext(x)[0] == TRAJ_FILE_NAME:\n",
        "            trajectories.append(path)\n",
        "\n",
        "assert len(image_folders) == len(trajectories), f\"{len(image_folders)} != {len(trajectories)} (len(image_folders) != len(trajectories))\"\n",
        "\n",
        "print('\\n\\n'.join([traj + '\\n' + img_folder for traj, img_folder in zip(trajectories, image_folders)]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/BreakoutNoFrameskip-v4_PPO2_20s/trajectory.npz\n",
            "data/BreakoutNoFrameskip-v4_PPO2_20s/images\n",
            "\n",
            "data/PongNoFrameskip-v4_PPO2_20s/trajectory.npz\n",
            "data/PongNoFrameskip-v4_PPO2_20s/images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOCcisNVlRbB",
        "colab_type": "code",
        "outputId": "2e4a00ea-7c53-4903-d087-a26d186c001e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x = np.load('data/PongNoFrameskip-v4_PPO2_20s/trajectory.npz', \n",
        "            allow_pickle=True, mmap_mode='r')\n",
        "print(x.files)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['actions', 'obs', 'rewards', 'episode_returns', 'episode_starts']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9SB2IQLlUlX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_trajectory_info(x):\n",
        "    # actions taken\n",
        "    print(\"- actions:\", x['actions'].shape)\n",
        "    print(\"  actions taken:\", ', '.join([ACTION_MEANING[action] for action in set(x['actions'].reshape(-1))]))\n",
        "    # path to the observed images\n",
        "    print(\"- obs:\", x['obs'].shape)\n",
        "    # reward for each step\n",
        "    print(\"- rewards:\", x['rewards'].shape)\n",
        "    print(\"  reward values:\", ', '.join([str(r) for r in set(x['rewards'].reshape(-1))]))\n",
        "    # reward for each trajectory\n",
        "    print(\"- episode_returns:\", x['episode_returns'].shape)\n",
        "    print(\"  episode returns\", x['episode_returns'])\n",
        "    # `done` value returned by env.step(action)\n",
        "    print(\"- episode_starts:\", x['episode_starts'].shape)\n",
        "    print(\"  episode starts:\", [i for i, ep_start in enumerate(x['episode_starts']) if ep_start])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBMqViaRqR9D",
        "colab_type": "code",
        "outputId": "95a136fd-bce0-494f-a802-3708c1b0d3e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "source": [
        "print(\"================\")\n",
        "for trajectory in trajectories:\n",
        "    print(trajectory)\n",
        "    print_trajectory_info(np.load(trajectory, allow_pickle=True, mmap_mode='r'))\n",
        "    print(\"================\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================\n",
            "data/BreakoutNoFrameskip-v4_PPO2_20s/trajectory.npz\n",
            "- actions: (2182, 1)\n",
            "  actions taken: NOOP, FIRE, UP, RIGHT\n",
            "- obs: (2182,)\n",
            "- rewards: (2182, 1)\n",
            "  reward values: 0.0, 1.0\n",
            "- episode_returns: (2,)\n",
            "  episode returns [96.  1.]\n",
            "- episode_starts: (2182,)\n",
            "  episode starts: [0, 2033]\n",
            "================\n",
            "data/PongNoFrameskip-v4_PPO2_20s/trajectory.npz\n",
            "- actions: (3400, 1)\n",
            "  actions taken: NOOP, FIRE, UP, RIGHT, LEFT, DOWN\n",
            "- obs: (3400,)\n",
            "- rewards: (3400, 1)\n",
            "  reward values: 0.0, 1.0\n",
            "- episode_returns: (2,)\n",
            "  episode returns [21. 21.]\n",
            "- episode_starts: (3400,)\n",
            "  episode starts: [0, 1691]\n",
            "================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwoq2YnzShiU",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate agents\n",
        "Let's evaluate a few trajectories of each game from `GAMES` to get a sense of how the agents are performing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70P1Tq1eTE6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of trajectories to evaluate each agent on\n",
        "N_OF_EVAL_TRAJECTORIES = 2\n",
        "\n",
        "results = {\n",
        "    'algo': [],\n",
        "    'env_id': [],\n",
        "    'mean_reward': [],\n",
        "    'std_reward': [],\n",
        "    'n_timesteps': [],\n",
        "    'n_episodes': []\n",
        "}\n",
        "\n",
        "TENSORBOARD_LOG = False\n",
        "# https://stable-baselines.readthedocs.io/en/master/guide/tensorboard.html\n",
        "\n",
        "if TENSORBOARD_LOG:\n",
        "    %load_ext tensorboard\n",
        "    %tensorboard --logdir tmp/tb_logs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClxDKFaISpJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "time_start = time()\n",
        "print(\"================\")\n",
        "for algo, env_id in GAMES:\n",
        "    time_start_env = time()\n",
        "\n",
        "    env = make_atari_env(env_id, num_env=1, seed=0)\n",
        "    env = VecFrameStack(env, n_stack=4) # Frame-stacking with 4 frames\n",
        "    agent_path = os.path.join(PATH_TO_AGENTS, algo.lower(), env_id + '.pkl')\n",
        "    \n",
        "    print(f\"('{algo}', '{env_id}')\")\n",
        "    print(f\"Getting pre-trained agent from: '{agent_path}'\")\n",
        "    if VERBOSE > 1:\n",
        "        print(f\"env.envs: {env.envs}\")\n",
        "    print()\n",
        "    \n",
        "    model = ALGO_IMPL[algo].load(agent_path, env)\n",
        "    if TENSORBOARD_LOG:\n",
        "        model.tensorboard_log = os.path.join(LOG_DIR, f\"tb_logs\")\n",
        "        print(f\"Adding TensorBoard logs to '{model.tensorboard_log}/'\")\n",
        "    \n",
        "    for trajectory in tqdm(range(N_OF_EVAL_TRAJECTORIES), position=0, leave=True):\n",
        "        # episode stats\n",
        "        ep_len, ep_reward, ep_rewards = 0, 0.0, []\n",
        "\n",
        "        obs = env.reset() # (84, 84, 4)\n",
        "        for step in range(N_OF_STEPS):\n",
        "            action = model.predict(obs)\n",
        "            # clip action to avoid out of bound errors\n",
        "            if isinstance(env.action_space, gym.spaces.Box):\n",
        "                action = np.clip(action, env.action_space.low, env.action_space.high)\n",
        "            \n",
        "            obs, reward, done, infos = env.step(action)\n",
        "            # NOTE action, reward and done are arrays since we're using a vectorized env\n",
        "            _env = env.envs[0]\n",
        "            # tqdm.write(f\"env.envs[0].episode_rewards: {env.envs[0].episode_rewards}\")\n",
        "            # tqdm.write(f\"env.envs[0].episode_lengths: {env.envs[0].episode_lengths}\")\n",
        "            # tqdm.write(f\"env.envs[0].episode_times: {env.envs[0].episode_times}\")\n",
        "            # tqdm.write(f\"env.envs[0].rewards: {env.envs[0].rewards}\")\n",
        "\n",
        "            # NOTE the return reward is not the Atari score\n",
        "            #      so we have to get it from the infos dict\n",
        "            ep_infos = infos[0].get('episode')\n",
        "            if ep_infos is not None:\n",
        "                tqdm.write(f\"\\nAtari Episode Score: {ep_infos['r']:.2f}\")\n",
        "                tqdm.write(f\"Atari Episode Length: {ep_infos['l']}\")\n",
        "                #tqdm.write(f\"\\nAtari Episode Score: {round(sum(_env.rewards), 6):.2f}\")\n",
        "                #tqdm.write(f\"Atari Episode Length: {len(_env.rewards)}\")\n",
        "            \n",
        "            # FIXME ep_infos is always None \n",
        "            # try checking stable-baselines' Monitor.step()\n",
        "            if infos is not None: # debug\n",
        "                # if step == 0:\n",
        "                #     tqdm.write(f\"(DEBUG) step == 0: {infos}\")\n",
        "                if len(infos) > 1:\n",
        "                    tqdm.write(f\"(DEBUG) infos: {infos}\")\n",
        "                elif len(infos[0].keys()) > 1:\n",
        "                    if 'terminal_observation' not in infos[0].keys() or len(infos[0].keys()) > 2:\n",
        "                        tqdm.write(f\"(DEBUG) infos[0]: {infos[0]}\")\n",
        "\n",
        "            ep_len += 1\n",
        "            ep_reward += reward[0]\n",
        "            if done:\n",
        "                obs = env.reset()\n",
        "                ep_rewards.append(ep_reward)\n",
        "                if VERBOSE > 1:\n",
        "                    tqdm.write(f\"\\nEpisode Reward: {ep_reward:.2f}\")\n",
        "                    tqdm.write(f\"Episode Length: {ep_len}\")\n",
        "                ep_reward = 0.0\n",
        "                ep_len = 0\n",
        "        \n",
        "        if VERBOSE > 0:\n",
        "            tqdm.write(\"\\nMean reward: {:.2f}, len(ep_rewards) == {}\".format(\n",
        "                       np.mean(ep_rewards) if len(ep_rewards) > 0 else 0.0, \n",
        "                       len(ep_rewards)))\n",
        "\n",
        "    env.close()\n",
        "    print(f\"Δt = {(time() - time_start_env):.2f}s\")\n",
        "    print(\"================\")\n",
        "\n",
        "print(f\"Total Δt = {(time() - time_start):.2f}s\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVTn0snAndBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls data/results/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHrpWV5vNviz",
        "colab_type": "text"
      },
      "source": [
        "## Generate trajectories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFvNuI8vb75g",
        "colab_type": "text"
      },
      "source": [
        "Note that we use `make_atari_env` + `VecFrameStack` for `NoFrameskip-v4` environments, so each frame is converted to grayscale and downscaled from 210x160 to 84x84. Therefore, the $observation$ shape is `(84, 84, 4)` (four stacked frames), and **not** `(210, 160, 3)`, nor `(84, 84, 1)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2ugXR8sZ6RM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PRINT_EARLY_DONE = False\n",
        "PRINT_ACTIONS_TAKEN = False\n",
        "\n",
        "PRINT_EVERY_N_TRAJECTORIES = N_OF_TRAJECTORIES // 10\n",
        "# uncomment below not to print\n",
        "# PRINT_EVERY_N_TRAJECTORIES = N_OF_TRAJECTORIES + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAKJ7SOIjA8q",
        "colab_type": "text"
      },
      "source": [
        "**TODO: evaluate the trajectories before saving the final datasets**  \n",
        "**TODO: add button to load from data/ or save to drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvtLqqVZwFEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "time_start = time()\n",
        "print(\"PRINT_EVERY_N_TRAJECTORIES:\", PRINT_EVERY_N_TRAJECTORIES)\n",
        "print(\"N_OF_TRAJECTORIES:\", N_OF_TRAJECTORIES)\n",
        "print(\"N_OF_STEPS:\", N_OF_STEPS)\n",
        "print(\"================\")\n",
        "for algo, env_id in GAMES:\n",
        "    time_start_env = time()\n",
        "\n",
        "    env = make_atari_env(env_id, num_env=1, seed=0)\n",
        "    env = VecFrameStack(env, n_stack=4) # Frame-stacking with 4 frames\n",
        "    agent_path = os.path.join(PATH_TO_AGENTS, algo.lower(), env_id + '.pkl')\n",
        "    \n",
        "    print(f\"('{algo}', '{env_id}')\")\n",
        "    print(f\"Getting pre-trained agent from: '{agent_path}'\\n\")\n",
        "    \n",
        "    model = ALGO_IMPL[algo].load(agent_path, env)\n",
        "    \n",
        "    for trajectory in tqdm(range(N_OF_TRAJECTORIES), position=0, leave=True):\n",
        "        # store the \"obs -> action\" mapping\n",
        "        observed_states, actions_taken = [], []\n",
        "\n",
        "        obs = env.reset() # (84, 84, 4)\n",
        "        for step in range(N_OF_STEPS):\n",
        "            action = model.predict(obs)\n",
        "            observed_states.append(obs)\n",
        "            actions_taken.append(action)\n",
        "            obs, reward, done, infos = env.step(action)\n",
        "            if done:\n",
        "                obs = env.reset()\n",
        "                if PRINT_EARLY_DONE:\n",
        "                    print(f\"done at step {step + 1} (reseting env)\")\n",
        "        \n",
        "        # NOTE action, reward and done are arrays since we're using a vectorized env\n",
        "        observed_states = [obs[0] for obs in observed_states]\n",
        "        actions_taken = [action[0][0] for action in actions_taken]\n",
        "        \n",
        "        np.savez_compressed(file=os.path.join(SAVE_DIR, f\"{env_id}_{algo}_t{trajectory+1}_{N_OF_STEPS}s\"), \n",
        "                            observations=observed_states, actions=actions_taken)\n",
        "        \n",
        "        if (trajectory + 1) % 10 == 0:\n",
        "            print(f\" Saved trajectory {trajectory+1} (of {N_OF_TRAJECTORIES})\")\n",
        "\n",
        "        if PRINT_ACTIONS_TAKEN and trajectory == N_OF_TRAJECTORIES - 1:\n",
        "            print(\"\\nActions taken:\", \", \".join([ACTION_MEANING[action] for action in set(actions_taken)]))\n",
        "\n",
        "    del observed_states\n",
        "    del actions_taken\n",
        "    env.close()\n",
        "    print(f\"Δt = {(time() - time_start_env):.2f}s\")\n",
        "    print(\"================\")\n",
        "\n",
        "print(f\"Total Δt = {(time() - time_start):.2f}s\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40zclw3dAGcv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trajectory_filenames = []\n",
        "for r, ds, fs in os.walk(SAVE_DIR): # r=root, d=directories, f=files\n",
        "    print(r + '/')\n",
        "    for f in fs:\n",
        "        print(\"|___\", f)\n",
        "        trajectory_filenames.append(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Td2MzGZK_7ht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_trajectory_filename = trajectory_filenames[0]\n",
        "print(f\"Loading from '{test_trajectory_filename}'\\n\")\n",
        "\n",
        "test_trajectory_load = np.load(os.path.join(SAVE_DIR, test_trajectory_filename), \n",
        "                               allow_pickle=True)\n",
        "\n",
        "print(\"observations shape:\", test_trajectory_load['observations'].shape)\n",
        "print(\"actions shape:\", test_trajectory_load['actions'].shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6vHSNX5YFng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/araffin/rl-baselines-zoo/blob/master/utils/record_video.py\n",
        "# https://github.com/araffin/rl-baselines-zoo/blob/master/enjoy.py\n",
        "# https://github.com/hill-a/stable-baselines#try-it-online-with-colab-notebooks-"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-bxg62b_5N7",
        "colab_type": "text"
      },
      "source": [
        "## Old"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ump8v2czCIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def save_as_image(observation, save_dir, img_name, prefix=\"img_\", downscale=False):\n",
        "#     # downscaling the image\n",
        "#     if downscale:\n",
        "#         im_array = cv2.resize(observation, INP_IMAGE_SHAPE) # TODO test tf.image.resize\n",
        "#         im_array = np.array(im_array, dtype='float32')\n",
        "#         im_array = (im_array/127.5) - 1\n",
        "#         im = PIL.Image.fromarray(im_array, 'RGB')\n",
        "#     else:\n",
        "#         try:\n",
        "#             im = PIL.Image.fromarray(observation, 'RGB')\n",
        "#         except:\n",
        "#             print(type(observation))\n",
        "#     imname = \"{}{}.png\".format(prefix, img_name)\n",
        "#     im.save(os.path.join(save_dir, imname))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n74gjFzbzkJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # you can change the default values here\n",
        "# save_dir = SAVE_DIR\n",
        "# num_images = IMAGES_TO_GENERATE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wd6qHz4zyp7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# os.makedirs(save_dir, exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHKIU3Vn0AcL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# envs = [gym.make(env_id) for env_id in ENV_IDS]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z4YJ4XV0HF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for env_id, env in zip(ENV_IDS, envs):\n",
        "#     print(env_id)\n",
        "#     env_dir = os.path.join(save_dir, f\"{env_id}_{IMAGES_TO_GENERATE}\")\n",
        "#     os.makedirs(env_dir, exist_ok=True)\n",
        "    \n",
        "#     env.reset()\n",
        "#     i, current_env_images = 0, 0\n",
        "    \n",
        "#     actions_taken = []\n",
        "#     while i < num_images:\n",
        "#         # take a random action (sampled from the action space)\n",
        "#         action = env.action_space.sample()\n",
        "#         actions_taken.append(action)\n",
        "#         assert 0 <= action < 18, f\"action = {action}\"\n",
        "#         obs, _, done, _ = env.step(action)\n",
        "#         if np.mean(obs) > 0.01:\n",
        "#             save_as_image(obs, env_dir, str(i))\n",
        "#             i += 1\n",
        "#         else:\n",
        "#             print(\"should I have been reached?\")\n",
        "#             continue\n",
        "#         if done:\n",
        "#             print(f\"reseting {env_id} at i={i}\")\n",
        "#             env.reset()\n",
        "    \n",
        "#     actions_taken = np.asarray(actions_taken, dtype='int8')\n",
        "#     print(actions_taken.shape, actions_taken.size, actions_taken.dtype)\n",
        "#     np.save(os.path.join(save_dir, f\"{env_id}_{IMAGES_TO_GENERATE}_actions\"), actions_taken)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NZCKFvS4J3sd",
        "colab": {}
      },
      "source": [
        "# IMG_SIZE = 160 # All images will be resized to 160x160\n",
        "\n",
        "# def load_image(image_path):\n",
        "#     image = tf.io.read_file(image_path)\n",
        "#     image = tf.image.decode_png(image, channels=3)\n",
        "#     image = tf.cast(image, tf.float32)\n",
        "#     image = (image/127.5) - 1\n",
        "#     image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "#     return image, image_path\n",
        "\n",
        "# IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
        "\n",
        "# # Create the base model from the pre-trained model MobileNet V2\n",
        "# base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
        "#                                                include_top=False,\n",
        "#                                                weights='imagenet')\n",
        "\n",
        "# s = time()\n",
        "# # Get unique images\n",
        "# encode_train = img_name_vector\n",
        "\n",
        "# # Feel free to change batch_size according to your system configuration\n",
        "# image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
        "# image_dataset = image_dataset.map(\n",
        "#   load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
        "\n",
        "# print((time()-s)/1000)\n",
        "\n",
        "# for img, path in image_dataset:\n",
        "#   batch_features = image_features_extract_model(img)\n",
        "#   batch_features = tf.reshape(batch_features,\n",
        "#                               (batch_features.shape[0], -1, batch_features.shape[3]))\n",
        "\n",
        "#   for bf, p in zip(batch_features, path):\n",
        "#     path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "#     np.save(path_of_feature, bf.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}