{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trajectories.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laurelkeys/machine-learning/blob/master/assignment-4/Trajectories.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8D0TEl9vLJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount=True)\n",
        "# PATH_TO_DATA = os.path.join(\"drive\", \"My Drive\", \"unicamp\", \"MC886\", \"atari\")\n",
        "\n",
        "import os\n",
        "PATH_TO_DATA = \"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSrCsl0Av6RV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "42268045-0fce-450a-cf96-8243aa705b64"
      },
      "source": [
        "SAVE_DIR = os.path.join(PATH_TO_DATA, \"data\")\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "SAVE_DIR"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'data'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6rL58MEZRQh",
        "colab_type": "text"
      },
      "source": [
        "## Generate a dataset of trajectories from pre-trained RL agents on [Atari](https://gym.openai.com/envs/#atari) [environments](https://github.com/openai/gym/wiki/Table-of-environments).\n",
        "That is, by the end of this notebook we will have $observation \\rightarrow action$ mappings, where $observation$s are images of shape `IMG_SHAPE` and $action$s are integer values in the range $[0, 18)$, meaning:\n",
        "\n",
        "| 0 | 1 | 2 | 3 | 4 | 5 |\n",
        "| --- | --- | --- | --- | --- | --- |\n",
        "| NOOP | FIRE | UP | RIGHT | LEFT | DOWN |\n",
        "\n",
        "\n",
        "| 6 | 7 | 8 | 9 |\n",
        "| --- | --- | --- | --- |\n",
        "| UPRIGHT | UPLEFT | DOWNRIGHT | DOWNLEFT |\n",
        "\n",
        "\n",
        "| 10 | 11 | 12 | 13 |\n",
        "| --- | --- | --- | --- |\n",
        "| UPFIRE | RIGHTFIRE | LEFTFIRE | DOWNFIRE |\n",
        "\n",
        "\n",
        "| 14 | 15 | 16 | 17 |\n",
        "| --- | --- | --- | --- |\n",
        "| UPRIGHTFIRE | UPLEFTFIRE | DOWNRIGHTFIRE | DOWNLEFTFIRE |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxiF53dtcx5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of trajectories to generate\n",
        "N_OF_TRAJECTORIES = 20\n",
        "\n",
        "# number of steps per trajectory\n",
        "N_OF_STEPS = 1000\n",
        "\n",
        "# # list of string tuples in the format (RL Algorithm, Game Environment)\n",
        "GAMES = [\n",
        "    (\"PPO2\", \"BreakoutNoFrameskip-v4\"),\n",
        "    (\"PPO2\", \"PongNoFrameskip-v4\"),\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xniuUnC1sohU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e6dc3c79-ed1f-4d0d-f447-ad26e5ae669c"
      },
      "source": [
        "[env_id for algo, env_id in GAMES]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['BreakoutNoFrameskip-v4', 'PongNoFrameskip-v4']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OQT9Se_lEWX",
        "colab_type": "text"
      },
      "source": [
        "[RL Baselines Zoo](https://github.com/araffin/rl-baselines-zoo) currently has the following environments with `PPO2` pre-trained agents:  \n",
        "`BeamRiderNoFrameskip-v4`, `BreakoutNoFrameskip-v4`, `EnduroNoFrameskip-v4`, `MsPacmanNoFrameskip-v4`, `PongNoFrameskip-v4`, `QbertNoFrameskip-v4`, `SeaquestNoFrameskip-v4`, `SpaceInvadersNoFrameskip-v4`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANURED63lzYB",
        "colab_type": "text"
      },
      "source": [
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOvHHxnVn5rp",
        "colab_type": "text"
      },
      "source": [
        "Note that we're not installing MPI, so the following algorithms will probably not work: `DDPG`, `GAIL`, `PPO1`, `TRPO`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmjBQE8sl8CP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get update                                                  > /dev/null 2>&1\n",
        "!apt-get install swig cmake zlib1g-dev ffmpeg freeglut3-dev xvfb > /dev/null 2>&1\n",
        "!pip install pyyaml pytablewriter optuna scikit-optimize         > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_p5-8pYmEsa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3aeb5144-03fd-446e-a8e6-a4d884ff383e"
      },
      "source": [
        "#### Stable Baselines only supports TF 1.x for now ####\n",
        "try:\n",
        "    # Colab only\n",
        "    # %tensorflow_version 2.x\n",
        "    %tensorflow_version 1.x\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "print(tf.__version__)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYDsdxKLulZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from time import time\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import gym\n",
        "from gym.envs.atari.atari_env import ACTION_MEANING"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6d58ZxSm7BR",
        "colab_type": "text"
      },
      "source": [
        "### Clone [Stable Baselines](https://github.com/hill-a/stable-baselines) and [RL Zoo Baselines](https://github.com/araffin/rl-baselines-zoo)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Mx9fivzl8KW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6a66ed3f-fa08-45e4-fadd-b278422b1d6b"
      },
      "source": [
        "!pip list | grep baselines"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "stable-baselines         2.2.1      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPOfKIrIm3Ef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!yes | pip uninstall stable-baselines                           > /dev/null 2>&1\n",
        "!pip install git+https://github.com/hill-a/stable-baselines.git > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZG914mxnHN_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "282c1859-2b75-478a-a474-c32cd8cee09e"
      },
      "source": [
        "!pip list | grep baselines"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "stable-baselines         2.9.0a0    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBZSNLAGnpkQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "f3f6f5d1-840d-4435-8e7f-5c9e96109129"
      },
      "source": [
        "!git clone https://github.com/araffin/rl-baselines-zoo.git"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'rl-baselines-zoo'...\n",
            "remote: Enumerating objects: 25, done.\u001b[K\n",
            "remote: Counting objects:   4% (1/25)\u001b[K\rremote: Counting objects:   8% (2/25)\u001b[K\rremote: Counting objects:  12% (3/25)\u001b[K\rremote: Counting objects:  16% (4/25)\u001b[K\rremote: Counting objects:  20% (5/25)\u001b[K\rremote: Counting objects:  24% (6/25)\u001b[K\rremote: Counting objects:  28% (7/25)\u001b[K\rremote: Counting objects:  32% (8/25)\u001b[K\rremote: Counting objects:  36% (9/25)\u001b[K\rremote: Counting objects:  40% (10/25)\u001b[K\rremote: Counting objects:  44% (11/25)\u001b[K\rremote: Counting objects:  48% (12/25)\u001b[K\rremote: Counting objects:  52% (13/25)\u001b[K\rremote: Counting objects:  56% (14/25)\u001b[K\rremote: Counting objects:  60% (15/25)\u001b[K\rremote: Counting objects:  64% (16/25)\u001b[K\rremote: Counting objects:  68% (17/25)\u001b[K\rremote: Counting objects:  72% (18/25)\u001b[K\rremote: Counting objects:  76% (19/25)\u001b[K\rremote: Counting objects:  80% (20/25)\u001b[K\rremote: Counting objects:  84% (21/25)\u001b[K\rremote: Counting objects:  88% (22/25)\u001b[K\rremote: Counting objects:  92% (23/25)\u001b[K\rremote: Counting objects:  96% (24/25)\u001b[K\rremote: Counting objects: 100% (25/25)\u001b[K\rremote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 1539 (delta 8), reused 15 (delta 3), pack-reused 1514\u001b[K\n",
            "Receiving objects: 100% (1539/1539), 375.51 MiB | 30.08 MiB/s, done.\n",
            "Resolving deltas: 100% (891/891), done.\n",
            "Checking out files: 100% (305/305), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULku9CT6ovu2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "19729942-17d8-4f14-a7f8-6a8214758cba"
      },
      "source": [
        "from stable_baselines.common.cmd_util import make_atari_env\n",
        "\n",
        "from stable_baselines.common.vec_env import VecFrameStack, DummyVecEnv\n",
        "\n",
        "# NOTE add more algorithms here\n",
        "from stable_baselines import PPO2, ACER, ACKTR\n",
        "ALGO_IMPL = {\n",
        "    'PPO2': PPO2,\n",
        "    'ACER': ACER,\n",
        "    'ACKTR': ACKTR,\n",
        "}"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZQhTCzfwAZT",
        "colab_type": "text"
      },
      "source": [
        "## Load the pre-trained agents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BukuKWcxxaed",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5cc4a9f8-b2bb-4244-97f5-f3b8e40f79cb"
      },
      "source": [
        "!ls rl-baselines-zoo/trained_agents/"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a2c  acer  acktr  ddpg\tdqn  her  ppo2\tsac  td3  trpo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhFU5ULCxYEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH_TO_AGENTS = os.path.join(\"rl-baselines-zoo\", \"trained_agents\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGcL8fuWUdHT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        },
        "outputId": "0bbca954-70da-474f-9ed6-789cdd3901c3"
      },
      "source": [
        "# check the available pre-trained models\n",
        "algorithms = [\"PPO2\"]\n",
        "for algo in algorithms:\n",
        "    algo_path = os.path.join(PATH_TO_AGENTS, algo.lower())\n",
        "    print(algo_path + '/')\n",
        "    for f in sorted(os.listdir(algo_path), key=lambda x: x[::-1]):\n",
        "        # sort by the reverse filename, so env types get grouped together\n",
        "        if f.endswith(\".pkl\"):\n",
        "            print(\"|___\", f)\n",
        "            # try:\n",
        "            #     model = ALGO_IMPL[algo].load(os.path.join(algo_path, f), verbose=0)\n",
        "            #     print(\"     observation_space:\", model.observation_space)\n",
        "            #     print(\"     action_space:\", model.action_space)\n",
        "            # except:\n",
        "            #     print(\"     ERROR: couldn't load model\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rl-baselines-zoo/trained_agents/ppo2/\n",
            "|___ Pendulum-v0.pkl\n",
            "|___ MountainCar-v0.pkl\n",
            "|___ MountainCarContinuous-v0.pkl\n",
            "|___ MinitaurBulletDuckEnv-v0.pkl\n",
            "|___ Walker2DBulletEnv-v0.pkl\n",
            "|___ HumanoidBulletEnv-v0.pkl\n",
            "|___ HalfCheetahBulletEnv-v0.pkl\n",
            "|___ InvertedDoublePendulumBulletEnv-v0.pkl\n",
            "|___ InvertedPendulumSwingupBulletEnv-v0.pkl\n",
            "|___ ReacherBulletEnv-v0.pkl\n",
            "|___ HopperBulletEnv-v0.pkl\n",
            "|___ MinitaurBulletEnv-v0.pkl\n",
            "|___ AntBulletEnv-v0.pkl\n",
            "|___ CartPole-v1.pkl\n",
            "|___ Acrobot-v1.pkl\n",
            "|___ BipedalWalkerHardcore-v2.pkl\n",
            "|___ LunarLander-v2.pkl\n",
            "|___ BipedalWalker-v2.pkl\n",
            "|___ LunarLanderContinuous-v2.pkl\n",
            "|___ PongNoFrameskip-v4.pkl\n",
            "|___ MsPacmanNoFrameskip-v4.pkl\n",
            "|___ EnduroNoFrameskip-v4.pkl\n",
            "|___ BeamRiderNoFrameskip-v4.pkl\n",
            "|___ SpaceInvadersNoFrameskip-v4.pkl\n",
            "|___ QbertNoFrameskip-v4.pkl\n",
            "|___ SeaquestNoFrameskip-v4.pkl\n",
            "|___ BreakoutNoFrameskip-v4.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5ye3CG3NzJo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674
        },
        "outputId": "d1de490c-d66d-4a09-f648-ae2a085d1724"
      },
      "source": [
        "for algo, env_id in GAMES:\n",
        "    print(f\"('{algo}', '{env_id}')\")\n",
        "    agent_path = os.path.join(PATH_TO_AGENTS, algo.lower(), env_id + '.pkl')\n",
        "    model = ALGO_IMPL[algo].load(agent_path, verbose=0)\n",
        "    print(\"observation_space:\", model.observation_space)\n",
        "    print(\"action_space:\", model.action_space)\n",
        "    print()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('PPO2', 'BreakoutNoFrameskip-v4')\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:58: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:67: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/policies.py:115: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/a2c/utils.py:136: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:194: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:202: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:210: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:244: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:246: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "observation_space: Box(84, 84, 4)\n",
            "action_space: Discrete(4)\n",
            "\n",
            "('PPO2', 'PongNoFrameskip-v4')\n",
            "observation_space: Box(84, 84, 4)\n",
            "action_space: Discrete(6)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHrpWV5vNviz",
        "colab_type": "text"
      },
      "source": [
        "## Generate trajectories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFvNuI8vb75g",
        "colab_type": "text"
      },
      "source": [
        "Note that we use `make_atari_env` + `VecFrameStack` for `NoFrameskip-v4` environments, so each frame is converted to grayscale and downscaled from 210x160 to 84x84. Therefore, the $observation$ shape is `(84, 84, 4)` (four stacked frames), and **not** `(210, 160, 3)`, nor `(84, 84, 1)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2ugXR8sZ6RM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PRINT_EARLY_DONE = False\n",
        "PRINT_ACTIONS_TAKEN = False\n",
        "\n",
        "PRINT_EVERY_N_STEPS = N_OF_STEPS + 1\n",
        "PRINT_EVERY_N_TRAJECTORIES = N_OF_TRAJECTORIES // 10\n",
        "\n",
        "# uncomment below not to print\n",
        "# PRINT_EVERY_N_STEPS = N_OF_STEPS + 1\n",
        "# PRINT_EVERY_N_TRAJECTORIES = N_OF_TRAJECTORIES + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvtLqqVZwFEW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "outputId": "05f51fa0-d146-4cfd-8755-d754f6cd9725"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "time_start = time()\n",
        "print(\"PRINT_EVERY_N_TRAJECTORIES:\", PRINT_EVERY_N_TRAJECTORIES)\n",
        "print(\"N_OF_TRAJECTORIES:\", N_OF_TRAJECTORIES)\n",
        "print(\"N_OF_STEPS:\", N_OF_STEPS)\n",
        "print(\"================\")\n",
        "for algo, env_id in GAMES:\n",
        "    time_start_env = time()\n",
        "\n",
        "    env = make_atari_env(env_id, num_env=1, seed=0)\n",
        "    env = VecFrameStack(env, n_stack=4)\n",
        "    agent_path = os.path.join(PATH_TO_AGENTS, algo.lower(), env_id + '.pkl')\n",
        "    \n",
        "    print(f\"('{algo}', '{env_id}')\")\n",
        "    print(f\"Getting pre-trained agent from: '{agent_path}'\\n\")\n",
        "    \n",
        "    model = ALGO_IMPL[algo].load(agent_path, env)\n",
        "    \n",
        "    for trajectory in tqdm(range(N_OF_TRAJECTORIES), position=0, leave=True):\n",
        "        # store the \"obs -> action\" mapping\n",
        "        observed_states, actions_taken = [], []\n",
        "\n",
        "        obs = env.reset() # (84, 84, 4)\n",
        "        for step in range(N_OF_STEPS):\n",
        "            action = model.predict(obs)\n",
        "            observed_states.append(obs)\n",
        "            actions_taken.append(action)\n",
        "\n",
        "            obs, reward, done, info = env.step(action)\n",
        "            if done:\n",
        "                obs = env.reset()\n",
        "                if PRINT_EARLY_DONE:\n",
        "                    print(f\"done at step {step + 1} (reseting env)\")\n",
        "            \n",
        "            if (step + 1) % PRINT_EVERY_N_STEPS == 0:\n",
        "                print(f\"step {step+1}\")\n",
        "        \n",
        "        # NOTE action, reward and done are arrays since we're using a vectorized env\n",
        "        observed_states = [obs[0] for obs in observed_states]\n",
        "        actions_taken = [action[0][0] for action in actions_taken]\n",
        "        \n",
        "        np.savez_compressed(file=os.path.join(SAVE_DIR, f\"{env_id}_{algo}_t{trajectory+1}_{N_OF_STEPS}s\"), \n",
        "                            observations=observed_states, actions=actions_taken)\n",
        "        \n",
        "        if (trajectory + 1) % 10 == 0:\n",
        "            print(f\"Saved trajectory {trajectory+1} (of {N_OF_TRAJECTORIES})\")\n",
        "\n",
        "        if PRINT_ACTIONS_TAKEN and trajectory == N_OF_TRAJECTORIES - 1:\n",
        "            print(\"\\nActions taken:\", \", \".join([ACTION_MEANING[action] for action in set(actions_taken)]))\n",
        "\n",
        "    del observed_states\n",
        "    del actions_taken\n",
        "    env.close()\n",
        "    print(f\"Δt = {(time() - time_start_env):.2f}s\")\n",
        "    print(\"================\")\n",
        "\n",
        "print(f\"Total Δt = {(time() - time_start):.2f}s\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PRINT_EVERY_N_TRAJECTORIES: 2\n",
            "N_OF_TRAJECTORIES: 20\n",
            "N_OF_STEPS: 1000\n",
            "================\n",
            "('PPO2', 'BreakoutNoFrameskip-v4')\n",
            "Getting pre-trained agent from: 'rl-baselines-zoo/trained_agents/ppo2/BreakoutNoFrameskip-v4.pkl'\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 25%|██▌       | 5/20 [00:22<01:06,  4.41s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-0a6968b2c899>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (84, 84, 4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_OF_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mobserved_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mactions_taken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stable_baselines/common/base_class.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, observation, state, mask, deterministic)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m         \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0mclipped_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stable_baselines/common/policies.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, obs, state, mask, deterministic)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             action, value, neglogp = self.sess.run([self.action, self.value_flat, self.neglogp],\n\u001b[0;32m--> 575\u001b[0;31m                                                    {self.obs_ph: obs})\n\u001b[0m\u001b[1;32m    576\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneglogp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40zclw3dAGcv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trajectory_filenames = []\n",
        "for r, ds, fs in os.walk(SAVE_DIR): # r=root, d=directories, f=files\n",
        "    print(r + '/')\n",
        "    for f in fs:\n",
        "        print(\"|___\", f)\n",
        "        trajectory_filenames.append(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Td2MzGZK_7ht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_trajectory_filename = trajectory_filenames[0]\n",
        "print(f\"Loading from '{test_trajectory_filename}'\\n\")\n",
        "\n",
        "test_trajectory_load = np.load(os.path.join(SAVE_DIR, test_trajectory_filename), \n",
        "                               allow_pickle=True)\n",
        "\n",
        "print(\"observations shape:\", test_trajectory_load['observations'].shape)\n",
        "print(\"actions shape:\", test_trajectory_load['actions'].shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6vHSNX5YFng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/araffin/rl-baselines-zoo/blob/master/utils/record_video.py\n",
        "# https://github.com/araffin/rl-baselines-zoo/blob/master/enjoy.py\n",
        "# https://github.com/hill-a/stable-baselines#try-it-online-with-colab-notebooks-"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-bxg62b_5N7",
        "colab_type": "text"
      },
      "source": [
        "## Old"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ump8v2czCIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def save_as_image(observation, save_dir, img_name, prefix=\"img_\", downscale=False):\n",
        "#     # downscaling the image\n",
        "#     if downscale:\n",
        "#         im_array = cv2.resize(observation, INP_IMAGE_SHAPE) # TODO test tf.image.resize\n",
        "#         im_array = np.array(im_array, dtype='float32')\n",
        "#         im_array = (im_array/127.5) - 1\n",
        "#         im = PIL.Image.fromarray(im_array, 'RGB')\n",
        "#     else:\n",
        "#         try:\n",
        "#             im = PIL.Image.fromarray(observation, 'RGB')\n",
        "#         except:\n",
        "#             print(type(observation))\n",
        "#     imname = \"{}{}.png\".format(prefix, img_name)\n",
        "#     im.save(os.path.join(save_dir, imname))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n74gjFzbzkJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # you can change the default values here\n",
        "# save_dir = SAVE_DIR\n",
        "# num_images = IMAGES_TO_GENERATE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wd6qHz4zyp7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# os.makedirs(save_dir, exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHKIU3Vn0AcL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# envs = [gym.make(env_id) for env_id in ENV_IDS]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z4YJ4XV0HF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for env_id, env in zip(ENV_IDS, envs):\n",
        "#     print(env_id)\n",
        "#     env_dir = os.path.join(save_dir, f\"{env_id}_{IMAGES_TO_GENERATE}\")\n",
        "#     os.makedirs(env_dir, exist_ok=True)\n",
        "    \n",
        "#     env.reset()\n",
        "#     i, current_env_images = 0, 0\n",
        "    \n",
        "#     actions_taken = []\n",
        "#     while i < num_images:\n",
        "#         # take a random action (sampled from the action space)\n",
        "#         action = env.action_space.sample()\n",
        "#         actions_taken.append(action)\n",
        "#         assert 0 <= action < 18, f\"action = {action}\"\n",
        "#         obs, _, done, _ = env.step(action)\n",
        "#         if np.mean(obs) > 0.01:\n",
        "#             save_as_image(obs, env_dir, str(i))\n",
        "#             i += 1\n",
        "#         else:\n",
        "#             print(\"should I have been reached?\")\n",
        "#             continue\n",
        "#         if done:\n",
        "#             print(f\"reseting {env_id} at i={i}\")\n",
        "#             env.reset()\n",
        "    \n",
        "#     actions_taken = np.asarray(actions_taken, dtype='int8')\n",
        "#     print(actions_taken.shape, actions_taken.size, actions_taken.dtype)\n",
        "#     np.save(os.path.join(save_dir, f\"{env_id}_{IMAGES_TO_GENERATE}_actions\"), actions_taken)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NZCKFvS4J3sd",
        "colab": {}
      },
      "source": [
        "# IMG_SIZE = 160 # All images will be resized to 160x160\n",
        "\n",
        "# def load_image(image_path):\n",
        "#     image = tf.io.read_file(image_path)\n",
        "#     image = tf.image.decode_png(image, channels=3)\n",
        "#     image = tf.cast(image, tf.float32)\n",
        "#     image = (image/127.5) - 1\n",
        "#     image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "#     return image, image_path\n",
        "\n",
        "# IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
        "\n",
        "# # Create the base model from the pre-trained model MobileNet V2\n",
        "# base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
        "#                                                include_top=False,\n",
        "#                                                weights='imagenet')\n",
        "\n",
        "# s = time()\n",
        "# # Get unique images\n",
        "# encode_train = img_name_vector\n",
        "\n",
        "# # Feel free to change batch_size according to your system configuration\n",
        "# image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
        "# image_dataset = image_dataset.map(\n",
        "#   load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
        "\n",
        "# print((time()-s)/1000)\n",
        "\n",
        "# for img, path in image_dataset:\n",
        "#   batch_features = image_features_extract_model(img)\n",
        "#   batch_features = tf.reshape(batch_features,\n",
        "#                               (batch_features.shape[0], -1, batch_features.shape[3]))\n",
        "\n",
        "#   for bf, p in zip(batch_features, path):\n",
        "#     path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "#     np.save(path_of_feature, bf.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}