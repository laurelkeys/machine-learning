{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laurelkeys/machine-learning/blob/master/assignment-4/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oYe8NdCrd1Go"
      },
      "source": [
        "# Transformer implementation\n",
        "Replicating the code (and explanations) from [Transformer model for language understanding](https://www.tensorflow.org/tutorials/text/transformer) TensorFlow tutorial, to build upon it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JDgwFv10d1Gp",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E2hTeIwNd1Gr",
        "colab": {}
      },
      "source": [
        "import warnings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IU8Ebn6w2qm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import functools\n",
        "\n",
        "SPACES = 0\n",
        "def indent_prints(func):\n",
        "    @functools.wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        global SPACES\n",
        "        SPACES += 2\n",
        "        try:\n",
        "            ret = func(*args, **kwargs)\n",
        "        except:\n",
        "            ret = func(*args)\n",
        "        SPACES -= 2\n",
        "        return ret\n",
        "    return wrapper"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nHfZoVUBd1Gs"
      },
      "source": [
        "## Positional encoding\n",
        "\n",
        "Since this model doesn't contain any recurrence or convolution, positional encoding is added to give the model some information about the relative position of the words in the sentence. \n",
        "\n",
        "The positional encoding vector is added to the embedding vector. Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other. But the embeddings do not encode the relative position of words in a sentence. So after adding the positional encoding, words will be closer to each other based on the *similarity of their meaning and their position in the sentence*, in the d-dimensional space.\n",
        "\n",
        "See the notebook on [positional encoding](https://github.com/tensorflow/examples/blob/master/community/en/position_encoding.ipynb) to learn more about it. The formula for calculating the positional encoding is as follows:\n",
        "\n",
        "${PE_{(pos,\\,2i)} = sin(pos / 10000^{2i / d_{model}})}\\,,\\;\\;\\;{PE_{(pos,\\,2i+1)} = cos(pos / 10000^{2i / d_{model}})} $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uknPVZ5fd1Gt",
        "colab": {}
      },
      "source": [
        "@indent_prints\n",
        "def get_angles(pos, i, d_model):\n",
        "    print(\" \"*SPACES, \"[get_angles] pos\", pos.shape)\n",
        "    print(\" \"*SPACES, \"[get_angles] i\", i.shape)\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RfpHkrmfd1Gu",
        "colab": {}
      },
      "source": [
        "@indent_prints\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                            np.arange(d_model)[np.newaxis, :],\n",
        "                            d_model)\n",
        "    print(\" \"*SPACES, \"[positional_encoding] angle_rads\", angle_rads.shape)\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6_kBBffqd1Gw"
      },
      "source": [
        "## Masking\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MVpIOWgnd1Gx",
        "colab": {}
      },
      "source": [
        "#    atari image    ==>   resized image   ==MobileNetV2==>  feature vector  ==GlobalAveragePooling2D==>   token\n",
        "# (bs, 210, 160, 3) ==> (bs, 160, 160, 3) ==MobileNetV2==> (bs, 5, 5, 1280) ==GlobalAveragePooling2D==> (bs, 1280)\n",
        "\n",
        "# (bs, input_seq_len, d_model) ==Transformer==> ...\n",
        "# bs            : batch_size\n",
        "# input_seq_len : 1000 for training (could be less if we want to use it as a policy network, predicting actions on-the-fly)\n",
        "# d_model       : 1280 (MobileNetV2 output with GlobalAveragePooling2D)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M04IJeold1Gz"
      },
      "source": [
        "Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value `PAD_VALUE` is present: it outputs a `1` at those locations, and a `0` otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tUVotvatd1Gz",
        "colab": {}
      },
      "source": [
        "PAD_VALUE = 0 # FIXME\n",
        "\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, PAD_VALUE), tf.float32)\n",
        "\n",
        "    # add extra dimensions to add the padding to the attention logits\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :] # (batch_size, 1, 1, seq_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KSDkJ8Ytd1G2"
      },
      "source": [
        "The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.\n",
        "\n",
        "This means that to predict the third token, only the first and second tokens will be used. Similarly to predict the fourth token, only the first, second and the third tokens will be used and so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V16PfxiNd1G3",
        "colab": {}
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask # (seq_len, seq_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "48c717e1-549b-487c-e0b6-01fc5ee53719",
        "id": "oQuL9bu3d1G5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "create_look_ahead_mask(size=4)"
      ],
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=372780, shape=(4, 4), dtype=float32, numpy=\n",
              "array([[0., 1., 1., 1.],\n",
              "       [0., 0., 1., 1.],\n",
              "       [0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 246
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AJoN2BYkd1G8"
      },
      "source": [
        "## Scaled dot product attention\n",
        "The attention function used by the transformer takes three inputs: Q (query), K (key), V (value). The equation used to calculate the attention weights is: ${Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V} $\n",
        "\n",
        "The mask is multiplied by $-10^9$ and summed with the scaled matrix multiplication of Q and K, and is applied immediately before a softmax. The goal is to zero out these cells, as large negative inputs to softmax are near zero in the output.\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/scaled_attention.png\" width=\"300\" alt=\"scaled_dot_product_attention\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-V7GDux5d1G8",
        "colab": {}
      },
      "source": [
        "@indent_prints\n",
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "    ''' Calculate Attention(q, k, v) and the attention weights.\n",
        "        Args:\n",
        "          q: query shape == (..., seq_len_q, depth)\n",
        "          k: key shape   == (..., seq_len_k, depth)\n",
        "          v: value shape == (..., seq_len_v, depth_v)\n",
        "          mask: float tensor with shape broadcastable to (..., seq_len_q, seq_len_k)\n",
        "        Returns:\n",
        "          output, attention_weights '''\n",
        "    print(\" \"*SPACES, \"[scaled_dot_product_attention]\")\n",
        "    print(\" \"*SPACES, \"- q\", q.shape)\n",
        "    print(\" \"*SPACES, \"- k\", k.shape)\n",
        "    print(\" \"*SPACES, \"- v\", v.shape)\n",
        "    \n",
        "    # assert q.shape[:-2] == k.shape[:-2] == v.shape[:-2], \"q, k, v must have matching leading dimensions\"\n",
        "    # assert k.shape[-2] == v.shape[-2], \"k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v\"\n",
        "\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True) # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    # scale matmul_qk\n",
        "    d_k = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(d_k)\n",
        "\n",
        "    # add the (padding or look ahead) mask to the scaled tensor\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1) # (..., seq_len_q, seq_len_k)\n",
        "    \n",
        "    output = tf.matmul(attention_weights, v) # (..., seq_len_q, depth_v)\n",
        "    \n",
        "    return output, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4kurS93xd1G-"
      },
      "source": [
        "As the softmax normalization is done on K, its values decide the amount of importance given to Q.\n",
        "\n",
        "The output represents the multiplication of the attention weights and the V (value) vector. This ensures that the tokens you want to focus on are kept as-is and the irrelevant tokens are flushed out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ls2-na9Ad1G_"
      },
      "source": [
        "### Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F9OaPjnod1HA",
        "colab": {}
      },
      "source": [
        "def print_out(q, k, v):\n",
        "    temp_out, temp_attn = scaled_dot_product_attention(q, k, v, None)\n",
        "    print('Attention weights are:', temp_attn)\n",
        "    print('Output is:            ', temp_out)\n",
        "np.set_printoptions(suppress=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6_AOyQVhd1HC",
        "colab": {}
      },
      "source": [
        "# def print_out_shape(q, k, v):\n",
        "#     temp_out, temp_attn = scaled_dot_product_attention(q, k, v, None)\n",
        "#     print('Attention weights are:', temp_attn.shape)\n",
        "#     print('Output is:            ', temp_out.shape)\n",
        "\n",
        "# print_out_shape(\n",
        "#     q=tf.random.uniform((64, 10, 24 // 8)), # (batch_size, seq_len, d_model_q)\n",
        "#     k=tf.random.uniform((64, 10, 1280 // 8)), # (batch_size, seq_len, d_model)\n",
        "#     v=tf.random.uniform((64, 10, 1280 // 8))) # (batch_size, seq_len, d_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-Fz7goOld1HE",
        "colab": {}
      },
      "source": [
        "temp_k = tf.constant([[10,0,0], [0,10,0], [0,0,10], [0,0,10]], dtype=tf.float32) # (4, 3)\n",
        "\n",
        "temp_v = tf.constant([[   1,0], [  10,0], [ 100,5], [1000,6]], dtype=tf.float32) # (4, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5585d631-4266-4df8-b785-1af0baa67d7d",
        "id": "TBFwYzG6d1HF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "# This query aligns with the second key, so the second value is returned\n",
        "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32) # (1, 3)\n",
        "print_out(temp_q, temp_k, temp_v)"
      ],
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   [scaled_dot_product_attention]\n",
            "   - q (1, 3)\n",
            "   - k (4, 3)\n",
            "   - v (4, 2)\n",
            "Attention weights are: tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
            "Output is:             tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ed3481db-01a3-43c4-86bf-8c0bf543ce81",
        "id": "Y5AhsftWd1HH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "# This query aligns with a repeated key (third and fourth), so all associated values get averaged\n",
        "temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32) # (1, 3)\n",
        "print_out(temp_q, temp_k, temp_v)"
      ],
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   [scaled_dot_product_attention]\n",
            "   - q (1, 3)\n",
            "   - k (4, 3)\n",
            "   - v (4, 2)\n",
            "Attention weights are: tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n",
            "Output is:             tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "560fbff5-67c4-471e-8dbe-fd088dfe5134",
        "id": "9O99UDXjd1HI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "# This query aligns equally with the first and second key, so their values get averaged\n",
        "temp_q = tf.constant([[10, 10, 0]], dtype=tf.float32) # (1, 3)\n",
        "print_out(temp_q, temp_k, temp_v)"
      ],
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   [scaled_dot_product_attention]\n",
            "   - q (1, 3)\n",
            "   - k (4, 3)\n",
            "   - v (4, 2)\n",
            "Attention weights are: tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)\n",
            "Output is:             tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7436bd4a-2358-477b-d92f-9553c940a9be",
        "id": "__k-CaBtd1HK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "# Pass all the queries together\n",
        "temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32) # (3, 3)\n",
        "print_out(temp_q, temp_k, temp_v)"
      ],
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   [scaled_dot_product_attention]\n",
            "   - q (3, 3)\n",
            "   - k (4, 3)\n",
            "   - v (4, 2)\n",
            "Attention weights are: tf.Tensor(\n",
            "[[0.  0.  0.5 0.5]\n",
            " [0.  1.  0.  0. ]\n",
            " [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n",
            "Output is:             tf.Tensor(\n",
            "[[550.    5.5]\n",
            " [ 10.    0. ]\n",
            " [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UkEVCebUd1HM"
      },
      "source": [
        "## Multi-head attention\n",
        "Multi-head attention consists of four parts:\n",
        "* Linear layers and split into heads\n",
        "* Scaled dot-product attention\n",
        "* Concatenation of heads\n",
        "* Final linear layer\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/multi_head_attention.png\" width=\"300\" alt=\"multi-head attention\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KFiZ8_X8d1HN"
      },
      "source": [
        "Each multi-head attention block gets three inputs; Q (query), K (key), and V (value). These are put through linear (Dense) layers and split up into multiple heads. \n",
        "\n",
        "The `scaled_dot_product_attention` defined above is applied to each head (broadcasted for efficiency). An appropriate mask must be used in the attention step.  The attention output for each head is then concatenated (using `tf.transpose`, and `tf.reshape`) and put through a final `Dense` layer.\n",
        "\n",
        "Instead of one single attention head, Q, K, and V are split into multiple heads because it allows the model to jointly attend to information at different positions from different representational spaces. After the split each head has a reduced dimensionality, so the total computation cost is the same as a single head attention with full dimensionality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zQk5pe5kd1HN",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.depth = d_model // num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        # NOTE I think this is d_model_q (and not d_model)\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "    \n",
        "    @indent_prints\n",
        "    def split_heads(self, x, batch_size):\n",
        "        ''' Split the last dimension into (num_heads, depth).\n",
        "            Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth) '''\n",
        "        print(\" \"*SPACES, \"[split_heads]\")\n",
        "        print(\" \"*SPACES,\"- batch_size\", batch_size)\n",
        "        print(\" \"*SPACES,\"- x\", x.shape)\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        print(\" \"*SPACES,\"- x (reshape)\", x.shape)\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    @indent_prints\n",
        "    def call(self, v, k, q, mask=None):\n",
        "        print(\" \"*SPACES, \"[MultiHeadAttention.call]\")\n",
        "        print(\" \"*SPACES,\"- v\", v.shape)\n",
        "        print(\" \"*SPACES,\"- k\", k.shape)\n",
        "        print(\" \"*SPACES,\"- q\", q.shape)\n",
        "        batch_size = tf.shape(q)[0]\n",
        "        \n",
        "        q = self.wq(q) # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k) # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v) # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # obs.: seq_len_k == seq_len_v\n",
        "        q = self.split_heads(q, batch_size)   # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)   # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)   # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # attention.shape         == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "\n",
        "        print(\" \"*SPACES,\"- scaled_dot_product_attention\", attention.shape, attention_weights.shape)\n",
        "\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) # (batch_size, seq_len_q, num_heads, depth)\n",
        "        # NOTE I think this is d_model (and not d_model)\n",
        "        print(\" \"*SPACES,\"- attention (transposed)\", attention.shape)\n",
        "        concat_attention = tf.reshape(attention, (batch_size, -1, self.d_model)) # (batch_size, seq_len_q, d_model)\n",
        "        print(\" \"*SPACES,\"- concat_attention\", concat_attention.shape)\n",
        "        output = self.dense(concat_attention) # (batch_size, seq_len_q, d_model)\n",
        "        print(\" \"*SPACES,\"- output\", concat_attention.shape)\n",
        "\n",
        "        return output, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "31eaa8fd-58f4-4a62-f645-5e226ac03b3c",
        "id": "G17zkLqQd1HP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "source": [
        "sample_multiheadattention_layer = MultiHeadAttention(24, 8)\n",
        "\n",
        "x_sample = tf.random.uniform((64, 100, 24))\n",
        "sample_multiheadattention_layer_output, _ = sample_multiheadattention_layer(\n",
        "    x_sample, x_sample, x_sample, None)\n",
        "\n",
        "sample_multiheadattention_layer_output.shape # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   [MultiHeadAttention.call]\n",
            "   - v (64, 100, 24)\n",
            "   - k (64, 100, 24)\n",
            "   - q (64, 100, 24)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(64, shape=(), dtype=int32)\n",
            "     - x (64, 100, 24)\n",
            "     - x (reshape) (64, 100, 8, 3)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(64, shape=(), dtype=int32)\n",
            "     - x (64, 100, 24)\n",
            "     - x (reshape) (64, 100, 8, 3)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(64, shape=(), dtype=int32)\n",
            "     - x (64, 100, 24)\n",
            "     - x (reshape) (64, 100, 8, 3)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (64, 8, 100, 3)\n",
            "     - k (64, 8, 100, 3)\n",
            "     - v (64, 8, 100, 3)\n",
            "   - scaled_dot_product_attention (64, 8, 100, 3) (64, 8, 100, 100)\n",
            "   - attention (transposed) (64, 100, 8, 3)\n",
            "   - concat_attention (64, 100, 24)\n",
            "   - output (64, 100, 24)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 100, 24])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6bc731d9-6f39-4971-95a3-d911e32811fe",
        "id": "a_wrm5Kgd1HR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "source": [
        "sample_multiheadattention_layer = MultiHeadAttention(24, 8)\n",
        "\n",
        "x_sample = tf.random.uniform((64, 100, 24))\n",
        "y_sample = tf.random.uniform((64, 100, 1280))\n",
        "sample_multiheadattention_layer_output, _ = sample_multiheadattention_layer(\n",
        "    y_sample, y_sample, x_sample, None)\n",
        "\n",
        "sample_multiheadattention_layer_output.shape # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   [MultiHeadAttention.call]\n",
            "   - v (64, 100, 1280)\n",
            "   - k (64, 100, 1280)\n",
            "   - q (64, 100, 24)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(64, shape=(), dtype=int32)\n",
            "     - x (64, 100, 24)\n",
            "     - x (reshape) (64, 100, 8, 3)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(64, shape=(), dtype=int32)\n",
            "     - x (64, 100, 24)\n",
            "     - x (reshape) (64, 100, 8, 3)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(64, shape=(), dtype=int32)\n",
            "     - x (64, 100, 24)\n",
            "     - x (reshape) (64, 100, 8, 3)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (64, 8, 100, 3)\n",
            "     - k (64, 8, 100, 3)\n",
            "     - v (64, 8, 100, 3)\n",
            "   - scaled_dot_product_attention (64, 8, 100, 3) (64, 8, 100, 100)\n",
            "   - attention (transposed) (64, 100, 8, 3)\n",
            "   - concat_attention (64, 100, 24)\n",
            "   - output (64, 100, 24)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 100, 24])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 257
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7m8KJMowd1HU"
      },
      "source": [
        "## Point wise feed forward network\n",
        "Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zz6xfVDqd1HU",
        "colab": {}
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'), # (batch_size, seq_len, dff)\n",
        "        tf.keras.layers.Dense(d_model) # (batch_size, seq_len, d_model)\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vPGiNbGRd1HV"
      },
      "source": [
        "## Encoder and decoder\n",
        "* The input sentence is passed through N encoder layers that generates an output for each token in the sequence.\n",
        "* The decoder attends on the encoder's output and its own input (self-attention) to predict the next token.\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\" width=\"450\" alt=\"transformer\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T6B91Jcrd1HW"
      },
      "source": [
        "### Encoder layer\n",
        "Each encoder layer consists of sublayers:\n",
        "1. Multi-head attention \n",
        "2. Point wise feed forward networks. \n",
        "\n",
        "Each of these sublayers has a residual connection around it followed by a layer normalization. Residual connections help in avoiding the vanishing gradient problem in deep networks.\n",
        "\n",
        "The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis. There are N encoder layers in the transformer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GaNXB5Dkd1HW",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        # x.shape == (batch_size, input_seq_len, d_model)\n",
        "        attn_output, _ = self.mha(x, x, x, mask) # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output) # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out1) # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output) # (batch_size, input_seq_len, d_model)\n",
        "        \n",
        "        return out2 # shape same as x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d210c6c1-6232-4032-d8c3-aaad85ba08ad",
        "id": "-QKXsuSJd1HX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "source": [
        "sample_encoder_layer = EncoderLayer(1280, 8, 256)\n",
        "\n",
        "sample_encoder_layer_output = sample_encoder_layer(\n",
        "    tf.random.uniform((32, 100, 1280)), False, None)\n",
        "\n",
        "sample_encoder_layer_output.shape # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   [MultiHeadAttention.call]\n",
            "   - v (32, 100, 1280)\n",
            "   - k (32, 100, 1280)\n",
            "   - q (32, 100, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(32, shape=(), dtype=int32)\n",
            "     - x (32, 100, 1280)\n",
            "     - x (reshape) (32, 100, 8, 160)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(32, shape=(), dtype=int32)\n",
            "     - x (32, 100, 1280)\n",
            "     - x (reshape) (32, 100, 8, 160)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(32, shape=(), dtype=int32)\n",
            "     - x (32, 100, 1280)\n",
            "     - x (reshape) (32, 100, 8, 160)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (32, 8, 100, 160)\n",
            "     - k (32, 8, 100, 160)\n",
            "     - v (32, 8, 100, 160)\n",
            "   - scaled_dot_product_attention (32, 8, 100, 160) (32, 8, 100, 100)\n",
            "   - attention (transposed) (32, 100, 8, 160)\n",
            "   - concat_attention (32, 100, 1280)\n",
            "   - output (32, 100, 1280)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([32, 100, 1280])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 260
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8SXlziaSd1HY"
      },
      "source": [
        "### Decoder layer\n",
        "Each decoder layer consists of sublayers:\n",
        "1. Masked multi-head attention (with look ahead mask and padding mask)\n",
        "2. Multi-head attention (with padding mask)  \n",
        "   V (value) and K (key) receive the *encoder output* as inputs  \n",
        "   Q (query) receives the *output from the masked multi-head attention sublayer*\n",
        "3. Point wise feed forward networks\n",
        "\n",
        "Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis.\n",
        "\n",
        "There are N decoder layers in the transformer.\n",
        "\n",
        "As Q receives the output from decoder's first attention block, and K receives the encoder output, the attention weights represent the importance given to the decoder's input based on the encoder's output. In other words, the decoder predicts the next word by looking at the encoder output and self-attending to its own output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4kaBO2lPd1Ha",
        "colab": {}
      },
      "source": [
        "# d_model == 1280 (comes from the encoder, i.e. images' feature vectors)\n",
        "# d_model_q == 18 (comes from the previous decoder output, i.e. actions)\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1, d_model_q=None):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        warnings.warn(f\"d_model_q must be 18 as we have actions from 0 to 17 (not {d_model_q})\")\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model_q, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model_q, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model_q, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)    \n",
        "    \n",
        "    @indent_prints\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        # x == (batch_size, target_seq_len, d_model_q)\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "        print(\" \"*SPACES, \"[DecoderLayer.call]\")\n",
        "        print(\" \"*SPACES, \"- x\", x.shape)\n",
        "        print(\" \"*SPACES, \"- enc_output\", enc_output.shape)\n",
        "\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask) # (batch_size, target_seq_len, d_model_q)\n",
        "        print(\" \"*SPACES, \"- attn1\", attn1.shape)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "        print(\" \"*SPACES, \"- out1\", out1.shape)\n",
        "\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask) # (batch_size, target_seq_len, d_model_q)\n",
        "        print(\" \"*SPACES, \"- attn2\", attn2.shape)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1) # (batch_size, target_seq_len, d_model_q)\n",
        "        print(\" \"*SPACES, \"- out2\", out1.shape)\n",
        "\n",
        "        ffn_output = self.ffn(out2) # (batch_size, target_seq_len, d_model_q)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2) # (batch_size, target_seq_len, d_model_q)\n",
        "        print(\" \"*SPACES, \"- out3\", out1.shape)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "52848fbf-88de-43c0-867a-912bb460e5c7",
        "id": "w1YDi_nVd1Hd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sample_decoder_layer = DecoderLayer(1280, 8, 256, d_model_q=24) # 24 since 18 isn't divisible by 8\n",
        "\n",
        "sample_decoder_layer_output, _, _ = sample_decoder_layer(tf.random.uniform((32, 100, 24)), \n",
        "                                                         sample_encoder_layer_output, False, None, None)\n",
        "\n",
        "sample_decoder_layer_output.shape # (batch_size, target_seq_len, d_model)"
      ],
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   [DecoderLayer.call]\n",
            "   - x (32, 100, 24)\n",
            "   - enc_output (32, 100, 1280)\n",
            "     [MultiHeadAttention.call]\n",
            "     - v (32, 100, 24)\n",
            "     - k (32, 100, 24)\n",
            "     - q (32, 100, 24)\n",
            "       [split_heads]\n",
            "       - batch_size tf.Tensor(32, shape=(), dtype=int32)\n",
            "       - x (32, 100, 24)\n",
            "       - x (reshape) (32, 100, 8, 3)\n",
            "       [split_heads]\n",
            "       - batch_size tf.Tensor(32, shape=(), dtype=int32)\n",
            "       - x (32, 100, 24)\n",
            "       - x (reshape) (32, 100, 8, 3)\n",
            "       [split_heads]\n",
            "       - batch_size tf.Tensor(32, shape=(), dtype=int32)\n",
            "       - x (32, 100, 24)\n",
            "       - x (reshape) (32, 100, 8, 3)\n",
            "       [scaled_dot_product_attention]\n",
            "       - q (32, 8, 100, 3)\n",
            "       - k (32, 8, 100, 3)\n",
            "       - v (32, 8, 100, 3)\n",
            "     - scaled_dot_product_attention (32, 8, 100, 3) (32, 8, 100, 100)\n",
            "     - attention (transposed) (32, 100, 8, 3)\n",
            "     - concat_attention (32, 100, 24)\n",
            "     - output (32, 100, 24)\n",
            "   - attn1 (32, 100, 24)\n",
            "   - out1 (32, 100, 24)\n",
            "     [MultiHeadAttention.call]\n",
            "     - v (32, 100, 1280)\n",
            "     - k (32, 100, 1280)\n",
            "     - q (32, 100, 24)\n",
            "       [split_heads]\n",
            "       - batch_size tf.Tensor(32, shape=(), dtype=int32)\n",
            "       - x (32, 100, 24)\n",
            "       - x (reshape) (32, 100, 8, 3)\n",
            "       [split_heads]\n",
            "       - batch_size tf.Tensor(32, shape=(), dtype=int32)\n",
            "       - x (32, 100, 24)\n",
            "       - x (reshape) (32, 100, 8, 3)\n",
            "       [split_heads]\n",
            "       - batch_size tf.Tensor(32, shape=(), dtype=int32)\n",
            "       - x (32, 100, 24)\n",
            "       - x (reshape) (32, 100, 8, 3)\n",
            "       [scaled_dot_product_attention]\n",
            "       - q (32, 8, 100, 3)\n",
            "       - k (32, 8, 100, 3)\n",
            "       - v (32, 8, 100, 3)\n",
            "     - scaled_dot_product_attention (32, 8, 100, 3) (32, 8, 100, 100)\n",
            "     - attention (transposed) (32, 100, 8, 3)\n",
            "     - concat_attention (32, 100, 24)\n",
            "     - output (32, 100, 24)\n",
            "   - attn2 (32, 100, 24)\n",
            "   - out2 (32, 100, 24)\n",
            "   - out3 (32, 100, 24)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: d_model_q must be 18 as we have actions from 0 to 17 (not 24)\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([32, 100, 24])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 262
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i-Ofc4dId1He"
      },
      "source": [
        "### Encoder\n",
        "The `Encoder` consists of:\n",
        "1. Input Embedding\n",
        "2. Positional Encoding\n",
        "3. N encoder layers\n",
        "\n",
        "The input is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the encoder layers. The output of the encoder is the input to the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L9IwuNmwd1He",
        "colab": {}
      },
      "source": [
        "# NOTE replacing Embedding inside the Encoder with MobileNetV2 + GlobalAveragePooling2D\n",
        "# The output should have shape == (batch_size, input_seq_len, d_model), where d_model will be 1280 as we're using MobileNetV2\n",
        "class FeatureExtractor(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "\n",
        "        self.mobile_net_v2 = tf.keras.applications.MobileNetV2(input_shape=(160, 160, 3), \n",
        "                                                               include_top=False, \n",
        "                                                               weights='imagenet')\n",
        "        self.mobile_net_v2.trainable = False\n",
        "        \n",
        "        self.global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "        \n",
        "    @indent_prints\n",
        "    def call(self, x):\n",
        "        # reshaping (batch_size, input_seq_len, 160, 160, 3) to (batch_size * input_seq_len, 160, 160, 3)\n",
        "        print(\" \"*SPACES, \"[FeatureExtractor.call]\")\n",
        "        print(\" \"*SPACES, \"- x\", x.shape)\n",
        "        x_shape = tf.shape(x)\n",
        "        x = tf.reshape(x, (-1, x_shape[2], x_shape[3], x_shape[4]))\n",
        "        print(\" \"*SPACES, \"- x (reshaped)\", x.shape)\n",
        "        x = self.mobile_net_v2(x) # (batch_size * input_seq_len, 5, 5, 1280)\n",
        "        print(\" \"*SPACES, \"- x (mobile_net_v2)\", x.shape)\n",
        "\n",
        "        x = self.global_average_layer(x) # (batch_size * input_seq_len, 1280)\n",
        "        print(\" \"*SPACES, \"- x (global_average_layer)\", x.shape)\n",
        "        x = tf.reshape(x, (x_shape[0], x_shape[1], -1))\n",
        "        print(\" \"*SPACES, \"- x (reshaped)\", x.shape)\n",
        "        return x # (batch_size, input_seq_len, d_model), with d_model == 1280"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZA8qsoeaKd9",
        "colab_type": "code",
        "outputId": "c7498853-21fe-44a4-af67-5f2f5a745e65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "sample_feature_extractor = FeatureExtractor()\n",
        "\n",
        "temp_input = tf.random.uniform((2, 10, 160, 160, 3), dtype=tf.float32, minval=-1, maxval=1)\n",
        "\n",
        "sample_feature_extractor_output = sample_feature_extractor(temp_input)\n",
        "\n",
        "print (sample_feature_extractor_output.shape) # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   [FeatureExtractor.call]\n",
            "   - x (2, 10, 160, 160, 3)\n",
            "   - x (reshaped) (20, 160, 160, 3)\n",
            "   - x (mobile_net_v2) (20, 5, 5, 1280)\n",
            "   - x (global_average_layer) (20, 1280)\n",
            "   - x (reshaped) (2, 10, 1280)\n",
            "(2, 10, 1280)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x-eKQZwud1Hj",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "                 maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        assert d_model == 1280, \"d_model must be 1280 as we're using MobileNetV2\"\n",
        "        warnings.warn(f\"input_vocab_size is no longer used (its value of {input_vocab_size} will be ignored)\")\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # NOTE replacing Embedding with MobileNetV2 + GlobalAveragePooling2D\n",
        "        # > self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.feature_extractor = FeatureExtractor()\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def call(self, x, training, mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        # adding embedding and position encoding        \n",
        "        # NOTE replacing Embedding with MobileNetV2 + GlobalAveragePooling2D\n",
        "        # > x = self.embedding(x) # (batch_size, input_seq_len, d_model)\n",
        "        x = self.feature_extractor(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "        return x # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c2cffca6-dcc4-4910-daed-362061c8cf5b",
        "id": "f8kzWhead1Hl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sample_encoder = Encoder(num_layers=2, d_model=1280, num_heads=8, \n",
        "                         dff=256, input_vocab_size=None,\n",
        "                         maximum_position_encoding=10000)\n",
        "\n",
        "temp_input = tf.random.uniform((8, 10, 160, 160, 3), dtype=tf.float32, minval=-1, maxval=1)\n",
        "\n",
        "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
        "\n",
        "print (sample_encoder_output.shape) # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: input_vocab_size is no longer used (its value of None will be ignored)\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "     [get_angles] pos (10000, 1)\n",
            "     [get_angles] i (1, 1280)\n",
            "   [positional_encoding] angle_rads (10000, 1280)\n",
            "   [FeatureExtractor.call]\n",
            "   - x (8, 10, 160, 160, 3)\n",
            "   - x (reshaped) (80, 160, 160, 3)\n",
            "   - x (mobile_net_v2) (80, 5, 5, 1280)\n",
            "   - x (global_average_layer) (80, 1280)\n",
            "   - x (reshaped) (8, 10, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (8, 10, 1280)\n",
            "   - k (8, 10, 1280)\n",
            "   - q (8, 10, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(8, shape=(), dtype=int32)\n",
            "     - x (8, 10, 1280)\n",
            "     - x (reshape) (8, 10, 8, 160)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(8, shape=(), dtype=int32)\n",
            "     - x (8, 10, 1280)\n",
            "     - x (reshape) (8, 10, 8, 160)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(8, shape=(), dtype=int32)\n",
            "     - x (8, 10, 1280)\n",
            "     - x (reshape) (8, 10, 8, 160)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (8, 8, 10, 160)\n",
            "     - k (8, 8, 10, 160)\n",
            "     - v (8, 8, 10, 160)\n",
            "   - scaled_dot_product_attention (8, 8, 10, 160) (8, 8, 10, 10)\n",
            "   - attention (transposed) (8, 10, 8, 160)\n",
            "   - concat_attention (8, 10, 1280)\n",
            "   - output (8, 10, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (8, 10, 1280)\n",
            "   - k (8, 10, 1280)\n",
            "   - q (8, 10, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(8, shape=(), dtype=int32)\n",
            "     - x (8, 10, 1280)\n",
            "     - x (reshape) (8, 10, 8, 160)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(8, shape=(), dtype=int32)\n",
            "     - x (8, 10, 1280)\n",
            "     - x (reshape) (8, 10, 8, 160)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(8, shape=(), dtype=int32)\n",
            "     - x (8, 10, 1280)\n",
            "     - x (reshape) (8, 10, 8, 160)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (8, 8, 10, 160)\n",
            "     - k (8, 8, 10, 160)\n",
            "     - v (8, 8, 10, 160)\n",
            "   - scaled_dot_product_attention (8, 8, 10, 160) (8, 8, 10, 10)\n",
            "   - attention (transposed) (8, 10, 8, 160)\n",
            "   - concat_attention (8, 10, 1280)\n",
            "   - output (8, 10, 1280)\n",
            "(8, 10, 1280)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PocXG-9Fd1Hn"
      },
      "source": [
        "### Decoder\n",
        "The `Decoder` consists of:\n",
        "1. Output Embedding\n",
        "2. Positional Encoding\n",
        "3. N decoder layers\n",
        "\n",
        "The target is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the decoder layers. The output of the decoder is the input to the final linear layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SQ352nDod1Ho",
        "colab": {}
      },
      "source": [
        "# d_model == 1280 (comes from the encoder, i.e. images' feature vectors)\n",
        "# d_model_q == 18 (comes from the previous decoder output, i.e. actions)\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "                 maximum_position_encoding, rate=0.1, d_model_q=None):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        warnings.warn(f\"d_model_q must be 18 as we have actions from 0 to 17 (not {d_model_q})\")\n",
        "        self.d_model_q = d_model_q\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model_q)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model_q)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate, d_model_q) for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    @indent_prints\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        print(\" \"*SPACES, \"[Decoder.call]\")\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        try:\n",
        "            print(\" \"*SPACES, \"- x\", x.shape)\n",
        "        except:\n",
        "            print(\" \"*SPACES, \"- len(x)\", len(x))\n",
        "        x = self.embedding(x) # (batch_size, target_seq_len, d_model_q)\n",
        "        print(\" \"*SPACES, \"- embedding\", x.shape)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        print(\" \"*SPACES, \"- pos_encoding\", x.shape)\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "\n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5e371611-296d-464c-9e47-eac6808a91d1",
        "id": "hBa1OdjLd1Hq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sample_decoder = Decoder(num_layers=2, d_model=1280, num_heads=8, \n",
        "                         dff=256, target_vocab_size=24, \n",
        "                         maximum_position_encoding=1000, \n",
        "                         d_model_q=24)\n",
        "temp_input = tf.random.uniform((8, 10), dtype=tf.int64, minval=0, maxval=18)\n",
        "\n",
        "output, attn = sample_decoder(temp_input, \n",
        "                              enc_output=sample_encoder_output, \n",
        "                              training=False,\n",
        "                              look_ahead_mask=None, \n",
        "                              padding_mask=None)\n",
        "\n",
        "output.shape, attn['decoder_layer2_block2'].shape"
      ],
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     [get_angles] pos (1000, 1)\n",
            "     [get_angles] i (1, 24)\n",
            "   [positional_encoding] angle_rads (1000, 24)\n",
            "   [Decoder.call]\n",
            "   - x (8, 10)\n",
            "   - embedding (8, 10, 24)\n",
            "   - pos_encoding (8, 10, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (8, 10, 24)\n",
            "     - enc_output (8, 10, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (8, 10, 24)\n",
            "       - k (8, 10, 24)\n",
            "       - q (8, 10, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(8, shape=(), dtype=int32)\n",
            "         - x (8, 10, 24)\n",
            "         - x (reshape) (8, 10, 8, 3)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(8, shape=(), dtype=int32)\n",
            "         - x (8, 10, 24)\n",
            "         - x (reshape) (8, 10, 8, 3)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(8, shape=(), dtype=int32)\n",
            "         - x (8, 10, 24)\n",
            "         - x (reshape) (8, 10, 8, 3)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (8, 8, 10, 3)\n",
            "         - k (8, 8, 10, 3)\n",
            "         - v (8, 8, 10, 3)\n",
            "       - scaled_dot_product_attention (8, 8, 10, 3) (8, 8, 10, 10)\n",
            "       - attention (transposed) (8, 10, 8, 3)\n",
            "       - concat_attention (8, 10, 24)\n",
            "       - output (8, 10, 24)\n",
            "     - attn1 (8, 10, 24)\n",
            "     - out1 (8, 10, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (8, 10, 1280)\n",
            "       - k (8, 10, 1280)\n",
            "       - q (8, 10, 24)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: d_model_q must be 18 as we have actions from 0 to 17 (not 24)\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: d_model_q must be 18 as we have actions from 0 to 17 (not 24)\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(8, shape=(), dtype=int32)\n",
            "         - x (8, 10, 24)\n",
            "         - x (reshape) (8, 10, 8, 3)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(8, shape=(), dtype=int32)\n",
            "         - x (8, 10, 24)\n",
            "         - x (reshape) (8, 10, 8, 3)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(8, shape=(), dtype=int32)\n",
            "         - x (8, 10, 24)\n",
            "         - x (reshape) (8, 10, 8, 3)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (8, 8, 10, 3)\n",
            "         - k (8, 8, 10, 3)\n",
            "         - v (8, 8, 10, 3)\n",
            "       - scaled_dot_product_attention (8, 8, 10, 3) (8, 8, 10, 10)\n",
            "       - attention (transposed) (8, 10, 8, 3)\n",
            "       - concat_attention (8, 10, 24)\n",
            "       - output (8, 10, 24)\n",
            "     - attn2 (8, 10, 24)\n",
            "     - out2 (8, 10, 24)\n",
            "     - out3 (8, 10, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (8, 10, 24)\n",
            "     - enc_output (8, 10, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (8, 10, 24)\n",
            "       - k (8, 10, 24)\n",
            "       - q (8, 10, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(8, shape=(), dtype=int32)\n",
            "         - x (8, 10, 24)\n",
            "         - x (reshape) (8, 10, 8, 3)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(8, shape=(), dtype=int32)\n",
            "         - x (8, 10, 24)\n",
            "         - x (reshape) (8, 10, 8, 3)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(8, shape=(), dtype=int32)\n",
            "         - x (8, 10, 24)\n",
            "         - x (reshape) (8, 10, 8, 3)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (8, 8, 10, 3)\n",
            "         - k (8, 8, 10, 3)\n",
            "         - v (8, 8, 10, 3)\n",
            "       - scaled_dot_product_attention (8, 8, 10, 3) (8, 8, 10, 10)\n",
            "       - attention (transposed) (8, 10, 8, 3)\n",
            "       - concat_attention (8, 10, 24)\n",
            "       - output (8, 10, 24)\n",
            "     - attn1 (8, 10, 24)\n",
            "     - out1 (8, 10, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (8, 10, 1280)\n",
            "       - k (8, 10, 1280)\n",
            "       - q (8, 10, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(8, shape=(), dtype=int32)\n",
            "         - x (8, 10, 24)\n",
            "         - x (reshape) (8, 10, 8, 3)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(8, shape=(), dtype=int32)\n",
            "         - x (8, 10, 24)\n",
            "         - x (reshape) (8, 10, 8, 3)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(8, shape=(), dtype=int32)\n",
            "         - x (8, 10, 24)\n",
            "         - x (reshape) (8, 10, 8, 3)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (8, 8, 10, 3)\n",
            "         - k (8, 8, 10, 3)\n",
            "         - v (8, 8, 10, 3)\n",
            "       - scaled_dot_product_attention (8, 8, 10, 3) (8, 8, 10, 10)\n",
            "       - attention (transposed) (8, 10, 8, 3)\n",
            "       - concat_attention (8, 10, 24)\n",
            "       - output (8, 10, 24)\n",
            "     - attn2 (8, 10, 24)\n",
            "     - out2 (8, 10, 24)\n",
            "     - out3 (8, 10, 24)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([8, 10, 24]), TensorShape([8, 8, 10, 10]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 268
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yhcJ3SI6d1Hr"
      },
      "source": [
        "## Create the Transformer\n",
        "Transformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Fx5nQ0-td1Hs",
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, \n",
        "                 input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1, \n",
        "                 d_model_q=None):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate, d_model_q)\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask) # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "        # dec_output.shape == (batch_size, tar_seq_len, d_model_q)\n",
        "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output) # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "        return final_output, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b68c18ed-02b3-4d78-cdbb-f76db391aa23",
        "id": "EpIrkEQxd1Ht",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sample_transformer = Transformer(\n",
        "    num_layers=2, d_model=1280, num_heads=8, dff=256, \n",
        "    input_vocab_size=None, target_vocab_size=24, \n",
        "    pe_input=100, pe_target=100, # https://www.youtube.com/watch?v=HNtz05bhI1k\n",
        "    d_model_q=24)\n",
        "\n",
        "#temp_input = tf.random.uniform((64, 10, 160, 160, 3), dtype=tf.int64, minval=-1, maxval=1)\n",
        "temp_input = tf.random.uniform((1, 10, 160, 160, 3), dtype=tf.float32, minval=-1, maxval=1)\n",
        "temp_target = tf.random.uniform((1, 10), dtype=tf.int64, minval=0, maxval=17)\n",
        "\n",
        "fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n",
        "                               enc_padding_mask=None, \n",
        "                               look_ahead_mask=None,\n",
        "                               dec_padding_mask=None)\n",
        "\n",
        "fn_out.shape # (batch_size, tar_seq_len, target_vocab_size)"
      ],
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: input_vocab_size is no longer used (its value of None will be ignored)\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "     [get_angles] pos (100, 1)\n",
            "     [get_angles] i (1, 1280)\n",
            "   [positional_encoding] angle_rads (100, 1280)\n",
            "     [get_angles] pos (100, 1)\n",
            "     [get_angles] i (1, 24)\n",
            "   [positional_encoding] angle_rads (100, 24)\n",
            "   [FeatureExtractor.call]\n",
            "   - x (1, 10, 160, 160, 3)\n",
            "   - x (reshaped) (10, 160, 160, 3)\n",
            "   - x (mobile_net_v2) (10, 5, 5, 1280)\n",
            "   - x (global_average_layer) (10, 1280)\n",
            "   - x (reshaped) (1, 10, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (1, 10, 1280)\n",
            "   - k (1, 10, 1280)\n",
            "   - q (1, 10, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(1, shape=(), dtype=int32)\n",
            "     - x (1, 10, 1280)\n",
            "     - x (reshape) (1, 10, 8, 160)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(1, shape=(), dtype=int32)\n",
            "     - x (1, 10, 1280)\n",
            "     - x (reshape) (1, 10, 8, 160)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(1, shape=(), dtype=int32)\n",
            "     - x (1, 10, 1280)\n",
            "     - x (reshape) (1, 10, 8, 160)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (1, 8, 10, 160)\n",
            "     - k (1, 8, 10, 160)\n",
            "     - v (1, 8, 10, 160)\n",
            "   - scaled_dot_product_attention (1, 8, 10, 160) (1, 8, 10, 10)\n",
            "   - attention (transposed) (1, 10, 8, 160)\n",
            "   - concat_attention (1, 10, 1280)\n",
            "   - output (1, 10, 1280)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: d_model_q must be 18 as we have actions from 0 to 17 (not 24)\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: d_model_q must be 18 as we have actions from 0 to 17 (not 24)\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   [MultiHeadAttention.call]\n",
            "   - v (1, 10, 1280)\n",
            "   - k (1, 10, 1280)\n",
            "   - q (1, 10, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(1, shape=(), dtype=int32)\n",
            "     - x (1, 10, 1280)\n",
            "     - x (reshape) (1, 10, 8, 160)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(1, shape=(), dtype=int32)\n",
            "     - x (1, 10, 1280)\n",
            "     - x (reshape) (1, 10, 8, 160)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(1, shape=(), dtype=int32)\n",
            "     - x (1, 10, 1280)\n",
            "     - x (reshape) (1, 10, 8, 160)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (1, 8, 10, 160)\n",
            "     - k (1, 8, 10, 160)\n",
            "     - v (1, 8, 10, 160)\n",
            "   - scaled_dot_product_attention (1, 8, 10, 160) (1, 8, 10, 10)\n",
            "   - attention (transposed) (1, 10, 8, 160)\n",
            "   - concat_attention (1, 10, 1280)\n",
            "   - output (1, 10, 1280)\n",
            "   [Decoder.call]\n",
            "   - x (1, 10)\n",
            "   - embedding (1, 10, 24)\n",
            "   - pos_encoding (1, 10, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (1, 10, 24)\n",
            "     - enc_output (1, 10, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (1, 10, 24)\n",
            "       - k (1, 10, 24)\n",
            "       - q (1, 10, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(1, shape=(), dtype=int32)\n",
            "         - x (1, 10, 24)\n",
            "         - x (reshape) (1, 10, 8, 3)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(1, shape=(), dtype=int32)\n",
            "         - x (1, 10, 24)\n",
            "         - x (reshape) (1, 10, 8, 3)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(1, shape=(), dtype=int32)\n",
            "         - x (1, 10, 24)\n",
            "         - x (reshape) (1, 10, 8, 3)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (1, 8, 10, 3)\n",
            "         - k (1, 8, 10, 3)\n",
            "         - v (1, 8, 10, 3)\n",
            "       - scaled_dot_product_attention (1, 8, 10, 3) (1, 8, 10, 10)\n",
            "       - attention (transposed) (1, 10, 8, 3)\n",
            "       - concat_attention (1, 10, 24)\n",
            "       - output (1, 10, 24)\n",
            "     - attn1 (1, 10, 24)\n",
            "     - out1 (1, 10, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (1, 10, 1280)\n",
            "       - k (1, 10, 1280)\n",
            "       - q (1, 10, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(1, shape=(), dtype=int32)\n",
            "         - x (1, 10, 24)\n",
            "         - x (reshape) (1, 10, 8, 3)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(1, shape=(), dtype=int32)\n",
            "         - x (1, 10, 24)\n",
            "         - x (reshape) (1, 10, 8, 3)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(1, shape=(), dtype=int32)\n",
            "         - x (1, 10, 24)\n",
            "         - x (reshape) (1, 10, 8, 3)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (1, 8, 10, 3)\n",
            "         - k (1, 8, 10, 3)\n",
            "         - v (1, 8, 10, 3)\n",
            "       - scaled_dot_product_attention (1, 8, 10, 3) (1, 8, 10, 10)\n",
            "       - attention (transposed) (1, 10, 8, 3)\n",
            "       - concat_attention (1, 10, 24)\n",
            "       - output (1, 10, 24)\n",
            "     - attn2 (1, 10, 24)\n",
            "     - out2 (1, 10, 24)\n",
            "     - out3 (1, 10, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (1, 10, 24)\n",
            "     - enc_output (1, 10, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (1, 10, 24)\n",
            "       - k (1, 10, 24)\n",
            "       - q (1, 10, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(1, shape=(), dtype=int32)\n",
            "         - x (1, 10, 24)\n",
            "         - x (reshape) (1, 10, 8, 3)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(1, shape=(), dtype=int32)\n",
            "         - x (1, 10, 24)\n",
            "         - x (reshape) (1, 10, 8, 3)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(1, shape=(), dtype=int32)\n",
            "         - x (1, 10, 24)\n",
            "         - x (reshape) (1, 10, 8, 3)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (1, 8, 10, 3)\n",
            "         - k (1, 8, 10, 3)\n",
            "         - v (1, 8, 10, 3)\n",
            "       - scaled_dot_product_attention (1, 8, 10, 3) (1, 8, 10, 10)\n",
            "       - attention (transposed) (1, 10, 8, 3)\n",
            "       - concat_attention (1, 10, 24)\n",
            "       - output (1, 10, 24)\n",
            "     - attn1 (1, 10, 24)\n",
            "     - out1 (1, 10, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (1, 10, 1280)\n",
            "       - k (1, 10, 1280)\n",
            "       - q (1, 10, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(1, shape=(), dtype=int32)\n",
            "         - x (1, 10, 24)\n",
            "         - x (reshape) (1, 10, 8, 3)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(1, shape=(), dtype=int32)\n",
            "         - x (1, 10, 24)\n",
            "         - x (reshape) (1, 10, 8, 3)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(1, shape=(), dtype=int32)\n",
            "         - x (1, 10, 24)\n",
            "         - x (reshape) (1, 10, 8, 3)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (1, 8, 10, 3)\n",
            "         - k (1, 8, 10, 3)\n",
            "         - v (1, 8, 10, 3)\n",
            "       - scaled_dot_product_attention (1, 8, 10, 3) (1, 8, 10, 10)\n",
            "       - attention (transposed) (1, 10, 8, 3)\n",
            "       - concat_attention (1, 10, 24)\n",
            "       - output (1, 10, 24)\n",
            "     - attn2 (1, 10, 24)\n",
            "     - out2 (1, 10, 24)\n",
            "     - out3 (1, 10, 24)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 10, 24])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 270
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Fo7tOx8xd1Hv"
      },
      "source": [
        "## Set hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RntlqCQZd1Hv",
        "colab": {}
      },
      "source": [
        "num_layers = 4\n",
        "d_model = 1280\n",
        "d_model_q = 24\n",
        "dff = 512\n",
        "#num_heads = 8\n",
        "num_heads = 2 # FIXME fix asserts above to use 8 instead of 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5yKiQ0J2d1Hx",
        "colab": {}
      },
      "source": [
        "N_OF_INP_IMAGES = 1000\n",
        "INP_IMAGE_SHAPE = (160, 160, 3) # Atari images are 210x160 RGB.. so you should resize this somewhere mate ;)\n",
        "INP_IMAGE_SIZE  = np.prod(INP_IMAGE_SHAPE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PnwpqGV2d1Hy"
      },
      "source": [
        "A total of 18 actions can be performed with the joystick: doing nothing, pressing the action button, going in one of 8 directions (up, down, left and right as well as the 4 diagonals) and going in any of these directions while pressing the button."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L2bamPQld1Hy",
        "colab": {}
      },
      "source": [
        "input_vocab_size = None # NOTE this is no longer used\n",
        "target_vocab_size = 24 # actions to take\n",
        "dropout_rate = 0.1\n",
        "\n",
        "pe_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2EcdAnFDd1H0"
      },
      "source": [
        "## Optimizer\n",
        "Use the Adam optimizer with a custom learning rate scheduler according to the formula in the [paper](https://arxiv.org/abs/1706.03762): ${lrate = d_{model}^{-0.5} \\cdot min(step{\\_}num^{-0.5},\\, step{\\_}num \\cdot warmup{\\_}steps^{-1.5})}$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "thxqmJGzd1H0",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aV7xETdtd1H1",
        "colab": {}
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Md-hhNzkd1H3"
      },
      "source": [
        "## Loss and metrics\n",
        "Since the target sequences are padded, it is important to apply a padding mask when calculating the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8IQaSbF9d1H5",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z5GSjURfd1H7",
        "colab": {}
      },
      "source": [
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qv8bZ7Esd1H8",
        "colab": {}
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zojoc277d1H_"
      },
      "source": [
        "## Training and checkpointing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9624df66-c3cd-4743-e7c3-d9120953fcba",
        "id": "-BTYdKetd1H_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size, target_vocab_size, \n",
        "                          pe_input=pe_size, \n",
        "                          pe_target=pe_size,\n",
        "                          rate=dropout_rate,\n",
        "                          d_model_q=d_model_q)"
      ],
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: input_vocab_size is no longer used (its value of None will be ignored)\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "     [get_angles] pos (100, 1)\n",
            "     [get_angles] i (1, 1280)\n",
            "   [positional_encoding] angle_rads (100, 1280)\n",
            "     [get_angles] pos (100, 1)\n",
            "     [get_angles] i (1, 24)\n",
            "   [positional_encoding] angle_rads (100, 24)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: d_model_q must be 18 as we have actions from 0 to 17 (not 24)\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: d_model_q must be 18 as we have actions from 0 to 17 (not 24)\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PoJ99uzqd1IA",
        "colab": {}
      },
      "source": [
        "@indent_prints\n",
        "def create_masks(inp, tar):\n",
        "    print(\" \"*SPACES, \"[create_masks]\")\n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    print(\" \"*SPACES, \"- enc_padding_mask\", enc_padding_mask.shape)\n",
        "\n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "    print(\" \"*SPACES, \"- dec_padding_mask\", dec_padding_mask.shape)\n",
        "\n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by \n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    print(\" \"*SPACES, \"- look_ahead_mask\", look_ahead_mask.shape)\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    print(\" \"*SPACES, \"- dec_target_padding_mask\", dec_target_padding_mask.shape)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "    print(\" \"*SPACES, \"- combined_mask\", combined_mask.shape)\n",
        "\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BvR9DwWGd1IB"
      },
      "source": [
        "Create the checkpoint path and the checkpoint manager. This will be used to save checkpoints every `n` epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "24rb8-Ivd1IC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d78bf878-7455-44c6-ea13-8036e8f9efa7"
      },
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print ('Latest checkpoint restored!!')"
      ],
      "execution_count": 281,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Latest checkpoint restored!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecAo9dv3z84K",
        "colab_type": "text"
      },
      "source": [
        "### Setup input pipeline\n",
        "Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5T8rLQL1_H9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUK4N4oY0Xgi",
        "colab_type": "code",
        "outputId": "08c13a9c-346e-4cad-b3e8-ed2fa5251d8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 283,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voBDzb_4ih9Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "0fc2fb63-9fff-423e-8194-7aa271c6b4bc"
      },
      "source": [
        "!ls drive/My\\ Drive/unicamp/MC886/atari/data/"
      ],
      "execution_count": 284,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Breakout-v0_1000\t      Breakout-v4_1000_actions.npy\n",
            "Breakout-v0_1000_actions.npy  Pong-v0_1000\n",
            "Breakout-v4_1000\t      Pong-v0_1000_actions.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpz3d24x0Vto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "actions_taken = np.load('drive/My Drive/unicamp/MC886/atari/data/Breakout-v4_1000_actions.npy')\n",
        "actions_taken = np.array(actions_taken, dtype='int8')\n",
        "\n",
        "actions_data = np.array([actions_taken[:100] for _ in range(10)], dtype='int8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfhw25QY0aco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_all_png(path):\n",
        "    files = []\n",
        "    files_full = []\n",
        "    for root, dirs, file_names in os.walk(path):\n",
        "        files.extend([name for name in file_names if '.png' in name.lower()])\n",
        "        files_full.extend([os.path.join(root, name) for name in file_names if '.png' in name.lower()])\n",
        "    return files, files_full"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L84-lOOq1meK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMG_SIZE = 160 # All images will be resized to 160x160\n",
        "\n",
        "def load_image(image_path):#, action):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_png(image, channels=3)\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = (image/127.5) - 1\n",
        "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "    return image#, action\n",
        "  \n",
        "def load_all_imgs(paths, actions):\n",
        "    print(paths[0])\n",
        "    imgs = tf.map_fn(load_image, paths)\n",
        "    return imgs, actions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7U6GyQz1vzM",
        "colab_type": "code",
        "outputId": "a6e2b287-1ab3-4bc9-ed26-da62979e106e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "_, img_name_vector = get_all_png('drive/My Drive/unicamp/MC886/atari/data/Breakout-v4_1000/')\n",
        "print(len(img_name_vector))\n",
        "\n",
        "loaded_imgs = [load_image(path) for path in img_name_vector[:100]]\n",
        "\n",
        "img_name_all = [loaded_imgs for _ in range(10)]"
      ],
      "execution_count": 288,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoWCCPvF1wex",
        "colab_type": "code",
        "outputId": "67279902-3002-4348-900f-c629fcb8b119",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Feel free to change batch_size according to your system configuration\n",
        "image_dataset = tf.data.Dataset.from_tensor_slices((img_name_all, actions_data))\n",
        "\n",
        "print(image_dataset.element_spec)\n",
        "\n",
        "image_dataset = image_dataset.batch(2)\n",
        "# .map(load_all_imgs, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(2)"
      ],
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(TensorSpec(shape=(100, 160, 160, 3), dtype=tf.float32, name=None), TensorSpec(shape=(100,), dtype=tf.int8, name=None))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEqPZr3q2o4B",
        "colab_type": "code",
        "outputId": "2841e4af-a54b-4809-cf1c-30d6055eb230",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(actions_taken.shape)\n",
        "print(len(img_name_vector))"
      ],
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000,)\n",
            "1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1UrE5Pi45bi",
        "colab_type": "code",
        "outputId": "b200c534-ccab-46fa-dcc9-4a3c8d3d4637",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "train_dataset = image_dataset\n",
        "\n",
        "train_dataset.element_spec"
      ],
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(None, 100, 160, 160, 3), dtype=tf.float32, name=None),\n",
              " TensorSpec(shape=(None, 100), dtype=tf.int8, name=None))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 291
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vww_N64DYoHA",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1oY9B5Tyd1IS"
      },
      "source": [
        "The target is divided into `tar_inp` and `tar_real`. `tar_inp` is passed as an input to the decoder. `tar_real` is that same input shifted by 1: At each location in `tar_input`, `tar_real` contains the  next token that should be predicted.\n",
        "\n",
        "The transformer is an auto-regressive model: it makes predictions one part at a time, and uses its output so far to decide what to do next. \n",
        "\n",
        "During training this example uses teacher-forcing. Teacher forcing is passing the true output to the next time step regardless of what the model predicts at the current time step.\n",
        "\n",
        "As the transformer predicts each token, *self-attention* allows it to look at the previous tokens in the input sequence to better predict the next one.\n",
        "\n",
        "To prevent the model from peaking at the expected output the model uses a look-ahead mask."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5a4Zmo54d1IS",
        "colab": {}
      },
      "source": [
        "EPOCHS = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R2mKmc9Rd1IU",
        "colab": {}
      },
      "source": [
        "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None, 160, 160, 3), dtype=tf.float32),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int8),\n",
        "]\n",
        "\n",
        "_feature_extractor = FeatureExtractor()\n",
        "\n",
        "@indent_prints\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "    print(\" \"*SPACES, \"[train_step]\")\n",
        "    print(\" \"*SPACES, \"- tar\", tar.shape)\n",
        "    print(\" \"*SPACES, \"- tar_inp\", tar_inp.shape)\n",
        "    print(\" \"*SPACES, \"- tar_real\", tar_real.shape)\n",
        "\n",
        "    # > enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "    inp_feat_vec = _feature_extractor(inp)\n",
        "    print(\" \"*SPACES, \"- inp\", inp.shape)\n",
        "    print(\" \"*SPACES, \"- inp_feat_vec\", inp_feat_vec.shape)\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp_feat_vec, tar_inp)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # predictions, _ = transformer(inp, tar_inp, \n",
        "        #                              True, \n",
        "        #                              enc_padding_mask, \n",
        "        #                              combined_mask, \n",
        "        #                              dec_padding_mask)\n",
        "        # FIXME some mask has a wrong shape\n",
        "        predictions, _ = transformer(inp, tar_inp, \n",
        "                                     True, None, None, None)\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(tar_real, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaooyEj1k-le",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "#     print(inp.shape, tar.shape)\n",
        "#     print(_feature_extractor(inp).shape)\n",
        "#     print()\n",
        "#     train_step(inp, tar)\n",
        "#     break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3jAmwZf0d1IV"
      },
      "source": [
        "Atari frame images are used as the input language and Atari actions are the target language."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "61d0339d-b0c8-44aa-ce2e-971c420f4efd",
        "id": "lobPqcU3d1IW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "\n",
        "    # inp -> observations, tar -> actions\n",
        "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "        print(inp.shape, tar.shape)\n",
        "        train_step(inp, tar)\n",
        "\n",
        "        if batch % 50 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "                  epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "        \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
        "                                                             ckpt_save_path))\n",
        "\n",
        "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
        "                                                         train_loss.result(), \n",
        "                                                         train_accuracy.result()))\n",
        "\n",
        "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": 295,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "   [train_step]\n",
            "   - tar (None, None)\n",
            "   - tar_inp (None, None)\n",
            "   - tar_real (None, None)\n",
            "     [FeatureExtractor.call]\n",
            "     - x (None, None, 160, 160, 3)\n",
            "     - x (reshaped) (None, None, None, None)\n",
            "     - x (mobile_net_v2) (None, None, None, 1280)\n",
            "     - x (global_average_layer) (None, 1280)\n",
            "     - x (reshaped) (None, None, None)\n",
            "   - inp (None, None, 160, 160, 3)\n",
            "   - inp_feat_vec (None, None, None)\n",
            "     [create_masks]\n",
            "     - enc_padding_mask (None, 1, 1, None, None)\n",
            "     - dec_padding_mask (None, 1, 1, None, None)\n",
            "     - look_ahead_mask (None, None)\n",
            "     - dec_target_padding_mask (None, 1, 1, None)\n",
            "     - combined_mask (None, 1, None, None)\n",
            "     [FeatureExtractor.call]\n",
            "     - x (None, None, 160, 160, 3)\n",
            "     - x (reshaped) (None, None, None, None)\n",
            "     - x (mobile_net_v2) (None, None, None, 1280)\n",
            "     - x (global_average_layer) (None, 1280)\n",
            "     - x (reshaped) (None, None, None)\n",
            "     [MultiHeadAttention.call]\n",
            "     - v (None, None, 1280)\n",
            "     - k (None, None, 1280)\n",
            "     - q (None, None, 1280)\n",
            "       [split_heads]\n",
            "       - batch_size Tensor(\"transformer_9/encoder_12/encoder_layer_35/multi_head_attention_113/strided_slice:0\", shape=(), dtype=int32)\n",
            "       - x (None, None, 1280)\n",
            "       - x (reshape) (None, None, 2, 640)\n",
            "       [split_heads]\n",
            "       - batch_size Tensor(\"transformer_9/encoder_12/encoder_layer_35/multi_head_attention_113/strided_slice:0\", shape=(), dtype=int32)\n",
            "       - x (None, None, 1280)\n",
            "       - x (reshape) (None, None, 2, 640)\n",
            "       [split_heads]\n",
            "       - batch_size Tensor(\"transformer_9/encoder_12/encoder_layer_35/multi_head_attention_113/strided_slice:0\", shape=(), dtype=int32)\n",
            "       - x (None, None, 1280)\n",
            "       - x (reshape) (None, None, 2, 640)\n",
            "       [scaled_dot_product_attention]\n",
            "       - q (None, 2, None, 640)\n",
            "       - k (None, 2, None, 640)\n",
            "       - v (None, 2, None, 640)\n",
            "     - scaled_dot_product_attention (None, 2, None, 640) (None, 2, None, None)\n",
            "     - attention (transposed) (None, None, 2, 640)\n",
            "     - concat_attention (None, None, 1280)\n",
            "     - output (None, None, 1280)\n",
            "     [MultiHeadAttention.call]\n",
            "     - v (None, None, 1280)\n",
            "     - k (None, None, 1280)\n",
            "     - q (None, None, 1280)\n",
            "       [split_heads]\n",
            "       - batch_size Tensor(\"transformer_9/encoder_12/encoder_layer_36/multi_head_attention_114/strided_slice:0\", shape=(), dtype=int32)\n",
            "       - x (None, None, 1280)\n",
            "       - x (reshape) (None, None, 2, 640)\n",
            "       [split_heads]\n",
            "       - batch_size Tensor(\"transformer_9/encoder_12/encoder_layer_36/multi_head_attention_114/strided_slice:0\", shape=(), dtype=int32)\n",
            "       - x (None, None, 1280)\n",
            "       - x (reshape) (None, None, 2, 640)\n",
            "       [split_heads]\n",
            "       - batch_size Tensor(\"transformer_9/encoder_12/encoder_layer_36/multi_head_attention_114/strided_slice:0\", shape=(), dtype=int32)\n",
            "       - x (None, None, 1280)\n",
            "       - x (reshape) (None, None, 2, 640)\n",
            "       [scaled_dot_product_attention]\n",
            "       - q (None, 2, None, 640)\n",
            "       - k (None, 2, None, 640)\n",
            "       - v (None, 2, None, 640)\n",
            "     - scaled_dot_product_attention (None, 2, None, 640) (None, 2, None, None)\n",
            "     - attention (transposed) (None, None, 2, 640)\n",
            "     - concat_attention (None, None, 1280)\n",
            "     - output (None, None, 1280)\n",
            "     [MultiHeadAttention.call]\n",
            "     - v (None, None, 1280)\n",
            "     - k (None, None, 1280)\n",
            "     - q (None, None, 1280)\n",
            "       [split_heads]\n",
            "       - batch_size Tensor(\"transformer_9/encoder_12/encoder_layer_37/multi_head_attention_115/strided_slice:0\", shape=(), dtype=int32)\n",
            "       - x (None, None, 1280)\n",
            "       - x (reshape) (None, None, 2, 640)\n",
            "       [split_heads]\n",
            "       - batch_size Tensor(\"transformer_9/encoder_12/encoder_layer_37/multi_head_attention_115/strided_slice:0\", shape=(), dtype=int32)\n",
            "       - x (None, None, 1280)\n",
            "       - x (reshape) (None, None, 2, 640)\n",
            "       [split_heads]\n",
            "       - batch_size Tensor(\"transformer_9/encoder_12/encoder_layer_37/multi_head_attention_115/strided_slice:0\", shape=(), dtype=int32)\n",
            "       - x (None, None, 1280)\n",
            "       - x (reshape) (None, None, 2, 640)\n",
            "       [scaled_dot_product_attention]\n",
            "       - q (None, 2, None, 640)\n",
            "       - k (None, 2, None, 640)\n",
            "       - v (None, 2, None, 640)\n",
            "     - scaled_dot_product_attention (None, 2, None, 640) (None, 2, None, None)\n",
            "     - attention (transposed) (None, None, 2, 640)\n",
            "     - concat_attention (None, None, 1280)\n",
            "     - output (None, None, 1280)\n",
            "     [MultiHeadAttention.call]\n",
            "     - v (None, None, 1280)\n",
            "     - k (None, None, 1280)\n",
            "     - q (None, None, 1280)\n",
            "       [split_heads]\n",
            "       - batch_size Tensor(\"transformer_9/encoder_12/encoder_layer_38/multi_head_attention_116/strided_slice:0\", shape=(), dtype=int32)\n",
            "       - x (None, None, 1280)\n",
            "       - x (reshape) (None, None, 2, 640)\n",
            "       [split_heads]\n",
            "       - batch_size Tensor(\"transformer_9/encoder_12/encoder_layer_38/multi_head_attention_116/strided_slice:0\", shape=(), dtype=int32)\n",
            "       - x (None, None, 1280)\n",
            "       - x (reshape) (None, None, 2, 640)\n",
            "       [split_heads]\n",
            "       - batch_size Tensor(\"transformer_9/encoder_12/encoder_layer_38/multi_head_attention_116/strided_slice:0\", shape=(), dtype=int32)\n",
            "       - x (None, None, 1280)\n",
            "       - x (reshape) (None, None, 2, 640)\n",
            "       [scaled_dot_product_attention]\n",
            "       - q (None, 2, None, 640)\n",
            "       - k (None, 2, None, 640)\n",
            "       - v (None, 2, None, 640)\n",
            "     - scaled_dot_product_attention (None, 2, None, 640) (None, 2, None, None)\n",
            "     - attention (transposed) (None, None, 2, 640)\n",
            "     - concat_attention (None, None, 1280)\n",
            "     - output (None, None, 1280)\n",
            "     [Decoder.call]\n",
            "     - x (None, None)\n",
            "     - embedding (None, None, 24)\n",
            "     - pos_encoding (None, None, 24)\n",
            "       [DecoderLayer.call]\n",
            "       - x (None, None, 24)\n",
            "       - enc_output (None, None, 1280)\n",
            "         [MultiHeadAttention.call]\n",
            "         - v (None, None, 24)\n",
            "         - k (None, None, 24)\n",
            "         - q (None, None, 24)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_35/multi_head_attention_117/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_35/multi_head_attention_117/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_35/multi_head_attention_117/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [scaled_dot_product_attention]\n",
            "           - q (None, 2, None, 12)\n",
            "           - k (None, 2, None, 12)\n",
            "           - v (None, 2, None, 12)\n",
            "         - scaled_dot_product_attention (None, 2, None, 12) (None, 2, None, None)\n",
            "         - attention (transposed) (None, None, 2, 12)\n",
            "         - concat_attention (None, None, 24)\n",
            "         - output (None, None, 24)\n",
            "       - attn1 (None, None, 24)\n",
            "       - out1 (None, None, 24)\n",
            "         [MultiHeadAttention.call]\n",
            "         - v (None, None, 1280)\n",
            "         - k (None, None, 1280)\n",
            "         - q (None, None, 24)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_35/multi_head_attention_118/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_35/multi_head_attention_118/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_35/multi_head_attention_118/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [scaled_dot_product_attention]\n",
            "           - q (None, 2, None, 12)\n",
            "           - k (None, 2, None, 12)\n",
            "           - v (None, 2, None, 12)\n",
            "         - scaled_dot_product_attention (None, 2, None, 12) (None, 2, None, None)\n",
            "         - attention (transposed) (None, None, 2, 12)\n",
            "         - concat_attention (None, None, 24)\n",
            "         - output (None, None, 24)\n",
            "       - attn2 (None, None, 24)\n",
            "       - out2 (None, None, 24)\n",
            "       - out3 (None, None, 24)\n",
            "       [DecoderLayer.call]\n",
            "       - x (None, None, 24)\n",
            "       - enc_output (None, None, 1280)\n",
            "         [MultiHeadAttention.call]\n",
            "         - v (None, None, 24)\n",
            "         - k (None, None, 24)\n",
            "         - q (None, None, 24)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_36/multi_head_attention_119/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_36/multi_head_attention_119/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_36/multi_head_attention_119/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [scaled_dot_product_attention]\n",
            "           - q (None, 2, None, 12)\n",
            "           - k (None, 2, None, 12)\n",
            "           - v (None, 2, None, 12)\n",
            "         - scaled_dot_product_attention (None, 2, None, 12) (None, 2, None, None)\n",
            "         - attention (transposed) (None, None, 2, 12)\n",
            "         - concat_attention (None, None, 24)\n",
            "         - output (None, None, 24)\n",
            "       - attn1 (None, None, 24)\n",
            "       - out1 (None, None, 24)\n",
            "         [MultiHeadAttention.call]\n",
            "         - v (None, None, 1280)\n",
            "         - k (None, None, 1280)\n",
            "         - q (None, None, 24)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_36/multi_head_attention_120/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_36/multi_head_attention_120/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_36/multi_head_attention_120/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [scaled_dot_product_attention]\n",
            "           - q (None, 2, None, 12)\n",
            "           - k (None, 2, None, 12)\n",
            "           - v (None, 2, None, 12)\n",
            "         - scaled_dot_product_attention (None, 2, None, 12) (None, 2, None, None)\n",
            "         - attention (transposed) (None, None, 2, 12)\n",
            "         - concat_attention (None, None, 24)\n",
            "         - output (None, None, 24)\n",
            "       - attn2 (None, None, 24)\n",
            "       - out2 (None, None, 24)\n",
            "       - out3 (None, None, 24)\n",
            "       [DecoderLayer.call]\n",
            "       - x (None, None, 24)\n",
            "       - enc_output (None, None, 1280)\n",
            "         [MultiHeadAttention.call]\n",
            "         - v (None, None, 24)\n",
            "         - k (None, None, 24)\n",
            "         - q (None, None, 24)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_37/multi_head_attention_121/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_37/multi_head_attention_121/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_37/multi_head_attention_121/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [scaled_dot_product_attention]\n",
            "           - q (None, 2, None, 12)\n",
            "           - k (None, 2, None, 12)\n",
            "           - v (None, 2, None, 12)\n",
            "         - scaled_dot_product_attention (None, 2, None, 12) (None, 2, None, None)\n",
            "         - attention (transposed) (None, None, 2, 12)\n",
            "         - concat_attention (None, None, 24)\n",
            "         - output (None, None, 24)\n",
            "       - attn1 (None, None, 24)\n",
            "       - out1 (None, None, 24)\n",
            "         [MultiHeadAttention.call]\n",
            "         - v (None, None, 1280)\n",
            "         - k (None, None, 1280)\n",
            "         - q (None, None, 24)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_37/multi_head_attention_122/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_37/multi_head_attention_122/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_37/multi_head_attention_122/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [scaled_dot_product_attention]\n",
            "           - q (None, 2, None, 12)\n",
            "           - k (None, 2, None, 12)\n",
            "           - v (None, 2, None, 12)\n",
            "         - scaled_dot_product_attention (None, 2, None, 12) (None, 2, None, None)\n",
            "         - attention (transposed) (None, None, 2, 12)\n",
            "         - concat_attention (None, None, 24)\n",
            "         - output (None, None, 24)\n",
            "       - attn2 (None, None, 24)\n",
            "       - out2 (None, None, 24)\n",
            "       - out3 (None, None, 24)\n",
            "       [DecoderLayer.call]\n",
            "       - x (None, None, 24)\n",
            "       - enc_output (None, None, 1280)\n",
            "         [MultiHeadAttention.call]\n",
            "         - v (None, None, 24)\n",
            "         - k (None, None, 24)\n",
            "         - q (None, None, 24)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_38/multi_head_attention_123/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_38/multi_head_attention_123/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_38/multi_head_attention_123/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [scaled_dot_product_attention]\n",
            "           - q (None, 2, None, 12)\n",
            "           - k (None, 2, None, 12)\n",
            "           - v (None, 2, None, 12)\n",
            "         - scaled_dot_product_attention (None, 2, None, 12) (None, 2, None, None)\n",
            "         - attention (transposed) (None, None, 2, 12)\n",
            "         - concat_attention (None, None, 24)\n",
            "         - output (None, None, 24)\n",
            "       - attn1 (None, None, 24)\n",
            "       - out1 (None, None, 24)\n",
            "         [MultiHeadAttention.call]\n",
            "         - v (None, None, 1280)\n",
            "         - k (None, None, 1280)\n",
            "         - q (None, None, 24)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_38/multi_head_attention_124/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_38/multi_head_attention_124/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_38/multi_head_attention_124/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [scaled_dot_product_attention]\n",
            "           - q (None, 2, None, 12)\n",
            "           - k (None, 2, None, 12)\n",
            "           - v (None, 2, None, 12)\n",
            "         - scaled_dot_product_attention (None, 2, None, 12) (None, 2, None, None)\n",
            "         - attention (transposed) (None, None, 2, 12)\n",
            "         - concat_attention (None, None, 24)\n",
            "         - output (None, None, 24)\n",
            "       - attn2 (None, None, 24)\n",
            "       - out2 (None, None, 24)\n",
            "       - out3 (None, None, 24)\n",
            "   [train_step]\n",
            "   - tar (None, None)\n",
            "   - tar_inp (None, None)\n",
            "   - tar_real (None, None)\n",
            "     [FeatureExtractor.call]\n",
            "     - x (None, None, 160, 160, 3)\n",
            "     - x (reshaped) (None, None, None, None)\n",
            "     - x (mobile_net_v2) (None, None, None, 1280)\n",
            "     - x (global_average_layer) (None, 1280)\n",
            "     - x (reshaped) (None, None, None)\n",
            "   - inp (None, None, 160, 160, 3)\n",
            "   - inp_feat_vec (None, None, None)\n",
            "     [create_masks]\n",
            "     - enc_padding_mask (None, 1, 1, None, None)\n",
            "     - dec_padding_mask (None, 1, 1, None, None)\n",
            "     - look_ahead_mask (None, None)\n",
            "     - dec_target_padding_mask (None, 1, 1, None)\n",
            "     - combined_mask (None, 1, None, None)\n",
            "     [FeatureExtractor.call]\n",
            "     - x (None, None, 160, 160, 3)\n",
            "     - x (reshaped) (None, None, None, None)\n",
            "     - x (mobile_net_v2) (None, None, None, 1280)\n",
            "     - x (global_average_layer) (None, 1280)\n",
            "     - x (reshaped) (None, None, None)\n",
            "     [MultiHeadAttention.call]\n",
            "     - v (None, None, 1280)\n",
            "     - k (None, None, 1280)\n",
            "     - q (None, None, 1280)\n",
            "       [split_heads]\n",
            "       - batch_size Tensor(\"transformer_9/encoder_12/encoder_layer_35/multi_head_attention_113/strided_slice:0\", shape=(), dtype=int32)\n",
            "       - x (None, None, 1280)\n",
            "       - x (reshape) (None, None, 2, 640)\n",
            "       [split_heads]\n",
            "       - batch_size Tensor(\"transformer_9/encoder_12/encoder_layer_35/multi_head_attention_113/strided_slice:0\", shape=(), dtype=int32)\n",
            "       - x (None, None, 1280)\n",
            "       - x (reshape) (None, None, 2, 640)\n",
            "       [split_heads]\n",
            "       - batch_size Tensor(\"transformer_9/encoder_12/encoder_layer_35/multi_head_attention_113/strided_slice:0\", shape=(), dtype=int32)\n",
            "       - x (None, None, 1280)\n",
            "       - x (reshape) (None, None, 2, 640)\n",
            "       [scaled_dot_product_attention]\n",
            "       - q (None, 2, None, 640)\n",
            "       - k (None, 2, None, 640)\n",
            "       - v (None, 2, None, 640)\n",
            "     - scaled_dot_product_attention (None, 2, None, 640) (None, 2, None, None)\n",
            "     - attention (transposed) (None, None, 2, 640)\n",
            "     - concat_attention (None, None, 1280)\n",
            "     - output (None, None, 1280)\n",
            "     [MultiHeadAttention.call]\n",
            "     - v (None, None, 1280)\n",
            "     - k (None, None, 1280)\n",
            "     - q (None, None, 1280)\n",
            "       [split_heads]\n",
            "       - batch_size Tensor(\"transformer_9/encoder_12/encoder_layer_36/multi_head_attention_114/strided_slice:0\", shape=(), dtype=int32)\n",
            "       - x (None, None, 1280)\n",
            "       - x (reshape) (None, None, 2, 640)\n",
            "       [split_heads]\n",
            "       - batch_size Tensor(\"transformer_9/encoder_12/encoder_layer_36/multi_head_attention_114/strided_slice:0\", shape=(), dtype=int32)\n",
            "       - x (None, None, 1280)\n",
            "       - x (reshape) (None, None, 2, 640)\n",
            "       [split_heads]\n",
            "       - batch_size Tensor(\"transformer_9/encoder_12/encoder_layer_36/multi_head_attention_114/strided_slice:0\", shape=(), dtype=int32)\n",
            "       - x (None, None, 1280)\n",
            "       - x (reshape) (None, None, 2, 640)\n",
            "       [scaled_dot_product_attention]\n",
            "       - q (None, 2, None, 640)\n",
            "       - k (None, 2, None, 640)\n",
            "       - v (None, 2, None, 640)\n",
            "     - scaled_dot_product_attention (None, 2, None, 640) (None, 2, None, None)\n",
            "     - attention (transposed) (None, None, 2, 640)\n",
            "     - concat_attention (None, None, 1280)\n",
            "     - output (None, None, 1280)\n",
            "     [MultiHeadAttention.call]\n",
            "     - v (None, None, 1280)\n",
            "     - k (None, None, 1280)\n",
            "     - q (None, None, 1280)\n",
            "       [split_heads]\n",
            "       - batch_size Tensor(\"transformer_9/encoder_12/encoder_layer_37/multi_head_attention_115/strided_slice:0\", shape=(), dtype=int32)\n",
            "       - x (None, None, 1280)\n",
            "       - x (reshape) (None, None, 2, 640)\n",
            "       [split_heads]\n",
            "       - batch_size Tensor(\"transformer_9/encoder_12/encoder_layer_37/multi_head_attention_115/strided_slice:0\", shape=(), dtype=int32)\n",
            "       - x (None, None, 1280)\n",
            "       - x (reshape) (None, None, 2, 640)\n",
            "       [split_heads]\n",
            "       - batch_size Tensor(\"transformer_9/encoder_12/encoder_layer_37/multi_head_attention_115/strided_slice:0\", shape=(), dtype=int32)\n",
            "       - x (None, None, 1280)\n",
            "       - x (reshape) (None, None, 2, 640)\n",
            "       [scaled_dot_product_attention]\n",
            "       - q (None, 2, None, 640)\n",
            "       - k (None, 2, None, 640)\n",
            "       - v (None, 2, None, 640)\n",
            "     - scaled_dot_product_attention (None, 2, None, 640) (None, 2, None, None)\n",
            "     - attention (transposed) (None, None, 2, 640)\n",
            "     - concat_attention (None, None, 1280)\n",
            "     - output (None, None, 1280)\n",
            "     [MultiHeadAttention.call]\n",
            "     - v (None, None, 1280)\n",
            "     - k (None, None, 1280)\n",
            "     - q (None, None, 1280)\n",
            "       [split_heads]\n",
            "       - batch_size Tensor(\"transformer_9/encoder_12/encoder_layer_38/multi_head_attention_116/strided_slice:0\", shape=(), dtype=int32)\n",
            "       - x (None, None, 1280)\n",
            "       - x (reshape) (None, None, 2, 640)\n",
            "       [split_heads]\n",
            "       - batch_size Tensor(\"transformer_9/encoder_12/encoder_layer_38/multi_head_attention_116/strided_slice:0\", shape=(), dtype=int32)\n",
            "       - x (None, None, 1280)\n",
            "       - x (reshape) (None, None, 2, 640)\n",
            "       [split_heads]\n",
            "       - batch_size Tensor(\"transformer_9/encoder_12/encoder_layer_38/multi_head_attention_116/strided_slice:0\", shape=(), dtype=int32)\n",
            "       - x (None, None, 1280)\n",
            "       - x (reshape) (None, None, 2, 640)\n",
            "       [scaled_dot_product_attention]\n",
            "       - q (None, 2, None, 640)\n",
            "       - k (None, 2, None, 640)\n",
            "       - v (None, 2, None, 640)\n",
            "     - scaled_dot_product_attention (None, 2, None, 640) (None, 2, None, None)\n",
            "     - attention (transposed) (None, None, 2, 640)\n",
            "     - concat_attention (None, None, 1280)\n",
            "     - output (None, None, 1280)\n",
            "     [Decoder.call]\n",
            "     - x (None, None)\n",
            "     - embedding (None, None, 24)\n",
            "     - pos_encoding (None, None, 24)\n",
            "       [DecoderLayer.call]\n",
            "       - x (None, None, 24)\n",
            "       - enc_output (None, None, 1280)\n",
            "         [MultiHeadAttention.call]\n",
            "         - v (None, None, 24)\n",
            "         - k (None, None, 24)\n",
            "         - q (None, None, 24)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_35/multi_head_attention_117/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_35/multi_head_attention_117/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_35/multi_head_attention_117/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [scaled_dot_product_attention]\n",
            "           - q (None, 2, None, 12)\n",
            "           - k (None, 2, None, 12)\n",
            "           - v (None, 2, None, 12)\n",
            "         - scaled_dot_product_attention (None, 2, None, 12) (None, 2, None, None)\n",
            "         - attention (transposed) (None, None, 2, 12)\n",
            "         - concat_attention (None, None, 24)\n",
            "         - output (None, None, 24)\n",
            "       - attn1 (None, None, 24)\n",
            "       - out1 (None, None, 24)\n",
            "         [MultiHeadAttention.call]\n",
            "         - v (None, None, 1280)\n",
            "         - k (None, None, 1280)\n",
            "         - q (None, None, 24)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_35/multi_head_attention_118/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_35/multi_head_attention_118/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_35/multi_head_attention_118/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [scaled_dot_product_attention]\n",
            "           - q (None, 2, None, 12)\n",
            "           - k (None, 2, None, 12)\n",
            "           - v (None, 2, None, 12)\n",
            "         - scaled_dot_product_attention (None, 2, None, 12) (None, 2, None, None)\n",
            "         - attention (transposed) (None, None, 2, 12)\n",
            "         - concat_attention (None, None, 24)\n",
            "         - output (None, None, 24)\n",
            "       - attn2 (None, None, 24)\n",
            "       - out2 (None, None, 24)\n",
            "       - out3 (None, None, 24)\n",
            "       [DecoderLayer.call]\n",
            "       - x (None, None, 24)\n",
            "       - enc_output (None, None, 1280)\n",
            "         [MultiHeadAttention.call]\n",
            "         - v (None, None, 24)\n",
            "         - k (None, None, 24)\n",
            "         - q (None, None, 24)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_36/multi_head_attention_119/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_36/multi_head_attention_119/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_36/multi_head_attention_119/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [scaled_dot_product_attention]\n",
            "           - q (None, 2, None, 12)\n",
            "           - k (None, 2, None, 12)\n",
            "           - v (None, 2, None, 12)\n",
            "         - scaled_dot_product_attention (None, 2, None, 12) (None, 2, None, None)\n",
            "         - attention (transposed) (None, None, 2, 12)\n",
            "         - concat_attention (None, None, 24)\n",
            "         - output (None, None, 24)\n",
            "       - attn1 (None, None, 24)\n",
            "       - out1 (None, None, 24)\n",
            "         [MultiHeadAttention.call]\n",
            "         - v (None, None, 1280)\n",
            "         - k (None, None, 1280)\n",
            "         - q (None, None, 24)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_36/multi_head_attention_120/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_36/multi_head_attention_120/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_36/multi_head_attention_120/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [scaled_dot_product_attention]\n",
            "           - q (None, 2, None, 12)\n",
            "           - k (None, 2, None, 12)\n",
            "           - v (None, 2, None, 12)\n",
            "         - scaled_dot_product_attention (None, 2, None, 12) (None, 2, None, None)\n",
            "         - attention (transposed) (None, None, 2, 12)\n",
            "         - concat_attention (None, None, 24)\n",
            "         - output (None, None, 24)\n",
            "       - attn2 (None, None, 24)\n",
            "       - out2 (None, None, 24)\n",
            "       - out3 (None, None, 24)\n",
            "       [DecoderLayer.call]\n",
            "       - x (None, None, 24)\n",
            "       - enc_output (None, None, 1280)\n",
            "         [MultiHeadAttention.call]\n",
            "         - v (None, None, 24)\n",
            "         - k (None, None, 24)\n",
            "         - q (None, None, 24)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_37/multi_head_attention_121/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_37/multi_head_attention_121/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_37/multi_head_attention_121/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [scaled_dot_product_attention]\n",
            "           - q (None, 2, None, 12)\n",
            "           - k (None, 2, None, 12)\n",
            "           - v (None, 2, None, 12)\n",
            "         - scaled_dot_product_attention (None, 2, None, 12) (None, 2, None, None)\n",
            "         - attention (transposed) (None, None, 2, 12)\n",
            "         - concat_attention (None, None, 24)\n",
            "         - output (None, None, 24)\n",
            "       - attn1 (None, None, 24)\n",
            "       - out1 (None, None, 24)\n",
            "         [MultiHeadAttention.call]\n",
            "         - v (None, None, 1280)\n",
            "         - k (None, None, 1280)\n",
            "         - q (None, None, 24)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_37/multi_head_attention_122/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_37/multi_head_attention_122/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_37/multi_head_attention_122/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [scaled_dot_product_attention]\n",
            "           - q (None, 2, None, 12)\n",
            "           - k (None, 2, None, 12)\n",
            "           - v (None, 2, None, 12)\n",
            "         - scaled_dot_product_attention (None, 2, None, 12) (None, 2, None, None)\n",
            "         - attention (transposed) (None, None, 2, 12)\n",
            "         - concat_attention (None, None, 24)\n",
            "         - output (None, None, 24)\n",
            "       - attn2 (None, None, 24)\n",
            "       - out2 (None, None, 24)\n",
            "       - out3 (None, None, 24)\n",
            "       [DecoderLayer.call]\n",
            "       - x (None, None, 24)\n",
            "       - enc_output (None, None, 1280)\n",
            "         [MultiHeadAttention.call]\n",
            "         - v (None, None, 24)\n",
            "         - k (None, None, 24)\n",
            "         - q (None, None, 24)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_38/multi_head_attention_123/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_38/multi_head_attention_123/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_38/multi_head_attention_123/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [scaled_dot_product_attention]\n",
            "           - q (None, 2, None, 12)\n",
            "           - k (None, 2, None, 12)\n",
            "           - v (None, 2, None, 12)\n",
            "         - scaled_dot_product_attention (None, 2, None, 12) (None, 2, None, None)\n",
            "         - attention (transposed) (None, None, 2, 12)\n",
            "         - concat_attention (None, None, 24)\n",
            "         - output (None, None, 24)\n",
            "       - attn1 (None, None, 24)\n",
            "       - out1 (None, None, 24)\n",
            "         [MultiHeadAttention.call]\n",
            "         - v (None, None, 1280)\n",
            "         - k (None, None, 1280)\n",
            "         - q (None, None, 24)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_38/multi_head_attention_124/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_38/multi_head_attention_124/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [split_heads]\n",
            "           - batch_size Tensor(\"transformer_9/decoder_13/decoder_layer_38/multi_head_attention_124/strided_slice:0\", shape=(), dtype=int32)\n",
            "           - x (None, None, 24)\n",
            "           - x (reshape) (None, None, 2, 12)\n",
            "           [scaled_dot_product_attention]\n",
            "           - q (None, 2, None, 12)\n",
            "           - k (None, 2, None, 12)\n",
            "           - v (None, 2, None, 12)\n",
            "         - scaled_dot_product_attention (None, 2, None, 12) (None, 2, None, None)\n",
            "         - attention (transposed) (None, None, 2, 12)\n",
            "         - concat_attention (None, None, 24)\n",
            "         - output (None, None, 24)\n",
            "       - attn2 (None, None, 24)\n",
            "       - out2 (None, None, 24)\n",
            "       - out3 (None, None, 24)\n",
            "Epoch 1 Batch 0 Loss 1.1294 Accuracy 0.3586\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 1 Loss 1.1319 Accuracy 0.3212\n",
            "Time taken for 1 epoch: 27.059396505355835 secs\n",
            "\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 2 Batch 0 Loss 1.1290 Accuracy 0.3131\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 2 Loss 1.1275 Accuracy 0.3131\n",
            "Time taken for 1 epoch: 2.1688127517700195 secs\n",
            "\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 3 Batch 0 Loss 1.1212 Accuracy 0.3232\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 3 Loss 1.1230 Accuracy 0.3283\n",
            "Time taken for 1 epoch: 2.1162681579589844 secs\n",
            "\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 4 Batch 0 Loss 1.1295 Accuracy 0.3283\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 4 Loss 1.1240 Accuracy 0.3020\n",
            "Time taken for 1 epoch: 2.0982272624969482 secs\n",
            "\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 5 Batch 0 Loss 1.1108 Accuracy 0.3232\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-17\n",
            "Epoch 5 Loss 1.1198 Accuracy 0.3101\n",
            "Time taken for 1 epoch: 3.4395484924316406 secs\n",
            "\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 6 Batch 0 Loss 1.1130 Accuracy 0.3535\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 6 Loss 1.1178 Accuracy 0.3253\n",
            "Time taken for 1 epoch: 2.1234076023101807 secs\n",
            "\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 7 Batch 0 Loss 1.1441 Accuracy 0.2677\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 7 Loss 1.1159 Accuracy 0.3091\n",
            "Time taken for 1 epoch: 2.12660551071167 secs\n",
            "\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 8 Batch 0 Loss 1.1207 Accuracy 0.3030\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 8 Loss 1.1116 Accuracy 0.3071\n",
            "Time taken for 1 epoch: 2.126469373703003 secs\n",
            "\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 9 Batch 0 Loss 1.1161 Accuracy 0.3081\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 9 Loss 1.1024 Accuracy 0.3141\n",
            "Time taken for 1 epoch: 2.1443986892700195 secs\n",
            "\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 10 Batch 0 Loss 1.1018 Accuracy 0.2828\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-18\n",
            "Epoch 10 Loss 1.0956 Accuracy 0.3283\n",
            "Time taken for 1 epoch: 3.4023866653442383 secs\n",
            "\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 11 Batch 0 Loss 1.1024 Accuracy 0.3081\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 11 Loss 1.0941 Accuracy 0.3293\n",
            "Time taken for 1 epoch: 2.1397905349731445 secs\n",
            "\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 12 Batch 0 Loss 1.1015 Accuracy 0.2980\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 12 Loss 1.0963 Accuracy 0.3222\n",
            "Time taken for 1 epoch: 2.1365768909454346 secs\n",
            "\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 13 Batch 0 Loss 1.0884 Accuracy 0.3485\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 13 Loss 1.0855 Accuracy 0.3303\n",
            "Time taken for 1 epoch: 2.191664695739746 secs\n",
            "\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 14 Batch 0 Loss 1.0801 Accuracy 0.3586\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 14 Loss 1.0862 Accuracy 0.3384\n",
            "Time taken for 1 epoch: 2.131291389465332 secs\n",
            "\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 15 Batch 0 Loss 1.0791 Accuracy 0.3687\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-19\n",
            "Epoch 15 Loss 1.0759 Accuracy 0.3434\n",
            "Time taken for 1 epoch: 3.632049322128296 secs\n",
            "\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 16 Batch 0 Loss 1.0893 Accuracy 0.3434\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 16 Loss 1.0783 Accuracy 0.3273\n",
            "Time taken for 1 epoch: 2.1649584770202637 secs\n",
            "\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 17 Batch 0 Loss 1.0616 Accuracy 0.3434\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 17 Loss 1.0576 Accuracy 0.3485\n",
            "Time taken for 1 epoch: 2.165020704269409 secs\n",
            "\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 18 Batch 0 Loss 1.0768 Accuracy 0.3131\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 18 Loss 1.0657 Accuracy 0.3556\n",
            "Time taken for 1 epoch: 2.0998198986053467 secs\n",
            "\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 19 Batch 0 Loss 1.0650 Accuracy 0.3283\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 19 Loss 1.0633 Accuracy 0.3374\n",
            "Time taken for 1 epoch: 2.1235835552215576 secs\n",
            "\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Epoch 20 Batch 0 Loss 1.0820 Accuracy 0.3333\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "(2, 100, 160, 160, 3) (2, 100)\n",
            "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-20\n",
            "Epoch 20 Loss 1.0666 Accuracy 0.3485\n",
            "Time taken for 1 epoch: 3.5635430812835693 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j5KfDkLzd1IX"
      },
      "source": [
        "## Evaluate\n",
        "TODO continue from [this section](https://www.tensorflow.org/tutorials/text/transformer#evaluate) onwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7veBnOn7Tum",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nhijZjpFf_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@indent_prints\n",
        "def evaluate(inp_sentence, last_action, max_len=None):\n",
        "    print(\" \"*SPACES, \"[evaluate]\")\n",
        "    print(\" \"*SPACES, \"- inp_sentence\", inp_sentence.shape)\n",
        "    print(\" \"*SPACES, \"- last_action\", len(last_action))\n",
        "    decoder_input = last_action\n",
        "    # output = tf.expand_dims(decoder_input, 0)\n",
        "    output = last_action\n",
        "    print(\" \"*SPACES, \"- output:\", output)\n",
        "\n",
        "    for i in range(MAX_LENGTH):\n",
        "        # FIXME\n",
        "        # enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "        #     encoder_input, output)\n",
        "\n",
        "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        # predictions, attention_weights = transformer(inp_sentence, \n",
        "        #                                                 output,\n",
        "        #                                                 False,\n",
        "        #                                                 enc_padding_mask,\n",
        "        #                                                 combined_mask,\n",
        "        #                                                 dec_padding_mask)\n",
        "        predictions, attention_weights = transformer(inp_sentence, decoder_input, \n",
        "                                                     False, None, None, None)\n",
        "        print(\" \"*SPACES, \"- predictions\", predictions.shape)\n",
        "\n",
        "        # select the last word from the seq_len dimension\n",
        "        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
        "        print(\" \"*SPACES, \"- predictions[:, -1:, :]\", predictions.shape)\n",
        "\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "        print(\" \"*SPACES, \"- predicted_id\", predicted_id.shape)\n",
        "\n",
        "        # return the result if the predicted_id is equal to the end token\n",
        "        # if predicted_id == tokenizer_en.vocab_size+1:\n",
        "        #     return tf.squeeze(output, axis=0), attention_weights\n",
        "        if max_len is not None and i > max_len:\n",
        "            break\n",
        "\n",
        "        # concatentate the predicted_id to the output which is given to the decoder\n",
        "        # as its input.\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "        print(\" \"*SPACES, \"- output\", output.shape)\n",
        "\n",
        "    print(\" \"*SPACES, \"- output\", output.shape)\n",
        "    try:\n",
        "        out = tf.squeeze(output, axis=0)\n",
        "        print(\" \"*SPACES, \"  (squeezed)\")\n",
        "    except:\n",
        "        out = output\n",
        "        print(\" \"*SPACES, \"  (didn't squeeze)\")\n",
        "    print(\" \"*SPACES, \"- out\", out.shape)\n",
        "    return out, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViJHO5uz9MeX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SPACES = -2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNs2aApG6kuK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "53e44e8a-335c-4721-bae0-b6f4a7cbf655"
      },
      "source": [
        "evaluate(tf.random.uniform((2, 24, 160, 160, 3), dtype=tf.float32, minval=0, maxval=255), \n",
        "         tf.random.uniform((2, 24), dtype=tf.int32, minval=0, maxval=17),\n",
        "         10)\n",
        "# images: (batch_size, seq_len, 160, 160, 3)\n",
        "# actions: (batch_size, seq_len)"
      ],
      "execution_count": 309,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " [evaluate]\n",
            " - inp_sentence (2, 24, 160, 160, 3)\n",
            " - last_action 2\n",
            " - output: tf.Tensor(\n",
            "[[ 8  4  1 16  8  7  9  0  2  8  8  3  2 11 12  8  6 11  2 11  3 14  4  1]\n",
            " [11  9  2  3  5  7 10 11 14  1  5  3  1  4  4  9 15 12 12 16  5  7 11  8]], shape=(2, 24), dtype=int32)\n",
            "   [FeatureExtractor.call]\n",
            "   - x (2, 24, 160, 160, 3)\n",
            "   - x (reshaped) (48, 160, 160, 3)\n",
            "   - x (mobile_net_v2) (48, 5, 5, 1280)\n",
            "   - x (global_average_layer) (48, 1280)\n",
            "   - x (reshaped) (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [Decoder.call]\n",
            "   - x (2, 24)\n",
            "   - embedding (2, 24, 24)\n",
            "   - pos_encoding (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            " - predictions (2, 24, 24)\n",
            " - predictions[:, -1:, :] (2, 1, 24)\n",
            " - predicted_id (2, 1)\n",
            " - output (2, 25)\n",
            "   [FeatureExtractor.call]\n",
            "   - x (2, 24, 160, 160, 3)\n",
            "   - x (reshaped) (48, 160, 160, 3)\n",
            "   - x (mobile_net_v2) (48, 5, 5, 1280)\n",
            "   - x (global_average_layer) (48, 1280)\n",
            "   - x (reshaped) (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [Decoder.call]\n",
            "   - x (2, 24)\n",
            "   - embedding (2, 24, 24)\n",
            "   - pos_encoding (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            " - predictions (2, 24, 24)\n",
            " - predictions[:, -1:, :] (2, 1, 24)\n",
            " - predicted_id (2, 1)\n",
            " - output (2, 26)\n",
            "   [FeatureExtractor.call]\n",
            "   - x (2, 24, 160, 160, 3)\n",
            "   - x (reshaped) (48, 160, 160, 3)\n",
            "   - x (mobile_net_v2) (48, 5, 5, 1280)\n",
            "   - x (global_average_layer) (48, 1280)\n",
            "   - x (reshaped) (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [Decoder.call]\n",
            "   - x (2, 24)\n",
            "   - embedding (2, 24, 24)\n",
            "   - pos_encoding (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            " - predictions (2, 24, 24)\n",
            " - predictions[:, -1:, :] (2, 1, 24)\n",
            " - predicted_id (2, 1)\n",
            " - output (2, 27)\n",
            "   [FeatureExtractor.call]\n",
            "   - x (2, 24, 160, 160, 3)\n",
            "   - x (reshaped) (48, 160, 160, 3)\n",
            "   - x (mobile_net_v2) (48, 5, 5, 1280)\n",
            "   - x (global_average_layer) (48, 1280)\n",
            "   - x (reshaped) (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [Decoder.call]\n",
            "   - x (2, 24)\n",
            "   - embedding (2, 24, 24)\n",
            "   - pos_encoding (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            " - predictions (2, 24, 24)\n",
            " - predictions[:, -1:, :] (2, 1, 24)\n",
            " - predicted_id (2, 1)\n",
            " - output (2, 28)\n",
            "   [FeatureExtractor.call]\n",
            "   - x (2, 24, 160, 160, 3)\n",
            "   - x (reshaped) (48, 160, 160, 3)\n",
            "   - x (mobile_net_v2) (48, 5, 5, 1280)\n",
            "   - x (global_average_layer) (48, 1280)\n",
            "   - x (reshaped) (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [Decoder.call]\n",
            "   - x (2, 24)\n",
            "   - embedding (2, 24, 24)\n",
            "   - pos_encoding (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            " - predictions (2, 24, 24)\n",
            " - predictions[:, -1:, :] (2, 1, 24)\n",
            " - predicted_id (2, 1)\n",
            " - output (2, 29)\n",
            "   [FeatureExtractor.call]\n",
            "   - x (2, 24, 160, 160, 3)\n",
            "   - x (reshaped) (48, 160, 160, 3)\n",
            "   - x (mobile_net_v2) (48, 5, 5, 1280)\n",
            "   - x (global_average_layer) (48, 1280)\n",
            "   - x (reshaped) (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [Decoder.call]\n",
            "   - x (2, 24)\n",
            "   - embedding (2, 24, 24)\n",
            "   - pos_encoding (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            " - predictions (2, 24, 24)\n",
            " - predictions[:, -1:, :] (2, 1, 24)\n",
            " - predicted_id (2, 1)\n",
            " - output (2, 30)\n",
            "   [FeatureExtractor.call]\n",
            "   - x (2, 24, 160, 160, 3)\n",
            "   - x (reshaped) (48, 160, 160, 3)\n",
            "   - x (mobile_net_v2) (48, 5, 5, 1280)\n",
            "   - x (global_average_layer) (48, 1280)\n",
            "   - x (reshaped) (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [Decoder.call]\n",
            "   - x (2, 24)\n",
            "   - embedding (2, 24, 24)\n",
            "   - pos_encoding (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            " - predictions (2, 24, 24)\n",
            " - predictions[:, -1:, :] (2, 1, 24)\n",
            " - predicted_id (2, 1)\n",
            " - output (2, 31)\n",
            "   [FeatureExtractor.call]\n",
            "   - x (2, 24, 160, 160, 3)\n",
            "   - x (reshaped) (48, 160, 160, 3)\n",
            "   - x (mobile_net_v2) (48, 5, 5, 1280)\n",
            "   - x (global_average_layer) (48, 1280)\n",
            "   - x (reshaped) (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [Decoder.call]\n",
            "   - x (2, 24)\n",
            "   - embedding (2, 24, 24)\n",
            "   - pos_encoding (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            " - predictions (2, 24, 24)\n",
            " - predictions[:, -1:, :] (2, 1, 24)\n",
            " - predicted_id (2, 1)\n",
            " - output (2, 32)\n",
            "   [FeatureExtractor.call]\n",
            "   - x (2, 24, 160, 160, 3)\n",
            "   - x (reshaped) (48, 160, 160, 3)\n",
            "   - x (mobile_net_v2) (48, 5, 5, 1280)\n",
            "   - x (global_average_layer) (48, 1280)\n",
            "   - x (reshaped) (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [Decoder.call]\n",
            "   - x (2, 24)\n",
            "   - embedding (2, 24, 24)\n",
            "   - pos_encoding (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            " - predictions (2, 24, 24)\n",
            " - predictions[:, -1:, :] (2, 1, 24)\n",
            " - predicted_id (2, 1)\n",
            " - output (2, 33)\n",
            "   [FeatureExtractor.call]\n",
            "   - x (2, 24, 160, 160, 3)\n",
            "   - x (reshaped) (48, 160, 160, 3)\n",
            "   - x (mobile_net_v2) (48, 5, 5, 1280)\n",
            "   - x (global_average_layer) (48, 1280)\n",
            "   - x (reshaped) (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [Decoder.call]\n",
            "   - x (2, 24)\n",
            "   - embedding (2, 24, 24)\n",
            "   - pos_encoding (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            " - predictions (2, 24, 24)\n",
            " - predictions[:, -1:, :] (2, 1, 24)\n",
            " - predicted_id (2, 1)\n",
            " - output (2, 34)\n",
            "   [FeatureExtractor.call]\n",
            "   - x (2, 24, 160, 160, 3)\n",
            "   - x (reshaped) (48, 160, 160, 3)\n",
            "   - x (mobile_net_v2) (48, 5, 5, 1280)\n",
            "   - x (global_average_layer) (48, 1280)\n",
            "   - x (reshaped) (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [Decoder.call]\n",
            "   - x (2, 24)\n",
            "   - embedding (2, 24, 24)\n",
            "   - pos_encoding (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            " - predictions (2, 24, 24)\n",
            " - predictions[:, -1:, :] (2, 1, 24)\n",
            " - predicted_id (2, 1)\n",
            " - output (2, 35)\n",
            "   [FeatureExtractor.call]\n",
            "   - x (2, 24, 160, 160, 3)\n",
            "   - x (reshaped) (48, 160, 160, 3)\n",
            "   - x (mobile_net_v2) (48, 5, 5, 1280)\n",
            "   - x (global_average_layer) (48, 1280)\n",
            "   - x (reshaped) (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [MultiHeadAttention.call]\n",
            "   - v (2, 24, 1280)\n",
            "   - k (2, 24, 1280)\n",
            "   - q (2, 24, 1280)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [split_heads]\n",
            "     - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "     - x (2, 24, 1280)\n",
            "     - x (reshape) (2, 24, 2, 640)\n",
            "     [scaled_dot_product_attention]\n",
            "     - q (2, 2, 24, 640)\n",
            "     - k (2, 2, 24, 640)\n",
            "     - v (2, 2, 24, 640)\n",
            "   - scaled_dot_product_attention (2, 2, 24, 640) (2, 2, 24, 24)\n",
            "   - attention (transposed) (2, 24, 2, 640)\n",
            "   - concat_attention (2, 24, 1280)\n",
            "   - output (2, 24, 1280)\n",
            "   [Decoder.call]\n",
            "   - x (2, 24)\n",
            "   - embedding (2, 24, 24)\n",
            "   - pos_encoding (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            "     [DecoderLayer.call]\n",
            "     - x (2, 24, 24)\n",
            "     - enc_output (2, 24, 1280)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 24)\n",
            "       - k (2, 24, 24)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn1 (2, 24, 24)\n",
            "     - out1 (2, 24, 24)\n",
            "       [MultiHeadAttention.call]\n",
            "       - v (2, 24, 1280)\n",
            "       - k (2, 24, 1280)\n",
            "       - q (2, 24, 24)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [split_heads]\n",
            "         - batch_size tf.Tensor(2, shape=(), dtype=int32)\n",
            "         - x (2, 24, 24)\n",
            "         - x (reshape) (2, 24, 2, 12)\n",
            "         [scaled_dot_product_attention]\n",
            "         - q (2, 2, 24, 12)\n",
            "         - k (2, 2, 24, 12)\n",
            "         - v (2, 2, 24, 12)\n",
            "       - scaled_dot_product_attention (2, 2, 24, 12) (2, 2, 24, 24)\n",
            "       - attention (transposed) (2, 24, 2, 12)\n",
            "       - concat_attention (2, 24, 24)\n",
            "       - output (2, 24, 24)\n",
            "     - attn2 (2, 24, 24)\n",
            "     - out2 (2, 24, 24)\n",
            "     - out3 (2, 24, 24)\n",
            " - predictions (2, 24, 24)\n",
            " - predictions[:, -1:, :] (2, 1, 24)\n",
            " - predicted_id (2, 1)\n",
            " - output (2, 35)\n",
            "   (didn't squeeze)\n",
            " - out (2, 35)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: id=613866, shape=(2, 35), dtype=int32, numpy=\n",
              " array([[ 8,  4,  1, 16,  8,  7,  9,  0,  2,  8,  8,  3,  2, 11, 12,  8,\n",
              "          6, 11,  2, 11,  3, 14,  4,  1,  2,  2,  2,  2,  2,  2,  2,  2,\n",
              "          2,  2,  2],\n",
              "        [11,  9,  2,  3,  5,  7, 10, 11, 14,  1,  5,  3,  1,  4,  4,  9,\n",
              "         15, 12, 12, 16,  5,  7, 11,  8,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "          1,  1,  1]], dtype=int32)>,\n",
              " {'decoder_layer1_block1': <tf.Tensor: id=615431, shape=(2, 2, 24, 24), dtype=float32, numpy=\n",
              "  array([[[[0.01812618, 0.02345452, 0.00266425, ..., 0.41354075,\n",
              "            0.02828045, 0.00262965],\n",
              "           [0.0146041 , 0.03165805, 0.00715527, ..., 0.28450862,\n",
              "            0.05005802, 0.01156814],\n",
              "           [0.01483813, 0.01022757, 0.00389879, ..., 0.4778958 ,\n",
              "            0.01209863, 0.00405353],\n",
              "           ...,\n",
              "           [0.00397873, 0.02233867, 0.05306857, ..., 0.01319798,\n",
              "            0.0749886 , 0.12466578],\n",
              "           [0.04303937, 0.05205029, 0.01380283, ..., 0.07389493,\n",
              "            0.06659568, 0.02010642],\n",
              "           [0.06474633, 0.01680669, 0.00614637, ..., 0.12710935,\n",
              "            0.02127094, 0.00691392]],\n",
              "  \n",
              "          [[0.09468018, 0.09926637, 0.02226158, ..., 0.09401   ,\n",
              "            0.06116812, 0.01493651],\n",
              "           [0.11665661, 0.01990243, 0.00785489, ..., 0.00226175,\n",
              "            0.05677638, 0.02365783],\n",
              "           [0.08864595, 0.01565525, 0.02635745, ..., 0.01453923,\n",
              "            0.03343799, 0.04917137],\n",
              "           ...,\n",
              "           [0.03782209, 0.04889793, 0.03441136, ..., 0.00902452,\n",
              "            0.01811843, 0.01362889],\n",
              "           [0.06634941, 0.04568302, 0.01677445, ..., 0.00346836,\n",
              "            0.12325098, 0.03502472],\n",
              "           [0.04693707, 0.01396142, 0.03235027, ..., 0.0063023 ,\n",
              "            0.05296067, 0.07915262]]],\n",
              "  \n",
              "  \n",
              "         [[[0.00332269, 0.06814324, 0.00466873, ..., 0.01223239,\n",
              "            0.0041969 , 0.01145561],\n",
              "           [0.01481618, 0.00510495, 0.02700331, ..., 0.02098983,\n",
              "            0.11024848, 0.01013931],\n",
              "           [0.0158409 , 0.01680316, 0.03064586, ..., 0.05290103,\n",
              "            0.03211194, 0.02668938],\n",
              "           ...,\n",
              "           [0.01196547, 0.01796792, 0.03805599, ..., 0.0532818 ,\n",
              "            0.05619454, 0.03896926],\n",
              "           [0.01726302, 0.07159916, 0.02414215, ..., 0.05797805,\n",
              "            0.01587673, 0.03027838],\n",
              "           [0.0083311 , 0.03367655, 0.02785931, ..., 0.03813157,\n",
              "            0.00872128, 0.08548034]],\n",
              "  \n",
              "          [[0.00561939, 0.00813737, 0.01154309, ..., 0.06663302,\n",
              "            0.01092996, 0.06055779],\n",
              "           [0.06622787, 0.02852251, 0.02388352, ..., 0.00840871,\n",
              "            0.01987832, 0.02249872],\n",
              "           [0.0024102 , 0.14297958, 0.00839841, ..., 0.00432232,\n",
              "            0.00082549, 0.1378703 ],\n",
              "           ...,\n",
              "           [0.16894194, 0.00227723, 0.01692259, ..., 0.002655  ,\n",
              "            0.16724837, 0.00202945],\n",
              "           [0.02035278, 0.01107794, 0.03150177, ..., 0.06709163,\n",
              "            0.03537478, 0.01629817],\n",
              "           [0.07312425, 0.0233609 , 0.12712502, ..., 0.04964654,\n",
              "            0.05009742, 0.02002519]]]], dtype=float32)>,\n",
              "  'decoder_layer1_block2': <tf.Tensor: id=615548, shape=(2, 2, 24, 24), dtype=float32, numpy=\n",
              "  array([[[[0.04129535, 0.04180752, 0.04155675, ..., 0.04169029,\n",
              "            0.04193792, 0.04158447],\n",
              "           [0.04113103, 0.04154601, 0.04183544, ..., 0.04194229,\n",
              "            0.04187761, 0.04162828],\n",
              "           [0.04134014, 0.04152194, 0.04173709, ..., 0.04179689,\n",
              "            0.04217108, 0.04152963],\n",
              "           ...,\n",
              "           [0.04157513, 0.04126749, 0.04157443, ..., 0.04238349,\n",
              "            0.04193991, 0.04135903],\n",
              "           [0.04133092, 0.04145005, 0.04169492, ..., 0.0421955 ,\n",
              "            0.04182677, 0.0417317 ],\n",
              "           [0.04160338, 0.04141383, 0.04154179, ..., 0.04205199,\n",
              "            0.04200033, 0.04163226]],\n",
              "  \n",
              "          [[0.04122216, 0.04166059, 0.04166494, ..., 0.04168793,\n",
              "            0.04186328, 0.0416385 ],\n",
              "           [0.04103335, 0.04171307, 0.04142411, ..., 0.04175697,\n",
              "            0.04181209, 0.04181022],\n",
              "           [0.0416574 , 0.04164727, 0.04195847, ..., 0.04173984,\n",
              "            0.04169295, 0.04159778],\n",
              "           ...,\n",
              "           [0.04146946, 0.0417446 , 0.0418863 , ..., 0.04186743,\n",
              "            0.04172003, 0.04174343],\n",
              "           [0.04129781, 0.04177485, 0.04164683, ..., 0.04173622,\n",
              "            0.04162418, 0.04189249],\n",
              "           [0.04181825, 0.04167686, 0.04204825, ..., 0.04179075,\n",
              "            0.04168088, 0.04165706]]],\n",
              "  \n",
              "  \n",
              "         [[[0.04261552, 0.0414905 , 0.04104576, ..., 0.04225331,\n",
              "            0.04146831, 0.04172193],\n",
              "           [0.04273286, 0.04135338, 0.04115778, ..., 0.0415168 ,\n",
              "            0.04141161, 0.04138726],\n",
              "           [0.04206549, 0.04125118, 0.04139064, ..., 0.04168449,\n",
              "            0.04158336, 0.04170272],\n",
              "           ...,\n",
              "           [0.04190369, 0.0414752 , 0.04172993, ..., 0.04132529,\n",
              "            0.04151748, 0.04158134],\n",
              "           [0.04257791, 0.04139864, 0.04111652, ..., 0.04194251,\n",
              "            0.0415418 , 0.0417487 ],\n",
              "           [0.04154596, 0.04153842, 0.04178826, ..., 0.04168454,\n",
              "            0.04154611, 0.04173313]],\n",
              "  \n",
              "          [[0.04121369, 0.04165513, 0.04143422, ..., 0.04160842,\n",
              "            0.04200936, 0.04132103],\n",
              "           [0.04127959, 0.04157076, 0.04162917, ..., 0.04130147,\n",
              "            0.04195147, 0.04142094],\n",
              "           [0.04115787, 0.04205302, 0.04136608, ..., 0.04183805,\n",
              "            0.04190257, 0.0417251 ],\n",
              "           ...,\n",
              "           [0.04154283, 0.04158242, 0.04134997, ..., 0.04191074,\n",
              "            0.04194041, 0.04159839],\n",
              "           [0.04148754, 0.04174291, 0.04137794, ..., 0.04175603,\n",
              "            0.04192904, 0.04139927],\n",
              "           [0.04160929, 0.04199324, 0.04163158, ..., 0.04193944,\n",
              "            0.04152655, 0.04148406]]]], dtype=float32)>,\n",
              "  'decoder_layer2_block1': <tf.Tensor: id=615712, shape=(2, 2, 24, 24), dtype=float32, numpy=\n",
              "  array([[[[0.05344756, 0.07971385, 0.17180784, ..., 0.00345295,\n",
              "            0.04091961, 0.06060958],\n",
              "           [0.01098383, 0.04323027, 0.02515472, ..., 0.0124289 ,\n",
              "            0.08499056, 0.06278488],\n",
              "           [0.02140206, 0.05872388, 0.03137859, ..., 0.01037227,\n",
              "            0.07045479, 0.05136401],\n",
              "           ...,\n",
              "           [0.04565568, 0.07603931, 0.16170883, ..., 0.02277019,\n",
              "            0.03317969, 0.05053459],\n",
              "           [0.00890995, 0.05769203, 0.03117575, ..., 0.00872916,\n",
              "            0.10287358, 0.06367211],\n",
              "           [0.01468357, 0.0748999 , 0.05325601, ..., 0.01439077,\n",
              "            0.08362681, 0.06473195]],\n",
              "  \n",
              "          [[0.01202286, 0.03792772, 0.02803869, ..., 0.04602569,\n",
              "            0.06651972, 0.05703089],\n",
              "           [0.03683183, 0.0371428 , 0.04237246, ..., 0.05533981,\n",
              "            0.04812404, 0.07518255],\n",
              "           [0.09949752, 0.0237333 , 0.02646128, ..., 0.06125755,\n",
              "            0.03220407, 0.0431524 ],\n",
              "           ...,\n",
              "           [0.03655142, 0.02310409, 0.02647085, ..., 0.08083879,\n",
              "            0.04161833, 0.07350965],\n",
              "           [0.0475177 , 0.0264318 , 0.03786715, ..., 0.04214946,\n",
              "            0.0382891 , 0.07751778],\n",
              "           [0.08068247, 0.01220924, 0.01917658, ..., 0.02802576,\n",
              "            0.01908072, 0.03587236]]],\n",
              "  \n",
              "  \n",
              "         [[[0.03137443, 0.02261487, 0.04033284, ..., 0.02612556,\n",
              "            0.05799843, 0.03875841],\n",
              "           [0.06016868, 0.03246753, 0.02033281, ..., 0.03025088,\n",
              "            0.04916135, 0.02727913],\n",
              "           [0.02076123, 0.0175269 , 0.02397712, ..., 0.01160248,\n",
              "            0.01285374, 0.1442788 ],\n",
              "           ...,\n",
              "           [0.08515262, 0.03287867, 0.01941641, ..., 0.02900863,\n",
              "            0.05862938, 0.026511  ],\n",
              "           [0.02471305, 0.01882846, 0.03532947, ..., 0.0164652 ,\n",
              "            0.05210243, 0.03106695],\n",
              "           [0.08189447, 0.02493227, 0.01559995, ..., 0.00850724,\n",
              "            0.05667781, 0.01845948]],\n",
              "  \n",
              "          [[0.04766508, 0.05351985, 0.01909743, ..., 0.0249358 ,\n",
              "            0.03118974, 0.03408147],\n",
              "           [0.06458196, 0.02119636, 0.00830209, ..., 0.04821189,\n",
              "            0.07384557, 0.06460636],\n",
              "           [0.01644162, 0.0051226 , 0.00345828, ..., 0.04380754,\n",
              "            0.04587534, 0.12663953],\n",
              "           ...,\n",
              "           [0.06789407, 0.0352683 , 0.00940763, ..., 0.01606862,\n",
              "            0.05905756, 0.02918398],\n",
              "           [0.03843862, 0.04615289, 0.01592056, ..., 0.03635571,\n",
              "            0.02566841, 0.05338334],\n",
              "           [0.01697944, 0.02300949, 0.02365436, ..., 0.02211689,\n",
              "            0.04499161, 0.05839045]]]], dtype=float32)>,\n",
              "  'decoder_layer2_block2': <tf.Tensor: id=615829, shape=(2, 2, 24, 24), dtype=float32, numpy=\n",
              "  array([[[[0.04169471, 0.04158718, 0.04176167, ..., 0.04146598,\n",
              "            0.04189896, 0.04163129],\n",
              "           [0.04112513, 0.04097103, 0.04160106, ..., 0.04213057,\n",
              "            0.04189817, 0.04150834],\n",
              "           [0.04124196, 0.04132798, 0.041548  , ..., 0.04205277,\n",
              "            0.04214902, 0.04158233],\n",
              "           ...,\n",
              "           [0.0411348 , 0.0412232 , 0.04157827, ..., 0.04210892,\n",
              "            0.04207522, 0.04160959],\n",
              "           [0.04136907, 0.04111429, 0.0416198 , ..., 0.0420408 ,\n",
              "            0.04190333, 0.04177065],\n",
              "           [0.04157083, 0.04154771, 0.04157235, ..., 0.0419465 ,\n",
              "            0.04211076, 0.04193532]],\n",
              "  \n",
              "          [[0.04154338, 0.0415632 , 0.04162348, ..., 0.0414869 ,\n",
              "            0.04232526, 0.0411195 ],\n",
              "           [0.04182141, 0.04147342, 0.04193492, ..., 0.04181987,\n",
              "            0.0419693 , 0.04140529],\n",
              "           [0.04197935, 0.04187313, 0.04176619, ..., 0.04115606,\n",
              "            0.04227136, 0.04160574],\n",
              "           ...,\n",
              "           [0.04159079, 0.04129168, 0.04197422, ..., 0.04168976,\n",
              "            0.04196677, 0.04148505],\n",
              "           [0.04168778, 0.04134022, 0.041943  , ..., 0.04202598,\n",
              "            0.04178295, 0.04132308],\n",
              "           [0.04176119, 0.04169406, 0.0417637 , ..., 0.04138603,\n",
              "            0.04207946, 0.04147442]]],\n",
              "  \n",
              "  \n",
              "         [[[0.0419562 , 0.0412945 , 0.04169191, ..., 0.04174115,\n",
              "            0.04176097, 0.04159134],\n",
              "           [0.04169552, 0.04142713, 0.04160074, ..., 0.04176692,\n",
              "            0.04166737, 0.04163722],\n",
              "           [0.04158312, 0.0419962 , 0.04156508, ..., 0.04175016,\n",
              "            0.04122842, 0.04175749],\n",
              "           ...,\n",
              "           [0.04148065, 0.04152317, 0.04176711, ..., 0.0418554 ,\n",
              "            0.04173437, 0.04147056],\n",
              "           [0.04183904, 0.0412007 , 0.04169952, ..., 0.04188822,\n",
              "            0.04188896, 0.04146112],\n",
              "           [0.041491  , 0.04140847, 0.04185485, ..., 0.04219264,\n",
              "            0.04185968, 0.04160687]],\n",
              "  \n",
              "          [[0.04163707, 0.04218661, 0.04156953, ..., 0.04220679,\n",
              "            0.041466  , 0.04214393],\n",
              "           [0.0419889 , 0.04199592, 0.04108558, ..., 0.04219778,\n",
              "            0.04142932, 0.04209622],\n",
              "           [0.04184634, 0.04212715, 0.04135996, ..., 0.04194704,\n",
              "            0.04131178, 0.04153262],\n",
              "           ...,\n",
              "           [0.0417265 , 0.04203245, 0.04117481, ..., 0.04220009,\n",
              "            0.04143661, 0.04219036],\n",
              "           [0.0414138 , 0.04184661, 0.04143334, ..., 0.04217279,\n",
              "            0.04152904, 0.0422743 ],\n",
              "           [0.0414713 , 0.04175024, 0.04112086, ..., 0.04220617,\n",
              "            0.04129879, 0.04231996]]]], dtype=float32)>,\n",
              "  'decoder_layer3_block1': <tf.Tensor: id=615993, shape=(2, 2, 24, 24), dtype=float32, numpy=\n",
              "  array([[[[0.05468147, 0.0187643 , 0.01486617, ..., 0.02259541,\n",
              "            0.02472542, 0.026606  ],\n",
              "           [0.04433979, 0.02732825, 0.01286185, ..., 0.03429554,\n",
              "            0.03711563, 0.01807787],\n",
              "           [0.05633659, 0.04076719, 0.02978142, ..., 0.03564695,\n",
              "            0.038053  , 0.03033575],\n",
              "           ...,\n",
              "           [0.04887547, 0.02377338, 0.01220382, ..., 0.03389023,\n",
              "            0.04215813, 0.02450385],\n",
              "           [0.05167577, 0.02907143, 0.01851511, ..., 0.0379812 ,\n",
              "            0.03704506, 0.02236584],\n",
              "           [0.06154903, 0.03460425, 0.05031724, ..., 0.03608096,\n",
              "            0.02834618, 0.04187966]],\n",
              "  \n",
              "          [[0.05116964, 0.01923256, 0.01900788, ..., 0.02291497,\n",
              "            0.01738689, 0.011359  ],\n",
              "           [0.04103818, 0.01518557, 0.01156835, ..., 0.02265185,\n",
              "            0.0194841 , 0.00999434],\n",
              "           [0.04871735, 0.03392658, 0.01588257, ..., 0.02000927,\n",
              "            0.02909298, 0.00853146],\n",
              "           ...,\n",
              "           [0.04962647, 0.01898856, 0.01656399, ..., 0.01994583,\n",
              "            0.02224848, 0.01289625],\n",
              "           [0.04158842, 0.01898137, 0.01672789, ..., 0.02444565,\n",
              "            0.02020318, 0.0132627 ],\n",
              "           [0.04690715, 0.03534992, 0.02306969, ..., 0.02297729,\n",
              "            0.02855574, 0.01436538]]],\n",
              "  \n",
              "  \n",
              "         [[[0.01819087, 0.01547708, 0.05143219, ..., 0.01863276,\n",
              "            0.02405445, 0.10850193],\n",
              "           [0.01192602, 0.02524962, 0.05784336, ..., 0.02413657,\n",
              "            0.02066554, 0.14849241],\n",
              "           [0.00772826, 0.0423773 , 0.08022919, ..., 0.02579293,\n",
              "            0.00974853, 0.07149462],\n",
              "           ...,\n",
              "           [0.01301878, 0.02963955, 0.05364935, ..., 0.03744169,\n",
              "            0.02112991, 0.14496431],\n",
              "           [0.02171228, 0.02365174, 0.04049124, ..., 0.02876743,\n",
              "            0.02676856, 0.11083482],\n",
              "           [0.01391417, 0.03005097, 0.03614957, ..., 0.03821697,\n",
              "            0.01858642, 0.13167149]],\n",
              "  \n",
              "          [[0.00369282, 0.02745444, 0.16660865, ..., 0.0279761 ,\n",
              "            0.00486955, 0.06025548],\n",
              "           [0.00610342, 0.0378572 , 0.1953926 , ..., 0.0438314 ,\n",
              "            0.00790733, 0.10507652],\n",
              "           [0.00884535, 0.03913537, 0.1336134 , ..., 0.02949888,\n",
              "            0.01115903, 0.07701641],\n",
              "           ...,\n",
              "           [0.01080314, 0.04837641, 0.16413043, ..., 0.03960625,\n",
              "            0.01041426, 0.07179988],\n",
              "           [0.00953202, 0.04187242, 0.17621817, ..., 0.02615685,\n",
              "            0.00833435, 0.05414582],\n",
              "           [0.0184088 , 0.0490531 , 0.10986812, ..., 0.03024878,\n",
              "            0.01394396, 0.07047318]]]], dtype=float32)>,\n",
              "  'decoder_layer3_block2': <tf.Tensor: id=616110, shape=(2, 2, 24, 24), dtype=float32, numpy=\n",
              "  array([[[[0.04204988, 0.04163805, 0.04184032, ..., 0.04150924,\n",
              "            0.04183088, 0.04177754],\n",
              "           [0.04190964, 0.04173612, 0.04183343, ..., 0.0414506 ,\n",
              "            0.0419595 , 0.04175754],\n",
              "           [0.04164251, 0.04157824, 0.04179785, ..., 0.0415866 ,\n",
              "            0.04206922, 0.04173849],\n",
              "           ...,\n",
              "           [0.04208109, 0.04147539, 0.0418589 , ..., 0.04159389,\n",
              "            0.04167984, 0.04187237],\n",
              "           [0.04192162, 0.04174415, 0.04182952, ..., 0.04150457,\n",
              "            0.04187817, 0.04176679],\n",
              "           [0.041707  , 0.04159888, 0.04182558, ..., 0.04159253,\n",
              "            0.04182678, 0.04182488]],\n",
              "  \n",
              "          [[0.04155971, 0.04171944, 0.04171798, ..., 0.04133073,\n",
              "            0.04199034, 0.04148526],\n",
              "           [0.04121167, 0.04212768, 0.0414681 , ..., 0.04152103,\n",
              "            0.04188705, 0.04146514],\n",
              "           [0.04125486, 0.04202679, 0.04162855, ..., 0.04100469,\n",
              "            0.04209246, 0.04132869],\n",
              "           ...,\n",
              "           [0.04143479, 0.0418961 , 0.04154044, ..., 0.04149628,\n",
              "            0.04204801, 0.04144959],\n",
              "           [0.04113615, 0.042116  , 0.04148962, ..., 0.0416628 ,\n",
              "            0.04185202, 0.04142114],\n",
              "           [0.04113964, 0.04195994, 0.04166341, ..., 0.04125598,\n",
              "            0.0420915 , 0.04128519]]],\n",
              "  \n",
              "  \n",
              "         [[[0.04177149, 0.04167127, 0.04170241, ..., 0.04139046,\n",
              "            0.04173362, 0.04156743],\n",
              "           [0.04172526, 0.04176057, 0.0416896 , ..., 0.04103747,\n",
              "            0.0417126 , 0.04159437],\n",
              "           [0.04154781, 0.04172281, 0.04163938, ..., 0.0413314 ,\n",
              "            0.04166208, 0.04159595],\n",
              "           ...,\n",
              "           [0.04197547, 0.0417446 , 0.04161139, ..., 0.04133841,\n",
              "            0.04165315, 0.04170506],\n",
              "           [0.04187632, 0.04165324, 0.0416668 , ..., 0.04156205,\n",
              "            0.04164514, 0.0415884 ],\n",
              "           [0.04171902, 0.04179991, 0.04167672, ..., 0.04138346,\n",
              "            0.04169501, 0.04157963]],\n",
              "  \n",
              "          [[0.04243641, 0.04168721, 0.04138387, ..., 0.04154719,\n",
              "            0.04179759, 0.04132564],\n",
              "           [0.04215192, 0.04159842, 0.04150384, ..., 0.04150255,\n",
              "            0.04198078, 0.04141938],\n",
              "           [0.04202335, 0.0415385 , 0.04156494, ..., 0.04145386,\n",
              "            0.04194287, 0.04129188],\n",
              "           ...,\n",
              "           [0.04254951, 0.04134756, 0.04139918, ..., 0.04140588,\n",
              "            0.04184469, 0.04124252],\n",
              "           [0.0425615 , 0.04165986, 0.04137552, ..., 0.04154194,\n",
              "            0.04177828, 0.04127285],\n",
              "           [0.04220872, 0.04150589, 0.04140678, ..., 0.04151002,\n",
              "            0.04190103, 0.04132223]]]], dtype=float32)>,\n",
              "  'decoder_layer4_block1': <tf.Tensor: id=616274, shape=(2, 2, 24, 24), dtype=float32, numpy=\n",
              "  array([[[[0.03334875, 0.03633003, 0.05848417, ..., 0.04643801,\n",
              "            0.0342929 , 0.06016257],\n",
              "           [0.03543728, 0.03786705, 0.06128286, ..., 0.04099561,\n",
              "            0.036259  , 0.05972847],\n",
              "           [0.03459948, 0.03446561, 0.05265211, ..., 0.04324029,\n",
              "            0.03412103, 0.06769989],\n",
              "           ...,\n",
              "           [0.03487366, 0.03340966, 0.06106464, ..., 0.0440584 ,\n",
              "            0.03040979, 0.05795574],\n",
              "           [0.03464918, 0.03733273, 0.06227694, ..., 0.04274581,\n",
              "            0.03497934, 0.05892239],\n",
              "           [0.03321264, 0.03322747, 0.05688135, ..., 0.04492474,\n",
              "            0.03231522, 0.06684767]],\n",
              "  \n",
              "          [[0.03544289, 0.05040307, 0.02032141, ..., 0.05288545,\n",
              "            0.05566207, 0.02336443],\n",
              "           [0.03707424, 0.05166031, 0.01906645, ..., 0.05164828,\n",
              "            0.06144636, 0.02621879],\n",
              "           [0.04498944, 0.04208199, 0.0198678 , ..., 0.0469741 ,\n",
              "            0.04393294, 0.0214396 ],\n",
              "           ...,\n",
              "           [0.04086051, 0.05018091, 0.01808192, ..., 0.05127972,\n",
              "            0.05469721, 0.02317185],\n",
              "           [0.04079319, 0.04835388, 0.01743312, ..., 0.05091726,\n",
              "            0.05651446, 0.02447973],\n",
              "           [0.04685399, 0.04212312, 0.01755767, ..., 0.04705396,\n",
              "            0.04420399, 0.01971355]]],\n",
              "  \n",
              "  \n",
              "         [[[0.05607438, 0.03180294, 0.03658954, ..., 0.03544089,\n",
              "            0.04400811, 0.02997907],\n",
              "           [0.05415004, 0.03461789, 0.04043277, ..., 0.03916938,\n",
              "            0.04440752, 0.03205065],\n",
              "           [0.05529789, 0.03484138, 0.03920928, ..., 0.03938236,\n",
              "            0.04330051, 0.03063859],\n",
              "           ...,\n",
              "           [0.05560477, 0.03459186, 0.03721276, ..., 0.04149211,\n",
              "            0.04682693, 0.03107143],\n",
              "           [0.05584036, 0.03312178, 0.03665588, ..., 0.03782059,\n",
              "            0.04437578, 0.03024117],\n",
              "           [0.05664056, 0.03879448, 0.03713064, ..., 0.04113644,\n",
              "            0.04593129, 0.02886914]],\n",
              "  \n",
              "          [[0.04549019, 0.04506134, 0.03937498, ..., 0.04057271,\n",
              "            0.04113191, 0.04239163],\n",
              "           [0.04448619, 0.04443736, 0.03603487, ..., 0.04103786,\n",
              "            0.04312622, 0.04434907],\n",
              "           [0.0476719 , 0.04740341, 0.03986929, ..., 0.0450926 ,\n",
              "            0.03950979, 0.04278198],\n",
              "           ...,\n",
              "           [0.04709523, 0.04607367, 0.03917877, ..., 0.04207498,\n",
              "            0.04189571, 0.04336328],\n",
              "           [0.04476344, 0.04258683, 0.03907838, ..., 0.04034355,\n",
              "            0.03883271, 0.04818084],\n",
              "           [0.04804001, 0.04697726, 0.04199619, ..., 0.04111223,\n",
              "            0.04314631, 0.03971262]]]], dtype=float32)>,\n",
              "  'decoder_layer4_block2': <tf.Tensor: id=616391, shape=(2, 2, 24, 24), dtype=float32, numpy=\n",
              "  array([[[[0.04150017, 0.04202232, 0.04111052, ..., 0.0415615 ,\n",
              "            0.04227008, 0.0414768 ],\n",
              "           [0.04145405, 0.04188938, 0.04126098, ..., 0.04164885,\n",
              "            0.04209852, 0.04143621],\n",
              "           [0.0415545 , 0.04201126, 0.0411513 , ..., 0.04171712,\n",
              "            0.04215443, 0.04157631],\n",
              "           ...,\n",
              "           [0.04153007, 0.0418376 , 0.04125625, ..., 0.04178963,\n",
              "            0.04213875, 0.04138976],\n",
              "           [0.0414737 , 0.04186795, 0.04132208, ..., 0.04167468,\n",
              "            0.04199929, 0.04152123],\n",
              "           [0.041541  , 0.04192898, 0.04127891, ..., 0.04174687,\n",
              "            0.04199805, 0.04162445]],\n",
              "  \n",
              "          [[0.04173895, 0.04130863, 0.04213811, ..., 0.04201887,\n",
              "            0.04225921, 0.04143053],\n",
              "           [0.04173139, 0.0415216 , 0.04211211, ..., 0.0417143 ,\n",
              "            0.04206833, 0.04148398],\n",
              "           [0.04179755, 0.04141043, 0.04212903, ..., 0.0420129 ,\n",
              "            0.04199225, 0.04152071],\n",
              "           ...,\n",
              "           [0.04178108, 0.04142059, 0.04211419, ..., 0.04177802,\n",
              "            0.04211019, 0.0415008 ],\n",
              "           [0.04168085, 0.04153957, 0.04206724, ..., 0.04175463,\n",
              "            0.04208909, 0.04147094],\n",
              "           [0.04178024, 0.04144188, 0.04208935, ..., 0.04200922,\n",
              "            0.04199827, 0.04153822]]],\n",
              "  \n",
              "  \n",
              "         [[[0.04212717, 0.04167989, 0.04151984, ..., 0.04196537,\n",
              "            0.04149848, 0.04176409],\n",
              "           [0.04211872, 0.04178074, 0.04148056, ..., 0.04200426,\n",
              "            0.04145203, 0.04182748],\n",
              "           [0.04208593, 0.04177165, 0.04161526, ..., 0.04192292,\n",
              "            0.04141392, 0.0418052 ],\n",
              "           ...,\n",
              "           [0.04206282, 0.0417186 , 0.04151861, ..., 0.04199415,\n",
              "            0.04148904, 0.04180918],\n",
              "           [0.04198899, 0.04164339, 0.0416049 , ..., 0.04195452,\n",
              "            0.04148515, 0.04177031],\n",
              "           [0.04213915, 0.04179044, 0.04158981, ..., 0.0418908 ,\n",
              "            0.04149003, 0.04178081]],\n",
              "  \n",
              "          [[0.0415814 , 0.04172132, 0.04151135, ..., 0.04201332,\n",
              "            0.04195861, 0.04171108],\n",
              "           [0.04162892, 0.04179744, 0.04141575, ..., 0.04203944,\n",
              "            0.04185528, 0.04167608],\n",
              "           [0.0417009 , 0.04175394, 0.04147788, ..., 0.04190797,\n",
              "            0.04177088, 0.04171064],\n",
              "           ...,\n",
              "           [0.04159395, 0.04178897, 0.04148981, ..., 0.04197752,\n",
              "            0.04183495, 0.04169861],\n",
              "           [0.04165744, 0.0417175 , 0.04153154, ..., 0.04198656,\n",
              "            0.04188384, 0.04171001],\n",
              "           [0.04166085, 0.04176287, 0.04143542, ..., 0.04198858,\n",
              "            0.04180287, 0.04167736]]]], dtype=float32)>})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 309
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoqKvu8A7Ar_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}