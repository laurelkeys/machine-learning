{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YKhHc0XN5ujk"
   },
   "source": [
    "## Importing libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HLhIWQRC5ujn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_4cYzKSI5ujs"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [2, 2]\n",
    "sns.set() # apply the seaborn defaults to plotted figures (e.g. theme, scaling, color palette), instead of matplotlib's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SPsGc8lu5uj7"
   },
   "source": [
    "## Neural Network\n",
    "\n",
    "Being $m$ the number of samples in a batch, from a layer $k-1$ to a layer $k$ we have:\n",
    "- Weights $\\mathbb{W}^{(k)} \\in \\mathbb{R}^{n_{k-1} \\times n_k}$\n",
    "- Biases $\\mathbf{b}^{(k)} \\in \\mathbb{R}^{n_k}$\n",
    "- Activations $\\mathbb{A}^{(k)} = g_k(\\mathbb{Z}^{(k)}) \\in \\mathbb{R}^{m \\times n_k}$, where $g_k(\\mathbb{Z}^{(k)})$ is the activation function of the $k^{\\text{th}}$ layer and $\\mathbb{Z}^{(k)} = \\mathbb{A}^{(k-1)} \\mathbb{W}^{(k)} + \\mathbf{b}^{(k)}$\n",
    "\n",
    "(Xavier initialization: [[1]](https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/))\n",
    "\n",
    "For the first layer, the activation is the input itself: $\\mathbb{A}^{(1)} = \\mathbb{X} \\in \\mathbb{R}^{m \\times n_1}$, where $n_1$ is the input size (3072)  \n",
    "For the middle layers ($2 \\leq k < L$), the activation function is the sigmoid: $\\mathbb{A}^{(k)} = g_k(\\mathbb{Z}^{(k)}) = sigmoid(\\mathbb{Z}^{(k)})$  \n",
    "For the last layer, we have the predicted value with softmax activation: $\\mathbb{A}^{(L)} = g_k(\\mathbb{Z}^{(L)}) = softmax(\\mathbb{Z}^{(L)}) \\in \\mathbb{R}^{m \\times n_L}$, where $n_L$ is the output size (10)  \n",
    "(i.e. the hypothesis function $a^{(L)} = h_{W, b}(x) = y_{\\text{pred}} \\approx y$)\n",
    "\n",
    "obs.: the number of layers $L$ comes from: $1$ input layer + $1$ output layer + $L-2$ hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W4bmjMEg5uj8"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# RANDOM_SEED = 886"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_UMozuYp5ukB"
   },
   "outputs": [],
   "source": [
    "class ActivationFunction:\n",
    "    ''' An ActivationFunction is applied to Z to get the output A, \n",
    "        but its derivative expects the value A, not Z (!):\n",
    "\n",
    "        A == __call__(Z) and derivative(A) == derivative(__call__(Z)), \n",
    "        calling derivative(Z) will often yield WRONG results\n",
    "    '''\n",
    "    def __call__(self, Z):\n",
    "        ''' Z.shape=(n_examples, layer_output_size) '''\n",
    "        raise NotImplementedError    \n",
    "    def derivative(self, A):\n",
    "        ''' A.shape=(n_examples, layer_output_size) '''\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Linear(ActivationFunction):\n",
    "    def __call__(self, Z):\n",
    "        return Z\n",
    "    def derivative(self, A):\n",
    "        return np.ones_like(A)\n",
    "\n",
    "class Sigmoid(ActivationFunction):\n",
    "    def __call__(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    def derivative(self, A):\n",
    "        return A * (1 - A) # Sigmoid(Z) * (1 - Sigmoid(Z))\n",
    "\n",
    "class ReLU(ActivationFunction):\n",
    "    def __call__(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "    def derivative(self, A):\n",
    "        return np.where(A > 0, 1, 0)\n",
    "\n",
    "class SoftMax(ActivationFunction):\n",
    "    def __call__(self, Z):\n",
    "        exp = np.exp(Z - Z.max(axis=1, keepdims=True))\n",
    "        return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "    def derivative(self, A):\n",
    "        return A * (1 - A) # SoftMax(Z) * (1 - SoftMax(Z))\n",
    "    \n",
    "        #S = A.reshape(-1, m)\n",
    "        #return np.diagflat(S) - np.dot(S, S.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obs.: $\\mathbf{e}^{x+c}$ / sum($\\mathbf{e}^{x+c}$) == ($\\mathbf{e}^x$ * $\\mathbf{e}^c$) / ($\\mathbf{e}^c$ * sum($\\mathbf{e}^x$)) == $\\mathbf{e}^x$ / sum($\\mathbf{e}^x$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uj575tAo5ukF"
   },
   "outputs": [],
   "source": [
    "class CostFunction:\n",
    "    ''' A CostFunction is applied to Y (the target values) and Ypred to get a scalar output\n",
    "        Its derivative w.r.t. Ypred also expects Y and Ypred, but returns tensor (n_examples, last_layer_output_size)\n",
    "        \n",
    "        obs.: Ypred is the last layer's activation values: last_layer.A == last_layer.g(last.layer.Z)\n",
    "    '''\n",
    "    def __call__(self, Y, Ypred):\n",
    "        ''' Y.shape == Ypred.shape == (n_examples, last_layer_output_size) '''\n",
    "        raise NotImplementedError # [J(Y, Ypred)]\n",
    "    def derivative(self, Y, Ypred):\n",
    "        ''' Y.shape == Ypred.shape == (n_examples, last_layer_output_size) '''\n",
    "        raise NotImplementedError # [dJ/dYpred]\n",
    "\n",
    "class CrossEntropy(CostFunction):\n",
    "    def __call__(self, Y, Ypred):\n",
    "        return np.mean( -(Y * np.log(Ypred)).sum(axis=1) ) ## == - (1/m) * (Y * np.log(Ypred)).sum(axis=(0, 1))\n",
    "    def derivative(self, Y, Ypred):\n",
    "        m = Ypred.shape[0]\n",
    "        return - (Y / Ypred) / m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obs.:\n",
    "\n",
    "---\n",
    "```python\n",
    "CrossEntropy.derivative(Y, Ypred) * SoftMax.derivative(Ypred) ==  \n",
    "      - (1/m) * (Y / Ypred)       *    Ypred * (1 - Ypred)    == - (1/m) * Y * (1 - Ypred)  \n",
    "                                                              == - (1/m) * (Y - Y*Ypred)  \n",
    "                                                              == (Y*Ypred - Y) / m```\n",
    "---  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME return (Ypred - Y) / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    ''' The optimizer's optimization policy should be implemented on its update(layers) function '''\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "    def update(self, layers):\n",
    "        ''' Updates the parameters (i.e. weights and biases) for each layer in the layers list '''\n",
    "        raise NotImplementedError\n",
    "\n",
    "class GradientDescent(Optimizer):\n",
    "    def __init__(self, learning_rate):\n",
    "        super().__init__(learning_rate)\n",
    "    def update(self, layers):\n",
    "        for layer in layers[1:]:\n",
    "            layer.W -= self.learning_rate * layer.dW\n",
    "            layer.b -= self.learning_rate * layer.db   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BQMK9K0r5ukJ"
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    ''' A.shape == (n_examples, output_size)\n",
    "        Z.shape == (n_examples, output_size)\n",
    "        W.shape == (input_size, output_size)\n",
    "        b.shape == (output_size, )\n",
    "        obs.:\n",
    "            input_size == prev_layer.output_size\n",
    "            output_size == next_layer.input_size\n",
    "    '''\n",
    "    def __init__(self, output_size, activation_function):\n",
    "        if activation_function != None:\n",
    "            assert(isinstance(activation_function, ActivationFunction)), \"Invalid object type for activation_function\"\n",
    "        else:\n",
    "            warnings.warn(\"\\nThe activation_function isn't set for this layer (this is only ok if it is the input layer)\")\n",
    "        \n",
    "        self.input_size = None\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # activation function\n",
    "        self.g = activation_function # g_prime == activation_function.derivative\n",
    "        \n",
    "        # activation values\n",
    "        self.A = None # self.A == self.g(self.Z)\n",
    "        self.Z = None # prev_layer.A @ self.W + self.b\n",
    "        \n",
    "        # output value of the previous layer\n",
    "        self.X = None # == prev_layer.A\n",
    "        \n",
    "        # parameters (weights and biases)\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def init(self, input_size, weight_initialization):\n",
    "        ''' Sets the layer's input_size and initializes its weights and biases '''\n",
    "        self.input_size = input_size\n",
    "        if weight_initialization == 'xavier':\n",
    "            stddev = np.sqrt(1 / self.input_size)\n",
    "            self.W = stddev * np.random.randn(self.input_size, self.output_size)\n",
    "            self.b = np.random.randn(self.output_size, )\n",
    "        elif weight_initialization == 'xavier_avg':\n",
    "            stddev = np.sqrt(2 / (self.input_size + self.output_size))\n",
    "            self.W = stddev * np.random.randn(self.input_size, self.output_size)\n",
    "            self.b = np.random.randn(self.output_size, )\n",
    "        elif weight_initialization == 'rand_-1_to_1':\n",
    "            self.W = 2 * np.random.randn(self.input_size, self.output_size) - 1\n",
    "            self.b = 2 * np.random.randn(self.output_size, ) - 1\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid weight_initialization value: '{weight_initialization}'\")\n",
    "    \n",
    "    @property\n",
    "    def params_count(self):\n",
    "        return self.W.size + self.b.size\n",
    "    \n",
    "    # receives the activation values of the previous layer (i.e. this layer's input)\n",
    "    # returns the activation values of the current layer (i.e. next layer's input)\n",
    "    def feedforward(self, X):\n",
    "        ''' X.shape == (n_examples, self.input_size) '''\n",
    "        assert(X.shape[1] == self.input_size)\n",
    "        self.X = X\n",
    "        # (n_examples, output_size) = (n_examples, input_size) @ (input_size, output_size) + (output_size, )\n",
    "        self.Z = self.X @ self.W + self.b\n",
    "        self.A = self.g(self.Z)\n",
    "        return self.A\n",
    "    \n",
    "    # receives the derivative of the cost function w.r.t. the activation values of the current layer (i.e. next layer's input)\n",
    "    # returns the derivative of the cost function w.r.t. the activation values of the previous layer (i.e. this layer's input)\n",
    "    def backprop(self, dA):\n",
    "        ''' dA.shape == (n_examples, self.output_size)\n",
    "        \n",
    "            Note that only calling backprop doesn't actually update the layer parameters\n",
    "        '''\n",
    "        assert(dA.shape[1] == self.output_size)        \n",
    "        # (n_examples, output_size) = (n_examples, output_size) * (n_examples, output_size)\n",
    "        # (input_size, output_size) = (input_size, n_examples)  @ (n_examples, output_size)\n",
    "        # (output_size, )           = (n_examples, output_size).sum(axis=0)\n",
    "        # (n_examples, input_size)  = (n_examples, output_size) @ (output_size, input_size), input_size==prev_layer.output_size\n",
    "        delta = dA * self.g.derivative(self.Z) # [dJ/dZ = dJ/dA . dA/dZ]\n",
    "        self.dW = (self.X).T @ delta           # [dJ/dW = dJ/dZ . dZ/dX]\n",
    "        self.db = delta.sum(axis=0)            # [dJ/db = dJ/dZ . dZ/db]\n",
    "        return delta @ (self.W).T              # [dJ/dX = dJ/dZ . dZ/dX], note that dJ/dX is dA for the previous layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YBksc85X5ukO"
   },
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, layers, cost_function, optimizer, weight_initialization='xavier'):\n",
    "        assert(isinstance(cost_function, CostFunction)), \"Invalid object type for cost_function\"\n",
    "        \n",
    "        self.J = cost_function # cost_function(Y, Ypred)\n",
    "        # obs.: cost_function.derivative is the derivative of J w.r.t. the last layer's activation values [dJ/dYpred]\n",
    "        #       Ypred == self.layers[-1].A, thus self.J.derivative is the input (dA) for the last layer's backprop\n",
    "        \n",
    "        self.optimizer = optimizer # obs.: the learning rate is set on the optimizer object\n",
    "        \n",
    "        self.layers = []\n",
    "        self.layers.append(layers[0]) # input layer (we don't init it since we set it's activation values manually)\n",
    "        for l in range(1, len(layers)):            \n",
    "            # sets the layer's input_size as the last layer's output_size and initializes its weights and biases\n",
    "            layers[l].init(input_size=layers[l-1].output_size, weight_initialization=weight_initialization)            \n",
    "            self.layers.append(layers[l]) # adds the initialized layer to the network\n",
    "        \n",
    "        self.history = { \"loss\": [], \"loss_val\": [], \"acc\": [], \"acc_val\": [] }\n",
    "    \n",
    "    # note that we use zero-based indexing here, so\n",
    "    # the 1st layer is self.layers[0] and the last is self.layers[len(self.layers) - 1]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        ''' X.shape == (n_examples, self.layers[0].input_size) '''\n",
    "        assert(X.shape[1] == self.layers[0].output_size) # self.layers[0].input_size == self.layers[0].output_size\n",
    "        activation = X # network's input\n",
    "        for l in range(1, len(self.layers)):\n",
    "            Z = activation @ self.layers[l].W + self.layers[l].b\n",
    "            activation = self.layers[l].g(Z)\n",
    "        return activation # network's output (Ypred)\n",
    "    \n",
    "    def feedforward(self, X):\n",
    "        ''' X.shape     == (n_examples, self.layers[0].input_size)\n",
    "            Ypred.shape == (n_examples, self.layers[-1].output_size)\n",
    "        '''\n",
    "        assert(X.shape[1] == self.layers[0].output_size) # self.layers[0].input_size == self.layers[0].output_size\n",
    "        self.layers[0].A = X # input\n",
    "        for l in range(1, len(self.layers)):\n",
    "            self.layers[l].feedforward(self.layers[l-1].A)\n",
    "        Ypred = self.layers[-1].A # output\n",
    "        return Ypred\n",
    "    \n",
    "    def backprop(self, X, Y, Ypred):\n",
    "        ''' X.shape     == (n_examples, self.layers[0].input_size)\n",
    "            Y.shape     == (n_examples, self.layers[-1].output_size)\n",
    "            Ypred.shape == (n_examples, self.layers[-1].output_size)\n",
    "            where Ypred is the result of feedforward(X)\n",
    "            \n",
    "            Note that only calling backprop doesn't actually update the network parameters\n",
    "        '''\n",
    "        assert(X.shape[0] == Y.shape[0])\n",
    "        assert(X.shape[1] == self.layers[0].output_size) # self.layers[0].input_size == self.layers[0].output_size\n",
    "        assert(Y.shape[1] == self.layers[-1].output_size)\n",
    "        \n",
    "        cost_wrt_Ypred = self.J.derivative(Y, Ypred) # [dJ/dYpred]\n",
    "        dA = self.layers[-1].backprop(cost_wrt_Ypred)\n",
    "        for l in reversed(range(1, len(self.layers) - 1)): # we don't do backprop on the input layer\n",
    "            dA = self.layers[l].backprop(dA)\n",
    "    \n",
    "    def __shuffle_X_Y(self, X, Y):\n",
    "        m = X.shape[0] # == Y.shape[0]\n",
    "        p = np.random.permutation(m)\n",
    "        return X[p], Y[p]\n",
    "    \n",
    "    def __get_batches(self, X, Y, batch_size, shuffled):\n",
    "        m = X.shape[0] # == Y.shape[0]\n",
    "        n_batches = m // batch_size\n",
    "        if shuffled:\n",
    "            X, Y = self.__shuffle_X_Y(X, Y)\n",
    "        for i in range(n_batches):\n",
    "            yield (X[batch_size*i : batch_size*(i+1)], \n",
    "                   Y[batch_size*i : batch_size*(i+1)])\n",
    "        return\n",
    "    \n",
    "    # trainning and validation data\n",
    "    def train(self, X, Y, X_val, Y_val, n_epochs, batch_size, verbose=True):\n",
    "        ''' X.shape == (n_training_samples, self.layers[0].input_size)\n",
    "            Y.shape == (n_training_samples, self.layers[-1].output_size)\n",
    "            \n",
    "            X_val.shape == (n_validation_samples, self.layers[0].input_size)\n",
    "            Y_val.shape == (n_validation_samples, self.layers[-1].output_size)\n",
    "            \n",
    "            For each iteration we'll have:\n",
    "              n_examples = batch_size\n",
    "              batch_X.shape == (n_examples, self.layers[0].input_size)\n",
    "              batch_Y.shape == (n_examples, self.layers[-1].output_size)\n",
    "            Thus, each epoch has ceil(n_training_samples / batch_size) iterations\n",
    "            obs.: batch_X and batch_Y are rows of X and Y, and after each iteration (i.e. after going through\n",
    "                  each batch) we update our network parameters (weights and biases)\n",
    "            \n",
    "            If n_training_samples is not divisible by batch_size the last training batch will be smaller\n",
    "        '''\n",
    "        assert(X.shape[0] == Y.shape[0])\n",
    "        assert(X.shape[1] == self.layers[0].output_size) # self.layers[0].input_size == self.layers[0].output_size\n",
    "        assert(Y.shape[1] == self.layers[-1].output_size)\n",
    "        assert(X_val.shape[0] == Y_val.shape[0])\n",
    "        assert(X_val.shape[1] == self.layers[0].output_size) # self.layers[0].input_size == self.layers[0].output_size\n",
    "        assert(Y_val.shape[1] == self.layers[-1].output_size)\n",
    "        \n",
    "        n_training_samples = X.shape[0]\n",
    "        batches_per_epoch = int(np.ceil(n_training_samples / batch_size)) # same as the number of iterations per epoch\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            batch_number = 0\n",
    "            for batch_X, batch_Y in self.__get_batches(X, Y, batch_size, shuffled=True):\n",
    "                # calculates the predicted target values for this batch (with the current network parameters)\n",
    "                batch_Ypred = self.feedforward(batch_X)\n",
    "                \n",
    "                # sets the values of dW and db, used to then update the network parameters\n",
    "                self.backprop(batch_X, batch_Y, batch_Ypred)\n",
    "                \n",
    "                # updates each layer's parameters (i.e. weights and biases) with gradient descent\n",
    "                self.optimizer.update(self.layers)\n",
    "                \n",
    "                    batch_number += 1\n",
    "                if verbose and ((batches_per_epoch/10) % batch_number == 0):\n",
    "                    print(f\"batch ({batch_number}/{batches_per_epoch})\", end='\\r')\n",
    "            \n",
    "            # calculate the loss/cost value for this epoch\n",
    "            epoch_cost, epoch_accuracy = evaluate(X, Y) # training set\n",
    "            epoch_cost_val, epoch_accuracy_val = evaluate(X_val, Y_val) # validation set\n",
    "            self.history[\"loss\"].append(epoch_cost)\n",
    "            self.history[\"loss_val\"].append(epoch_cost_val)\n",
    "            self.history[\"acc\"].append(epoch_accuracy)\n",
    "            self.history[\"acc_val\"].append(epoch_accuracy_val)\n",
    "            if verbose:\n",
    "                print(f\"epoch ({epoch+1}/{n_epochs}) \"\n",
    "                      f\"loss: {epoch_cost:.4f}, loss_val: {epoch_cost_val:.4f} | \"\n",
    "                      f\"acc: {epoch_accuracy:.2f}, acc_val: {epoch_accuracy_val:.2f}\")\n",
    "    \n",
    "    # test data\n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        ''' X_test.shape == (n_test_samples, self.layers[0].input_size)\n",
    "            Y_test.shape == (n_test_samples, self.layers[-1].output_size)\n",
    "        '''\n",
    "        assert(X_test.shape[0] == Y_test.shape[0])\n",
    "        assert(X_test.shape[1] == self.layers[0].output_size) # self.layers[0].input_size == self.layers[0].output_size\n",
    "        assert(Y_test.shape[1] == self.layers[-1].output_size)\n",
    "        \n",
    "        # loss/cost value for the training set\n",
    "        Ypred = self.feedforward(X_test)\n",
    "        cost = self.J(Y_test, Ypred)\n",
    "        \n",
    "        # calculates the values not as one-hot encoded row vectors\n",
    "        target = np.argmax(Y_test, axis=1)\n",
    "        prediction = np.argmax(Ypred, axis=1)\n",
    "        accuracy = (prediction == target).mean()        \n",
    "        \n",
    "        return cost, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8cAYGgEN5ukT"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data sets\n",
    "\n",
    "**NOTE:** you should have the .npz files in the following directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes_and_names.csv\n",
      "images\n",
      "README.txt\n",
      "test.npz\n",
      "train.npz\n",
      "val.npz\n"
     ]
    }
   ],
   "source": [
    "# confirm that you have test.npz, train.npz and val.npz\n",
    "!ls ../mini_cinic10/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = os.path.join(\"..\", \"mini_cinic10\")\n",
    "\n",
    "train_data = np.load(os.path.join(PATH_TO_DATA, \"train.npz\"))\n",
    "val_data   = np.load(os.path.join(PATH_TO_DATA, \"val.npz\"))\n",
    "# test_data  = np.load(os.path.join(PATH_TO_DATA, \"test.npz\")) # assume this doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 3072) uint8\n",
      "(80000,) uint8\n"
     ]
    }
   ],
   "source": [
    "xs, ys = train_data['xs'], train_data['ys']\n",
    "print(xs.shape, xs.dtype) # 3072 = 3 * 1024 = 3 * (32 * 32)\n",
    "print(ys.shape, ys.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3072) uint8\n",
      "(10000,) uint8\n"
     ]
    }
   ],
   "source": [
    "xs_val, ys_val = val_data['xs'], val_data['ys']\n",
    "print(xs_val.shape, xs_val.dtype) # 3072 = 3 * 1024 = 3 * (32 * 32)\n",
    "print(ys_val.shape, ys_val.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = IMG_HEIGHT = 32\n",
    "IMG_SHAPE = (IMG_HEIGHT, IMG_WIDTH, 3) # RGB images\n",
    "IMG_FLAT_SHAPE = (IMG_HEIGHT*IMG_WIDTH*3, )\n",
    "\n",
    "# classes_and_names.csv\n",
    "CLASS_NAME = {\n",
    "    0: \"airplane\",\n",
    "    1: \"automobile\",\n",
    "    2: \"bird\",\n",
    "    3: \"cat\",\n",
    "    4: \"deer\",\n",
    "    5: \"dog\",\n",
    "    6: \"frog\",\n",
    "    7: \"horse\",\n",
    "    8: \"ship\",\n",
    "    9: \"truck\",\n",
    "}\n",
    "CLASS_COUNT = len(CLASS_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unflatten(flat_img, img_shape=IMG_SHAPE):\n",
    "    ''' Converts a flattened image back into a 3-layer RGB matrix representation '''\n",
    "    return flat_img.reshape(img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img_index, xs, ys, figsize=None):\n",
    "    ''' Shows the image represented by the img_index-th row of xs '''\n",
    "    if figsize != None:\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "    plt.imshow(unflatten(xs[img_index]))\n",
    "    plt.title(f'idx #{img_index} ({CLASS_NAME[ys[img_index]]})')\n",
    "    plt.axis(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(0, xs, ys, [1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize xs and one-hot encode ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xs: mean=117.3316, stddev=63.1088\n",
      "X:  mean=  0.0000, stddev= 1.0000\n"
     ]
    }
   ],
   "source": [
    "print(f\"xs: mean={xs.mean():>8.4f}, stddev={xs.std():>7.4f}\")\n",
    "\n",
    "# NOTE that we must use stats from train data to normalize the val and test sets aswell\n",
    "mean, std = xs.mean(), xs.std()\n",
    "X = (xs - mean) / std\n",
    "\n",
    "print(f\"X:  mean={X.mean():>8.4f}, stddev={X.std():>7.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xs_val: mean=118.1221, stddev=63.2348\n",
      "X_val:  mean=  0.0125, stddev= 1.0020\n"
     ]
    }
   ],
   "source": [
    "X_val = (xs_val - mean) / std\n",
    "print(f\"xs_val: mean={xs_val.mean():>8.4f}, stddev={xs_val.std():>7.4f}\")\n",
    "print(f\"X_val:  mean={X_val.mean():>8.4f}, stddev={X_val.std():>7.4f}\") # mean should be close to 0 and stddev close to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode(ys):\n",
    "    n_examples, *_ = ys.shape\n",
    "    onehot = np.zeros(shape=(n_examples, CLASS_COUNT))\n",
    "    onehot[np.arange(n_examples), ys] = 1\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peek_compare(ys, Y, n_rows=5):\n",
    "    ''' Y == onehot_encode(ys) '''\n",
    "    return np.append(ys[:n_rows].reshape((-1, 1)), Y[:n_rows], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000,) (80000, 10)\n"
     ]
    }
   ],
   "source": [
    "Y = onehot_encode(ys)\n",
    "print(ys.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "Y_val = onehot_encode(ys_val)\n",
    "print(ys_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\documents\\github\\unicamp\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: \n",
      "The activation_function isn't set for this layer (this is only ok if it is the input layer)\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "nn_log_reg = NN(\n",
    "    cost_function=CrossEntropy(),\n",
    "    optimizer=GradientDescent(learning_rate=1e-4),\n",
    "    weight_initialization='xavier',\n",
    "    layers=[\n",
    "        Layer(3072, None), Layer(10, SoftMax())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float modulo",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-269346f2318d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1250\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m ) # X, Y, X_val, Y_val, n_epochs, batch_size, verbose=True\n",
      "\u001b[1;32m<ipython-input-28-927940841d69>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, Y, X_val, Y_val, n_epochs, batch_size, verbose)\u001b[0m\n\u001b[0;32m    113\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches_per_epoch\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_number\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m                     \u001b[0mbatch_number\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"batch ({batch_number}/{batches_per_epoch})\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float modulo"
     ]
    }
   ],
   "source": [
    "nn_log_reg.train(\n",
    "    X, Y,\n",
    "    X_val, Y_val,\n",
    "    n_epochs=2,\n",
    "    batch_size=1250\n",
    ") # X, Y, X_val, Y_val, n_epochs, batch_size, verbose=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "3072\n",
      "None\n",
      "3072\n",
      "10\n",
      "<__main__.SoftMax object at 0x0000017F4823BB08>\n"
     ]
    }
   ],
   "source": [
    "for layer in nn_log_reg.layers:\n",
    "    print(layer.input_size)\n",
    "    print(layer.output_size)\n",
    "    print(layer.g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of NN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
