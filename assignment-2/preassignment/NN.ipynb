{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [2, 2]\n",
    "sns.set() # apply the seaborn defaults to plotted figures (e.g. theme, scaling, color palette), instead of matplotlib's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and derivatives [DEPRECATED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(z):\n",
    "    return z\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def sigmoid(z, limit=500):\n",
    "    if limit != None:\n",
    "        z = np.clip(z, -limit, limit) # avoid overflow\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def softmax(y_pred, axis=-1):\n",
    "    exp = np.exp(y_pred)\n",
    "    return exp / np.sum(exp, axis=axis, keepdims=True)\n",
    "\n",
    "''' y.shape == y_pred.shape == (m, C), where:\n",
    "    - m is the number of examples\n",
    "    - C is the number of classes \n",
    "    Thus, each row of y and y_pred is a one-hot encoded vector of shape (1, C)\n",
    "'''    \n",
    "\n",
    "def cross_entropy(y, y_pred, axis=-1, eps=1e-12):\n",
    "    if eps != None:\n",
    "        y_pred = np.clip(y_pred, eps, 1 - eps) # avoid overflow\n",
    "    m = y_pred.shape[0]\n",
    "    return -np.sum(y * log(y_pred), axis=axis) / m\n",
    "\n",
    "def xent(y, y_pred):\n",
    "    return -np.sum(y * log(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_linear(z):\n",
    "    return np.ones(shape=z.shape)\n",
    "\n",
    "def grad_relu(z):\n",
    "    return np.where(z > 0, 1, 0)\n",
    "\n",
    "def grad_sigmoid(z):\n",
    "    sigmoid_z = sigmoid(z)\n",
    "    return sigmoid_z * (1 - sigmoid_z)\n",
    "\n",
    "def grad_softmax(y_pred):\n",
    "    # y_pred[i]*(1-y_pred[j]) if i != j --> y_pred[i] - y_pred[i] * y_pred[j]\n",
    "    # -y_pred[i]*y_pred[j]    if i == j -->     0     - y_pred[i] * y_pred[j]\n",
    "    y_pred = y_pred.reshape(-1, 1)\n",
    "    return np.diagflat(y_pred) - np.dot(y_pred, y_pred.T)\n",
    "\n",
    "def grad_cross_entropy(y, y_pred, axis=-1):\n",
    "    return y_pred - y # FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "Being $m$ the number of samples in a batch, from a layer $k-1$ to a layer $k$ we have:\n",
    "- Weights $\\mathbb{W}^{(k)} \\in \\mathbb{R}^{n_{k-1} \\times n_k}$\n",
    "- Biases $\\mathbf{b}^{(k)} \\in \\mathbb{R}^{n_k}$\n",
    "- Activations $\\mathbb{A}^{(k)} = g_k(\\mathbb{Z}^{(k)}) \\in \\mathbb{R}^{m \\times n_k}$, where $g_k(\\mathbb{Z}^{(k)})$ is the activation function of the $k^{\\text{th}}$ layer and $\\mathbb{Z}^{(k)} = \\mathbb{A}^{(k-1)} \\mathbb{W}^{(k)} + \\mathbf{b}^{(k)}$\n",
    "\n",
    "(Xavier initialization: [[1]](https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/))\n",
    "\n",
    "For the first layer, the activation is the input itself: $\\mathbb{A}^{(1)} = \\mathbb{X} \\in \\mathbb{R}^{m \\times n_1}$, where $n_1$ is the input size (3072)  \n",
    "For the middle layers ($2 \\leq k < L$), the activation function is the sigmoid: $\\mathbb{A}^{(k)} = g_k(\\mathbb{Z}^{(k)}) = sigmoid(\\mathbb{Z}^{(k)})$  \n",
    "For the last layer, we have the predicted value with softmax activation: $\\mathbb{A}^{(L)} = g_k(\\mathbb{Z}^{(L)}) = softmax(\\mathbb{Z}^{(L)}) \\in \\mathbb{R}^{m \\times n_L}$, where $n_L$ is the output size (10)  \n",
    "(i.e. the hypothesis function $a^{(L)} = h_{W, b}(x) = y_{\\text{pred}} \\approx y$)\n",
    "\n",
    "obs.: the number of layers $L$ comes from: $1$ input layer + $1$ output layer + $L-2$ hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# RANDOM_SEED = 886"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction:\n",
    "    ''' An ActivationFunction is applied to Z to get the output A, \n",
    "        but its derivative expects the value A, not Z (!):\n",
    "\n",
    "        A == __call__(Z) and derivative(A) == derivative(__call__(Z)), \n",
    "        calling derivative(Z) will often yield WRONG results\n",
    "    '''\n",
    "    def __call__(self, Z):\n",
    "        ''' Z.shape=(n_examples, layer_output_size) '''\n",
    "        raise NotImplementedError    \n",
    "    def derivative(self, A):\n",
    "        ''' A.shape=(n_examples, layer_output_size) '''\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Linear(ActivationFunction):\n",
    "    def __call__(self, Z):\n",
    "        return Z\n",
    "    def derivative(self, A):\n",
    "        return np.ones_like(A)\n",
    "\n",
    "class Sigmoid(ActivationFunction):\n",
    "    def __call__(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    def derivative(self, A):\n",
    "        return A * (1 - A) # Sigmoid(Z) * (1 - Sigmoid(Z))\n",
    "\n",
    "class ReLU(ActivationFunction):\n",
    "    def __call__(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "    def derivative(self, A):\n",
    "        return np.where(A > 0, 1, 0)\n",
    "\n",
    "class SoftMax(ActivationFunction):\n",
    "    def __call__(self, Z):\n",
    "        # e^{x+c} / sum(e^{x+c}) == (e^x * e^c) / (e^c * sum(e^x)) == e^x / sum(e^x)\n",
    "        exp = np.exp(Z - Z.max(axis=1, keepdims=True))\n",
    "        return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "    def derivative(self, A):\n",
    "        # FIXME\n",
    "        # A == SoftMax(Z), A.shape == (n_examples, output_shape)\n",
    "        #return A * (1 - A) # SoftMax(Z) * (1 - SoftMax(Z))\n",
    "        raise NotImplementedError\n",
    "        # ref.: https://medium.com/@aerinykim/how-to-implement-the-softmax-derivative-independently-from-any-loss-function-ae6d44363a9d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostFunction:\n",
    "    ''' A CostFunction is applied to Y (the target values) and Ypred to get a scalar output\n",
    "        Its derivative w.r.t. Ypred also expects Y and Ypred, but returns tensor (n_examples, last_layer_output_size)\n",
    "        \n",
    "        obs.: Ypred is the last layer's activation values: last_layer.A == last_layer.g(last.layer.Z)\n",
    "    '''\n",
    "    def __call__(self, Y, Ypred):\n",
    "        ''' Y.shape == Ypred.shape == (n_examples, last_layer_output_size) '''\n",
    "        raise NotImplementedError # [J(Y, Ypred)]\n",
    "    def derivative(self, Y, Ypred):\n",
    "        ''' Y.shape == Ypred.shape == (n_examples, last_layer_output_size) '''\n",
    "        raise NotImplementedError # [dJ/dYpred]\n",
    "\n",
    "class CrossEntropy(CostFunction):\n",
    "    def __call__(self, Y, Ypred):\n",
    "        return np.mean( -(Y * np.log(Ypred)).sum(axis=1) )\n",
    "    def derivative(self, Y, Ypred):\n",
    "        # FIXME\n",
    "        raise NotImplementedError\n",
    "        #return Y / Ypred\n",
    "        #return (Ypred - Y) / Ypred.shape[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    ''' A.shape == (n_examples, output_size)\n",
    "        Z.shape == (n_examples, output_size)\n",
    "        W.shape == (input_size, output_size)\n",
    "        b.shape == (output_size, )\n",
    "        obs.:\n",
    "            input_size == prev_layer.output_size\n",
    "            output_size == next_layer.input_size\n",
    "    '''\n",
    "    def __init__(self, input_size, output_size, activation_function, \n",
    "                 weight_initialization='xavier'):\n",
    "        assert(isinstance(activation_function, ActivationFunction)), \"Invalid type for activation_function\"\n",
    "        \n",
    "        self.input_size  = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # activation function\n",
    "        self.g = activation_function # g_prime == activation_function.derivative\n",
    "        \n",
    "        # activation values\n",
    "        self.A = None # self.A == self.g(self.Z)\n",
    "        self.Z = None # prev_layer.A @ self.W + self.b\n",
    "        \n",
    "        if weight_initialization == 'xavier':\n",
    "            stddev = np.sqrt(1 / input_size)\n",
    "            self.W = stddev * np.random.randn(input_size, output_size)\n",
    "            self.b = np.random.randn(output_size, )\n",
    "        elif weight_initialization == 'xavier avg':\n",
    "            stddev = np.sqrt(2 / (input_size + output_size))\n",
    "            self.W = stddev * np.random.randn(input_size, output_size)\n",
    "            self.b = np.random.randn(output_size, )\n",
    "        elif weight_initialization == '-1 to 1':\n",
    "            self.W = 2 * np.random.randn(input_size, output_size) - 1\n",
    "            self.b = 2 * np.random.randn(output_size, ) - 1\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid weight_initialization value: '{weight_initialization}'\")\n",
    "    \n",
    "    @property\n",
    "    def params_count(self):\n",
    "        return self.W.size + self.b.size\n",
    "    \n",
    "    # receives the activation values of the previous layer (i.e. this layer's input)\n",
    "    # returns the activation values of the current layer (i.e. next layer's input)\n",
    "    def feedforward(self, X):\n",
    "        ''' X.shape == (n_examples, self.input_size) '''\n",
    "        assert(X.shape[1] == self.input_size)\n",
    "        self.X = X\n",
    "        # (n_examples, output_size) = (n_examples, input_size) @ (input_size, output_size) + (output_size, )\n",
    "        self.Z = self.X @ self.W + self.b\n",
    "        self.A = self.g(self.Z)\n",
    "        return self.A\n",
    "    \n",
    "    # receives the derivative of the cost function w.r.t. the activation values of the current layer (i.e. next layer's input)\n",
    "    # returns the derivative of the cost function w.r.t. the activation values of the previous layer (i.e. this layer's input)\n",
    "    def backprop(self, dA, learning_rate):\n",
    "        ''' dA.shape == (n_examples, self.output_size) '''\n",
    "        assert(dA.shape[1] == self.output_size)        \n",
    "        # (n_examples, output_size) = (n_examples, output_size) * (n_examples, output_size)\n",
    "        # (input_size, output_size) = (input_size, n_examples)  @ (n_examples, output_size)\n",
    "        # (output_size, )           = (n_examples, output_size).sum(axis=0)\n",
    "        delta = dA * self.g.derivative(self.A) # [dJ/dZ = dJ/dA . dA/dZ]\n",
    "        self.dW = (self.X).T @ delta           # [dJ/dW = dJ/dZ . dZ/dX]\n",
    "        self.db = delta.sum(axis=0)            # [dJ/db = dJ/dZ . dZ/db]\n",
    "        \n",
    "        # TODO update self.W and self.b (?)\n",
    "        self.__update_params()\n",
    "        \n",
    "        # (n_examples, input_size)  = (n_examples, output_size) @ (output_size, input_size), input_size==prev_layer.output_size\n",
    "        return delta @ (self.W).T              # [dJ/dX = dJ/dZ . dZ/dX]\n",
    "    \n",
    "    def __update_params(self):\n",
    "        # gradient descent\n",
    "        self.W += -learning_rate * self.dW\n",
    "        self.b += -learning_rate * self.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, layers, cost_function, cost_function_derivative):\n",
    "        \n",
    "        self.J = cost_function # cost_function(Y, Ypred)\n",
    "        self.dJ_dYpred = cost_function_derivative # derivative of J w.r.t. the last layer's activation values [dJ/dYpred]\n",
    "        # obs.: Ypred == self.layers[-1].A, thus self.dJ_dYpred is the input (dA) for the last layer's backprop\n",
    "        \n",
    "        self.layers = []\n",
    "        self.layers.append(layers[0]) # input layer\n",
    "        for l in range(1, len(layers)):\n",
    "            if layers[l-1].output_shape == layers[l].input_shape:\n",
    "                self.layers.append(layers[l])\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Invalid input shape at the {l}-th layer\"\n",
    "                    f\"\\n{l}-th layer's shape: {layer[l].shape}\"\n",
    "                    f\"\\n{l-1}-th layer's shape: {layer[l-1].shape}\"\n",
    "                )\n",
    "        \n",
    "        self.history = { \"loss\": [], \"val_loss\": [] }\n",
    "    \n",
    "    # note that we use zero-based indexing here, so\n",
    "    # the 1st layer is self.layers[0] and the last is self.layers[len(self.layers) - 1]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        ''' X.shape == (n_examples, self.layers[0].input_size) '''\n",
    "        assert(X.shape[1] == self.layers[0].input_size)\n",
    "        activation = X # network's input\n",
    "        for l in range(1, len(self.layers)):\n",
    "            Z = activation @ self.layers[l].W + self.layers[l].b\n",
    "            activation = self.layers[l].g(Z)\n",
    "        return activation # network's output (Ypred)\n",
    "    \n",
    "    def feedforward(self, X):\n",
    "        ''' X.shape     == (n_examples, self.layers[0].input_size)\n",
    "            Ypred.shape == (n_examples, self.layers[-1].output_size)\n",
    "        '''\n",
    "        assert(X.shape[1] == self.layers[0].input_size)\n",
    "        self.layers[0].A = X # input\n",
    "        for l in range(1, len(self.layers)):\n",
    "            self.layers[l].feedforward(self.layers[l-1].A)\n",
    "        Ypred = self.layers[-1].A # output\n",
    "        return Ypred\n",
    "    \n",
    "    def backprop(self, X, Y, learning_rate):\n",
    "        ''' X.shape == (n_examples, self.layers[0].input_size)\n",
    "            Y.shape == (n_examples, self.layers[-1].output_size)\n",
    "        '''\n",
    "        assert(X.shape[0] == Y.shape[0])\n",
    "        assert(X.shape[1] == self.layers[0].input_size)\n",
    "        assert(Y.shape[1] == self.layers[-1].output_size)\n",
    "        \n",
    "        Ypred = self.feedforward(X) # == self.layers[-1].A\n",
    "        cost = self.J(Y, Ypred)\n",
    "        self.history[\"loss\"].append(cost)\n",
    "        \n",
    "        # FIXME\n",
    "        cost_wrt_Ypred = self.dJ_dYpred(Y, Ypred)\n",
    "        dA = self.layers[-1].backprop(cost_wrt_Ypred, learning_rate)\n",
    "        for l in reversed(range(len(self.layers) - 1)):\n",
    "            dA = self.layers[l].backprop(dA, learning_rate)\n",
    "    \n",
    "    def __get_batches(self, X, Y, batch_size):\n",
    "        m = X.shape[0] # == Y.shape[0]\n",
    "        n_batches = m // batch_size\n",
    "        try:\n",
    "            return np.split(X, n_batches), np.split(Y, n_batches)\n",
    "            #return zip(np.split(X, n_batches), np.split(Y, n_batches))\n",
    "        except:\n",
    "            warnings.warn(f\"\\nbatch_size={batch_size} does not result in an equal division for shapes: \" +\n",
    "                          f\"{X.shape} of X, and {y.shape} of Y. The last batch will have size {m % batch_size}\")\n",
    "            return np.array_split(X, n_batches), np.array_split(Y, n_batches)\n",
    "            #return zip(np.array_split(X, n_batches), np.array_split(Y, n_batches))\n",
    "    \n",
    "    # trainning data\n",
    "    def fit(self, X_train, Y_train, learning_rate, n_epochs, batch_size):\n",
    "        ''' X_train.shape == (n_total_examples, self.layers[0].input_size)\n",
    "            Y_train.shape == (n_total_examples, self.layers[-1].output_size)\n",
    "            \n",
    "            For each iteration we'll have:\n",
    "              n_examples = batch_size\n",
    "              X.shape == (n_examples, self.layers[0].input_size)\n",
    "              Y.shape == (n_examples, self.layers[-1].output_size)\n",
    "            Thus, each epoch has n_total_examples // batch_size iterations (batches)\n",
    "            (obs.: X and Y are rows of X_train and Y_train)\n",
    "            (obs.: if n_total_examples is not divisible by batch_size the last examples are ignored)\n",
    "        '''\n",
    "        assert(X_train.shape[0] == Y_train.shape[0])\n",
    "        assert(X_train.shape[1] == self.layers[0].input_size)\n",
    "        assert(Y_train.shape[1] == self.layers[-1].output_size)\n",
    "        \n",
    "        # TODO use the last examples even if the last batch is smaller than batch_size\n",
    "        batches_per_epoch = n_total_examples // batch_size\n",
    "        \n",
    "        for epoch in n_epochs:\n",
    "            #TODO\n",
    "            pass\n",
    "    \n",
    "    # test data\n",
    "    def evaluate(self, X_test, Y_test, learning_rate, n_epochs, batch_size):\n",
    "        ''' X_val.shape == (n_examples, self.layers[0].input_size)\n",
    "            Y_val.shape == (n_examples, self.layers[-1].output_size)\n",
    "        '''\n",
    "        assert(X_val.shape[0] == Y_val.shape[0])\n",
    "        assert(X_val.shape[1] == self.layers[0].input_size)\n",
    "        assert(Y_val.shape[1] == self.layers[-1].output_size)\n",
    "        \n",
    "        #TODO\n",
    "        pass        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
