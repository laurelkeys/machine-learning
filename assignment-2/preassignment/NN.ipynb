{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YKhHc0XN5ujk"
   },
   "source": [
    "## Importing libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HLhIWQRC5ujn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_4cYzKSI5ujs"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [2, 2]\n",
    "sns.set() # apply the seaborn defaults to plotted figures (e.g. theme, scaling, color palette), instead of matplotlib's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SPsGc8lu5uj7"
   },
   "source": [
    "## Neural Network\n",
    "\n",
    "Being $m$ the number of samples in a batch, from a layer $k-1$ to a layer $k$ we have:\n",
    "- Weights $\\mathbb{W}^{(k)} \\in \\mathbb{R}^{n_{k-1} \\times n_k}$\n",
    "- Biases $\\mathbf{b}^{(k)} \\in \\mathbb{R}^{n_k}$\n",
    "- Activations $\\mathbb{A}^{(k)} = g_k(\\mathbb{Z}^{(k)}) \\in \\mathbb{R}^{m \\times n_k}$, where $g_k(\\mathbb{Z}^{(k)})$ is the activation function of the $k^{\\text{th}}$ layer and $\\mathbb{Z}^{(k)} = \\mathbb{A}^{(k-1)} \\mathbb{W}^{(k)} + \\mathbf{b}^{(k)}$\n",
    "\n",
    "(Xavier initialization: [[1]](https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/))\n",
    "\n",
    "For the first layer, the activation is the input itself: $\\mathbb{A}^{(1)} = \\mathbb{X} \\in \\mathbb{R}^{m \\times n_1}$, where $n_1$ is the input size (3072)  \n",
    "For the middle layers ($2 \\leq k < L$), the activation function is the sigmoid: $\\mathbb{A}^{(k)} = g_k(\\mathbb{Z}^{(k)}) = sigmoid(\\mathbb{Z}^{(k)})$  \n",
    "For the last layer, we have the predicted value with softmax activation: $\\mathbb{A}^{(L)} = g_k(\\mathbb{Z}^{(L)}) = softmax(\\mathbb{Z}^{(L)}) \\in \\mathbb{R}^{m \\times n_L}$, where $n_L$ is the output size (10)  \n",
    "(i.e. the hypothesis function $a^{(L)} = h_{W, b}(x) = y_{\\text{pred}} \\approx y$)\n",
    "\n",
    "obs.: the number of layers $L$ comes from: $1$ input layer + $1$ output layer + $L-2$ hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W4bmjMEg5uj8"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# RANDOM_SEED = 886"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_UMozuYp5ukB"
   },
   "outputs": [],
   "source": [
    "class ActivationFunction:\n",
    "    ''' An ActivationFunction is applied to Z to get the output A, \n",
    "        but its derivative expects the value A, not Z (!):\n",
    "\n",
    "        A == __call__(Z) and derivative(A) == derivative(__call__(Z)), \n",
    "        calling derivative(Z) will often yield WRONG results\n",
    "    '''\n",
    "    def __call__(self, Z):\n",
    "        ''' Z.shape=(n_examples, layer_output_size) '''\n",
    "        raise NotImplementedError    \n",
    "    def derivative(self, A):\n",
    "        ''' A.shape=(n_examples, layer_output_size) '''\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Linear(ActivationFunction):\n",
    "    def __call__(self, Z):\n",
    "        return Z\n",
    "    def derivative(self, A):\n",
    "        return np.ones_like(A)\n",
    "\n",
    "class Sigmoid(ActivationFunction):\n",
    "    def __call__(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    def derivative(self, A):\n",
    "        return A * (1 - A) # Sigmoid(Z) * (1 - Sigmoid(Z))\n",
    "\n",
    "class ReLU(ActivationFunction):\n",
    "    def __call__(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "    def derivative(self, A):\n",
    "        return np.where(A > 0, 1, 0)\n",
    "\n",
    "class SoftMax(ActivationFunction):\n",
    "    def __call__(self, Z):\n",
    "        exp = np.exp(Z - Z.max(axis=1, keepdims=True))\n",
    "        return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "    def derivative(self, A):\n",
    "        return A * (1 - A) # SoftMax(Z) * (1 - SoftMax(Z))\n",
    "    \n",
    "        #S = A.reshape(-1, m)\n",
    "        #return np.diagflat(S) - np.dot(S, S.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obs.: $\\mathbf{e}^{x+c}$ / sum($\\mathbf{e}^{x+c}$) == ($\\mathbf{e}^x$ * $\\mathbf{e}^c$) / ($\\mathbf{e}^c$ * sum($\\mathbf{e}^x$)) == $\\mathbf{e}^x$ / sum($\\mathbf{e}^x$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uj575tAo5ukF"
   },
   "outputs": [],
   "source": [
    "class CostFunction:\n",
    "    ''' A CostFunction is applied to Y (the target values) and Ypred to get a scalar output\n",
    "        Its derivative w.r.t. Ypred also expects Y and Ypred, but returns tensor (n_examples, last_layer_output_size)\n",
    "        \n",
    "        obs.: Ypred is the last layer's activation values: last_layer.A == last_layer.g(last.layer.Z)\n",
    "    '''\n",
    "    def __call__(self, Y, Ypred):\n",
    "        ''' Y.shape == Ypred.shape == (n_examples, last_layer_output_size) '''\n",
    "        raise NotImplementedError # [J(Y, Ypred)]\n",
    "    def derivative(self, Y, Ypred):\n",
    "        ''' Y.shape == Ypred.shape == (n_examples, last_layer_output_size) '''\n",
    "        raise NotImplementedError # [dJ/dYpred]\n",
    "\n",
    "class CrossEntropy(CostFunction):\n",
    "    def __call__(self, Y, Ypred):\n",
    "        return np.mean( -(Y * np.log(Ypred)).sum(axis=1) ) ## == - (1/m) * (Y * np.log(Ypred)).sum(axis=(0, 1))\n",
    "    def derivative(self, Y, Ypred):\n",
    "        m = Ypred.shape[0]\n",
    "        return - (Y / Ypred) / m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obs.:\n",
    "\n",
    "---\n",
    "```python\n",
    "CrossEntropy.derivative(Y, Ypred) * SoftMax.derivative(Ypred) ==  \n",
    "      - (1/m) * (Y / Ypred)       *    Ypred * (1 - Ypred)    == - (1/m) * Y * (1 - Ypred)  \n",
    "                                                              == - (1/m) * (Y - Y*Ypred)  \n",
    "                                                              == (Y*Ypred - Y) / m```\n",
    "---  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME return (Ypred - Y) / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BQMK9K0r5ukJ"
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    ''' A.shape == (n_examples, output_size)\n",
    "        Z.shape == (n_examples, output_size)\n",
    "        W.shape == (input_size, output_size)\n",
    "        b.shape == (output_size, )\n",
    "        obs.:\n",
    "            input_size == prev_layer.output_size\n",
    "            output_size == next_layer.input_size\n",
    "    '''\n",
    "    def __init__(self, input_size, output_size, activation_function, weight_initialization='xavier'):\n",
    "        assert(isinstance(activation_function, ActivationFunction)), \"Invalid object type for activation_function\"\n",
    "        \n",
    "        self.input_size  = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # activation function\n",
    "        self.g = activation_function # g_prime == activation_function.derivative\n",
    "        \n",
    "        # activation values\n",
    "        self.A = None # self.A == self.g(self.Z)\n",
    "        self.Z = None # prev_layer.A @ self.W + self.b\n",
    "        \n",
    "        # parameters (weights self.W and biases self.b)\n",
    "        if weight_initialization == 'xavier':\n",
    "            stddev = np.sqrt(1 / input_size)\n",
    "            self.W = stddev * np.random.randn(input_size, output_size)\n",
    "            self.b = np.random.randn(output_size, )\n",
    "        elif weight_initialization == 'xavier avg':\n",
    "            stddev = np.sqrt(2 / (input_size + output_size))\n",
    "            self.W = stddev * np.random.randn(input_size, output_size)\n",
    "            self.b = np.random.randn(output_size, )\n",
    "        elif weight_initialization == '-1 to 1':\n",
    "            self.W = 2 * np.random.randn(input_size, output_size) - 1\n",
    "            self.b = 2 * np.random.randn(output_size, ) - 1\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid weight_initialization value: '{weight_initialization}'\")\n",
    "    \n",
    "    @property\n",
    "    def params_count(self):\n",
    "        return self.W.size + self.b.size\n",
    "    \n",
    "    # receives the activation values of the previous layer (i.e. this layer's input)\n",
    "    # returns the activation values of the current layer (i.e. next layer's input)\n",
    "    def feedforward(self, X):\n",
    "        ''' X.shape == (n_examples, self.input_size) '''\n",
    "        assert(X.shape[1] == self.input_size)\n",
    "        self.X = X\n",
    "        # (n_examples, output_size) = (n_examples, input_size) @ (input_size, output_size) + (output_size, )\n",
    "        self.Z = self.X @ self.W + self.b\n",
    "        self.A = self.g(self.Z)\n",
    "        return self.A\n",
    "    \n",
    "    # receives the derivative of the cost function w.r.t. the activation values of the current layer (i.e. next layer's input)\n",
    "    # returns the derivative of the cost function w.r.t. the activation values of the previous layer (i.e. this layer's input)\n",
    "    def backprop(self, dA):\n",
    "        ''' dA.shape == (n_examples, self.output_size)\n",
    "        \n",
    "            Note that only calling backprop doesn't actually update the layer parameters\n",
    "        '''\n",
    "        assert(dA.shape[1] == self.output_size)        \n",
    "        # (n_examples, output_size) = (n_examples, output_size) * (n_examples, output_size)\n",
    "        # (input_size, output_size) = (input_size, n_examples)  @ (n_examples, output_size)\n",
    "        # (output_size, )           = (n_examples, output_size).sum(axis=0)\n",
    "        # (n_examples, input_size)  = (n_examples, output_size) @ (output_size, input_size), input_size==prev_layer.output_size\n",
    "        delta = dA * self.g.derivative(self.Z) # [dJ/dZ = dJ/dA . dA/dZ]\n",
    "        self.dW = (self.X).T @ delta           # [dJ/dW = dJ/dZ . dZ/dX]\n",
    "        self.db = delta.sum(axis=0)            # [dJ/db = dJ/dZ . dZ/db]\n",
    "        return delta @ (self.W).T              # [dJ/dX = dJ/dZ . dZ/dX], note that dJ/dX is dA for the previous layer\n",
    "    \n",
    "    # TODO update self.W and self.b with the Optimizer (a.k.a. gradient descend)\n",
    "    def __update_params(self, learning_rate):\n",
    "        # gradient descent\n",
    "        self.W += -learning_rate * self.dW\n",
    "        self.b += -learning_rate * self.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YBksc85X5ukO"
   },
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, layers, cost_function, optimizer):\n",
    "        assert(isinstance(cost_function, CostFunction)), \"Invalid object type for cost_function\"\n",
    "        \n",
    "        self.J = cost_function # cost_function(Y, Ypred)\n",
    "        # obs.: cost_function.derivative is the derivative of J w.r.t. the last layer's activation values [dJ/dYpred]\n",
    "        #       Ypred == self.layers[-1].A, thus self.J.derivative is the input (dA) for the last layer's backprop\n",
    "        \n",
    "        self.optimizer = optimizer # obs.: the learning rate is set on the optimizer object\n",
    "        \n",
    "        self.layers = []\n",
    "        self.layers.append(layers[0]) # input layer\n",
    "        for l in range(1, len(layers)):\n",
    "            if layers[l-1].output_shape == layers[l].input_shape:\n",
    "                self.layers.append(layers[l])\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Invalid input shape at the {l}-th layer\"\n",
    "                    f\"\\n{l}-th layer's shape: {layer[l].shape}\"\n",
    "                    f\"\\n{l-1}-th layer's shape: {layer[l-1].shape}\"\n",
    "                )\n",
    "        \n",
    "        self.history = { \"loss\": [], \"val_loss\": [] }\n",
    "    \n",
    "    # note that we use zero-based indexing here, so\n",
    "    # the 1st layer is self.layers[0] and the last is self.layers[len(self.layers) - 1]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        ''' X.shape == (n_examples, self.layers[0].input_size) '''\n",
    "        assert(X.shape[1] == self.layers[0].input_size)\n",
    "        activation = X # network's input\n",
    "        for l in range(1, len(self.layers)):\n",
    "            Z = activation @ self.layers[l].W + self.layers[l].b\n",
    "            activation = self.layers[l].g(Z)\n",
    "        return activation # network's output (Ypred)\n",
    "    \n",
    "    def feedforward(self, X):\n",
    "        ''' X.shape     == (n_examples, self.layers[0].input_size)\n",
    "            Ypred.shape == (n_examples, self.layers[-1].output_size)\n",
    "        '''\n",
    "        assert(X.shape[1] == self.layers[0].input_size)\n",
    "        self.layers[0].A = X # input\n",
    "        for l in range(1, len(self.layers)):\n",
    "            self.layers[l].feedforward(self.layers[l-1].A)\n",
    "        Ypred = self.layers[-1].A # output\n",
    "        return Ypred\n",
    "    \n",
    "    def backprop(self, X, Y, Ypred):\n",
    "        ''' X.shape     == (n_examples, self.layers[0].input_size)\n",
    "            Y.shape     == (n_examples, self.layers[-1].output_size)\n",
    "            Ypred.shape == (n_examples, self.layers[-1].output_size)\n",
    "            where Ypred is the result of feedforward(X)\n",
    "            \n",
    "            Note that only calling backprop doesn't actually update the network parameters\n",
    "        '''\n",
    "        assert(X.shape[0] == Y.shape[0])\n",
    "        assert(X.shape[1] == self.layers[0].input_size)\n",
    "        assert(Y.shape[1] == self.layers[-1].output_size)\n",
    "        \n",
    "        cost_wrt_Ypred = self.J.derivative(Y, Ypred) # [dJ/dYpred]\n",
    "        dA = self.layers[-1].backprop(cost_wrt_Ypred)\n",
    "        for l in reversed(range(len(self.layers) - 1)):\n",
    "            dA = self.layers[l].backprop(dA)\n",
    "    \n",
    "    def __shuffle_X_Y(self, X, Y):\n",
    "        m = X.shape[0] # == Y.shape[0]\n",
    "        p = np.random.permutation(m)\n",
    "        return X[p], Y[p]\n",
    "    \n",
    "    def __get_batches(self, X, Y, batch_size, shuffled):\n",
    "        m = X.shape[0] # == Y.shape[0]\n",
    "        n_batches = m // batch_size\n",
    "        if shuffled:\n",
    "            X, Y = self.__shuffle_X_Y(X, Y)\n",
    "        return np.array_split(X, n_batches), np.array_split(Y, n_batches)\n",
    "    \n",
    "    # trainning and validation data\n",
    "    def train(self, X, Y, X_val, Y_val, n_epochs, batch_size):\n",
    "        ''' X.shape == (n_training_samples, self.layers[0].input_size)\n",
    "            Y.shape == (n_training_samples, self.layers[-1].output_size)\n",
    "            \n",
    "            X_val.shape == (n_validation_samples, self.layers[0].input_size)\n",
    "            Y_val.shape == (n_validation_samples, self.layers[-1].output_size)\n",
    "            \n",
    "            For each iteration we'll have:\n",
    "              n_examples = batch_size\n",
    "              batch_X.shape == (n_examples, self.layers[0].input_size)\n",
    "              batch_Y.shape == (n_examples, self.layers[-1].output_size)\n",
    "            Thus, each epoch has ceil(n_training_samples / batch_size) iterations\n",
    "            obs.: batch_X and batch_Y are rows of X and Y, and after each iteration (i.e. after going through\n",
    "                  each batch) we update our network parameters (weights and biases)\n",
    "            \n",
    "            If n_training_samples is not divisible by batch_size the last training batch will be smaller\n",
    "        '''\n",
    "        assert(X.shape[0] == Y.shape[0])\n",
    "        assert(X.shape[1] == self.layers[0].input_size)\n",
    "        assert(Y.shape[1] == self.layers[-1].output_size)\n",
    "        \n",
    "        batches_per_epoch = int(np.ceil(n_training_samples / batch_size)) # same as the number of iterations per epoch\n",
    "        \n",
    "        for epoch in n_epochs:\n",
    "            for batch_X, batch_Y in self.__get_batches(X, Y, batch_size, shuffled=True):\n",
    "                # calculates the predicted target values for this batch (with the current network parameters)\n",
    "                batch_Ypred = self.feedforward(batch_X)\n",
    "                \n",
    "                # sets the values of dW and db, used to then update the network parameters\n",
    "                self.backprop(batch_X, batch_Y, batch_Ypred)\n",
    "                \n",
    "                # updates each layer's parameters (i.e. weights and biases) with gradient descent\n",
    "                self.optimizer.update(self.layers)\n",
    "            \n",
    "            # calculate the loss/cost value for this epoch\n",
    "            epoch_cost = self.J(Y, Ypred=self.feedforward(X))\n",
    "            self.history[\"loss\"].append(epoch_cost)\n",
    "            epoch_cost_val = self.J(Y_val, Ypred=self.feedforward(X_val))\n",
    "            self.history[\"loss_val\"].append(epoch_cost_val)\n",
    "    \n",
    "    # test data\n",
    "    def evaluate(self, X_test, Y_test, learning_rate, n_epochs, batch_size):\n",
    "        ''' X_test.shape == (n_test_samples, self.layers[0].input_size)\n",
    "            Y_test.shape == (n_test_samples, self.layers[-1].output_size)\n",
    "        '''\n",
    "        assert(X_test.shape[0] == Y_test.shape[0])\n",
    "        assert(X_test.shape[1] == self.layers[0].input_size)\n",
    "        assert(Y_test.shape[1] == self.layers[-1].output_size)\n",
    "        \n",
    "        #TODO\n",
    "        pass        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8cAYGgEN5ukT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of NN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
