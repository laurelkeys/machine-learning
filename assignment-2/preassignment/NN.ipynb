{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [2, 2]\n",
    "sns.set() # apply the seaborn defaults to plotted figures (e.g. theme, scaling, color palette), instead of matplotlib's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(z):\n",
    "    return z\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def sigmoid(z, limit=500):\n",
    "    if limit != None:\n",
    "        z = np.clip(z, -limit, limit) # avoid overflow\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def softmax(y_pred, axis=-1):\n",
    "    exp = np.exp(y_pred)\n",
    "    return exp / np.sum(exp, axis=axis, keepdims=True)\n",
    "\n",
    "''' y.shape == y_pred.shape == (m, C), where:\n",
    "    - m is the number of examples\n",
    "    - C is the number of classes \n",
    "    Thus, each row of y and y_pred is a one-hot encoded vector of shape (1, C)\n",
    "'''    \n",
    "\n",
    "def cross_entropy(y, y_pred, axis=-1, eps=1e-12):\n",
    "    if eps != None:\n",
    "        y_pred = np.clip(y_pred, eps, 1 - eps) # avoid overflow\n",
    "    m = y_pred.shape[0]\n",
    "    return -np.sum(y * log(y_pred), axis=axis) / m\n",
    "\n",
    "def xent(y, y_pred):\n",
    "    return -np.sum(y * log(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_linear(z):\n",
    "    return np.ones(shape=z.shape)\n",
    "\n",
    "def grad_relu(z):\n",
    "    return np.where(z > 0, 1, 0)\n",
    "\n",
    "def grad_sigmoid(z):\n",
    "    sigmoid_z = sigmoid(z)\n",
    "    return sigmoid_z * (1 - sigmoid_z)\n",
    "\n",
    "def grad_softmax(y_pred):\n",
    "    # y_pred[i]*(1-y_pred[j]) if i != j --> y_pred[i] - y_pred[i] * y_pred[j]\n",
    "    # -y_pred[i]*y_pred[j]    if i == j -->     0     - y_pred[i] * y_pred[j]\n",
    "    y_pred = y_pred.reshape(-1, 1)\n",
    "    return np.diagflat(y_pred) - np.dot(y_pred, y_pred.T)\n",
    "\n",
    "def grad_cross_entropy(y, y_pred, axis=-1):\n",
    "    return y_pred - y # FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "From a layer $k-1$ to a layer $k$ we have:\n",
    "- Weights $W \\in \\mathbb{R}^{n_k \\times n_{k-1}}$\n",
    "- Biases $b \\in \\mathbb{R}^{n_k}$\n",
    "- Activations $a^{(k)} = g_k(z^{(k)}) \\in \\mathbb{R}^{n_k}$, where $g_k(z^{(k)})$ is the activation function of the $k^{\\text{th}}$ layer and $z^{(k)} = W^{(k)} a^{(k-1)} + b^{(k)}$\n",
    "\n",
    "(Xavier initialization: [[1]](https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/))\n",
    "\n",
    "For the first layer, the activation is the input itself: $a^{(1)} = x$  \n",
    "For the middle layers ($2 \\leq k < L$), the activation function is the sigmoid: $a^{(k)} = g_k(z^{(k)}) = sigmoid(z^{(k)})$  \n",
    "For the last layer, we have the predicted value with softmax activation: $a^{(L)} = g_k(z^{(L)}) = softmax(z^{(L)})$  \n",
    "(i.e. the hypothesis function $a^{(L)} = h_{W, b}(x) = y_{\\text{pred}} \\approx y$)\n",
    "\n",
    "obs.: the number of layers $L$ comes from: $1$ input layer + $1$ output layer + $L-2$ hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# RANDOM_SEED = 886"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' An ActivationFunction is applied to Z to get the output A, \n",
    "    but it's derivative expects the value A, not Z (!):\n",
    "    \n",
    "    A == __call__(Z) and derivative(A) == derivative(__call__(Z)), \n",
    "    calling derivative(Z) will often yield WRONG results\n",
    "'''\n",
    "class ActivationFunction:\n",
    "    def __call__(self, Z):\n",
    "        ''' `Z`.shape=(n_examples, output_size) '''\n",
    "        raise NotImplementedError    \n",
    "    def derivative(self, A):\n",
    "        ''' `A`.shape=(n_examples, output_size) '''\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Linear(ActivationFunction):\n",
    "    def __call__(self, Z):\n",
    "        return Z\n",
    "    def derivative(self, A):\n",
    "        return np.ones_like(A)\n",
    "\n",
    "class Sigmoid(ActivationFunction):\n",
    "    def __call__(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    def derivative(self, A):\n",
    "        return A * (1 - A) # Sigmoid(Z) * (1 - Sigmoid(Z))\n",
    "\n",
    "class ReLU(ActivationFunction):\n",
    "    def __call__(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "    def derivative(self, A):\n",
    "        return np.where(A > 0, 1, 0)\n",
    "\n",
    "class SoftMax(ActivationFunction):\n",
    "    def __call__(self, Z):\n",
    "        # e^{x+c} / sum(e^{x+c}) == (e^x * e^c) / (e^c * sum(e^x)) == e^x / sum(e^x)\n",
    "        exp = np.exp(Z - Z.max(axis=-1, keepdims=True))\n",
    "        return exp / np.sum(exp, axis=-1, keepdims=True)\n",
    "    def derivative(self, A):\n",
    "        raise NotImplementedError # FIXME ref.: https://medium.com/@aerinykim/how-to-implement-the-softmax-derivative-independently-from-any-loss-function-ae6d44363a9d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, activation_function, \n",
    "                 weight_initialization='xavier'):\n",
    "        \n",
    "        self.input_size  = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.g = activation_function\n",
    "        self.g_prime = activation_function_derivative\n",
    "        \n",
    "        # activation values shape=(output_size, 1)\n",
    "        self.a = None # self.g(self.z)\n",
    "        self.z = None # prev_layer.a @ self.W + self.b\n",
    "        \n",
    "        if weight_initialization == 'xavier':\n",
    "            stddev = np.sqrt(1 / input_size)\n",
    "            self.W = stddev * np.random.randn(input_size, output_size)\n",
    "            self.b = np.random.randn(output_size, 1)\n",
    "        elif weight_initialization == 'xavier avg':\n",
    "            stddev = np.sqrt(2 / (input_size + output_size))\n",
    "            self.W = stddev * np.random.randn(input_size, output_size)\n",
    "            self.b = np.random.randn(output_size, 1)\n",
    "        elif weight_initialization == '-1 to 1':\n",
    "            self.W = 2 * np.random.randn(input_size, output_size) - 1\n",
    "            self.b = 2 * np.random.randn(output_size, 1) - 1\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid weight_initialization value: '{weight_initialization}'\")\n",
    "    \n",
    "    @property\n",
    "    def params_count(self):\n",
    "        return self.W.size + self.b.size\n",
    "    \n",
    "    # receives the input of the current layer\n",
    "    # returns the activation value of the current layer\n",
    "    def feedforward(self, X):\n",
    "        ''' `X`.shape=(n_examples, `self.input_size`) '''\n",
    "        assert(X.shape[1] == self.input_size)\n",
    "        # (n_examples, output_size) = (n_examples, input_size) @ (input_size, output_size) + (output_size, )\n",
    "        self.z = X @ self.W + self.b\n",
    "        self.a = self.g(self.z)\n",
    "        return self.a\n",
    "    \n",
    "    # receives the derivative of J w.r.t. the next layer's of the current layer\n",
    "    # returns the derivative of the cost function w.r.t. activation value of the current layer\n",
    "    def backprop(self, delta):\n",
    "        ''' `delta`.shape '''\n",
    "        delta = delta * self.g_prime(self.z) # self.g_prime(self.z)\n",
    "        delta = delta * self.g_prime()\n",
    "        # a = g(z)\n",
    "        # g'(z) = da/dz\n",
    "        raise NotImplementedError\n",
    "\n",
    "class NN:\n",
    "    def __init__(self, layers, cost_function, cost_function_derivative):\n",
    "        self.layers = []\n",
    "    \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        activation = X\n",
    "        for l in range(1, L):\n",
    "            z = self.layers[l].W @ activation + self.layers[l].b\n",
    "            activation = self.layers[l].g(z)\n",
    "        return activation\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        L = len(self.layers)\n",
    "        # note that we use zero-based indexing here, so\n",
    "        # the 1st layer is self.layers[0] and the last is self.layers[L - 1]\n",
    "        self.layers[0].a = X # input\n",
    "        for l in range(1, L):\n",
    "            self.layers[l].z = self.layers[l].W @ self.layers[l-1].a + self.layers[l].b\n",
    "            self.layers[l].a = self.layers[l].g(self.layers[l].z) # apply the activation function g to the weighted sum z\n",
    "        y_pred = self.layers[L-1] # output\n",
    "        return y_pred\n",
    "    \n",
    "    def backprop(self, X, y, learning_rate):\n",
    "        L = len(self.layers) # number of layers\n",
    "        m = X.shape[0]       # number of examples\n",
    "        \n",
    "        y_pred = self.feed_forward(x) # == self.layers[L-1].a\n",
    "        cost = self.J(y, y_pred)\n",
    "        \n",
    "        self.layers[L-1].error = y_pred - y\n",
    "        self.layers[L-1].delta = self.layers[L-1].error * self.layers[L-1].g_prime(y_pred)\n",
    "        for l in reversed(range(L-1)):\n",
    "            self.layers[l].error = (self.layers[l+1].W).T @ self.layers[l+1].delta\n",
    "            self.layers[l].delta = self.layers[l].error * self.layers[l].g_prime(self.layers[l].a)\n",
    "        \n",
    "        # TODO gradient descent\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def train(self, x, y):\n",
    "        #TODO\n",
    "        pass\n",
    "            \n",
    "    # TODO fit(trainning data), evaluate(validation data)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
