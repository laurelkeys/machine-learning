{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [2, 2]\n",
    "sns.set() # apply the seaborn defaults to plotted figures (e.g. theme, scaling, color palette), instead of matplotlib's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions and derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(z):\n",
    "    return z\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def softmax(y_pred, axis=-1):\n",
    "    exp = np.exp(y_pred)\n",
    "    return exp / np.sum(exp, axis=axis, keepdims=True)\n",
    "\n",
    "def cross_entropy(y, y_pred):\n",
    "    return -np.sum(y * log(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_linear(z):\n",
    "    return np.ones(shape=z.shape)\n",
    "\n",
    "def grad_relu(z):\n",
    "    return np.where(z > 0, 1, 0)\n",
    "\n",
    "def grad_sigmoid(z):\n",
    "    return z * (1 - z)\n",
    "\n",
    "def grad_softmax(y_pred):\n",
    "    # y_pred[i]*(1-y_pred[j]) if i != j --> y_pred[i] - y_pred[i] * y_pred[j]\n",
    "    # -y_pred[i]*y_pred[j]    if i == j -->     0     - y_pred[i] * y_pred[j]\n",
    "    y_pred = y_pred.reshape(-1, 1)\n",
    "    return np.diagflat(y_pred) - np.dot(y_pred, y_pred.T)\n",
    "\n",
    "def grad_cross_entropy(y, y_pred):\n",
    "    return y_pred - y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "From a layer $k-1$ to a layer $k$ we have:\n",
    "- Weights $W \\in \\mathbb{R}^{n_k \\times n_{k-1}}$\n",
    "- Biases $b \\in \\mathbb{R}^{n_k}$\n",
    "- Activations $a^{(k)} = g_k(z^{(k)})$, where $g_k(z^{(k)})$ is the activation function and $z^{(k)} = (W^{(k)})^T a^{(k-1)} + b^{(k)}$\n",
    "\n",
    "(Xavier initialization: [[1]](https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/))\n",
    "\n",
    "For the first layer, the activation is the input itself: $a^{(1)} = x$  \n",
    "For the middle layers ($2 \\leq k < L$), the activation function is the sigmoid: $a^{(k)} = g_k(z^{(k)}) = sigmoid(z^{(k)})$  \n",
    "For the last layer, we have the predicted value with softmax activation: $a^{(L)} = g_k(z^{(L)}) = softmax(z^{(L)})$  \n",
    "(i.e. the hypothesis function $a^{(L)} = h_{W, b}(x) = y_{\\text{pred}} \\approx y$)\n",
    "\n",
    "obs.: the number of layers $L$ comes from: $1$ input layer + $1$ output layer + $L-2$ hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# RANDOM_SEED = 886"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, weight_initialization='xavier'):\n",
    "        \n",
    "        if weight_initialization == 'xavier':\n",
    "            stddev = np.sqrt(1 / input_size)\n",
    "            self.W = stddev * np.random.randn(output_size, input_size)\n",
    "            self.b = stddev * np.random.randn(output_size, 1)\n",
    "        elif weight_initialization == 'xavier_avg':\n",
    "            stddev = np.sqrt(2 / (input_size + output_size))\n",
    "            self.W = stddev * np.random.randn(output_size, input_size)\n",
    "            self.b = stddev * np.random.randn(output_size, 1)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid weight_initialization value: '{weight_initialization}'\")\n",
    "        \n",
    "        self.activation = np.array(shape=(output_size, 1)) # self.activation[i] is the “activation” of unit i this layer\n",
    "\n",
    "class NN:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        # TODO\n",
    "    \n",
    "    def forward_step(self, X, y):\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
