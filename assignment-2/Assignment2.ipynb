{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libs and loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [2, 2]\n",
    "sns.set() # apply the seaborn defaults to plotted figures (e.g. theme, scaling, color palette), instead of matplotlib's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Computational Graph definition and implementation\n",
    "\n",
    "We will define a Computational Graph [[ref]](https://colah.github.io/posts/2015-08-Backprop/) where each node either represents a value (i.e. what will later be a **layer** in a neural network) or a computation (i.e. an operation like the **activation** or **cost** functions in a neural network).\n",
    "\n",
    "With it we can then represent both an artificial neural network, aswell as a softmax regressor (which performs Multinomial Logistic Regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from time import time\n",
    "\n",
    "RANDOM_SEED = 886"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction:\n",
    "    ''' An ActivationFunction is applied to Z to get the output A, \n",
    "        but its derivative expects the value A, not Z (!):\n",
    "\n",
    "        A == __call__(Z) and derivative(A) == derivative(__call__(Z)), \n",
    "        calling derivative(Z) will often yield WRONG results\n",
    "    '''\n",
    "    def __call__(self, Z):\n",
    "        ''' Z.shape=(n_examples, layer_output_size) '''\n",
    "        raise NotImplementedError    \n",
    "    def derivative(self, A):\n",
    "        ''' A.shape=(n_examples, layer_output_size) '''\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Sigmoid(ActivationFunction):\n",
    "    def __call__(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    def derivative(self, A):\n",
    "        return A * (1 - A) # Sigmoid(Z) * (1 - Sigmoid(Z))\n",
    "\n",
    "class ReLU(ActivationFunction):\n",
    "    def __call__(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "    def derivative(self, A):\n",
    "        return np.where(A > 0, 1, 0)\n",
    "\n",
    "class SoftMax(ActivationFunction):\n",
    "    def __call__(self, Z):\n",
    "        exp = np.exp(Z - Z.max(axis=1, keepdims=True))\n",
    "        return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "    def derivative(self, A, eps=1e-9):\n",
    "        Ypred = A+eps\n",
    "        return Ypred * (1 - Ypred) # SoftMax(Z) * (1 - SoftMax(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostFunction:\n",
    "    ''' A CostFunction is applied to Y (the target values) and Ypred (the predicted values) to get a scalar output\n",
    "        Its derivative w.r.t. Ypred also expects Y and Ypred, but returns a tensor of shape (n_examples, last_layer_output_size)\n",
    "        \n",
    "        obs.: Ypred is the last layer's activation values: last_layer.A == last_layer.g(last.layer.Z), \n",
    "              i.e. last_layer is the output layer of the network\n",
    "    '''\n",
    "    def __call__(self, Y, Ypred):\n",
    "        ''' Y.shape == Ypred.shape == (n_examples, last_layer_output_size) '''\n",
    "        raise NotImplementedError # [J(Y, Ypred) == J(Y, A^L)]\n",
    "    def derivative(self, Y, Ypred):\n",
    "        ''' Y.shape == Ypred.shape == (n_examples, last_layer_output_size) '''\n",
    "        raise NotImplementedError # [dJ/dYpred == dJ/dA^L]\n",
    "    def deltaL(self, Y, Ypred, activation_function, Z):\n",
    "        ''' Y.shape == Ypred.shape == (n_examples, last_layer_output_size) \n",
    "\n",
    "            activation_function should be the last layer's g function, thus Ypred == activation_function(Z)\n",
    "            and the value returned is the delta for the output layer of the network (delta^L == dJ/dZ^L)\n",
    "        '''\n",
    "        return self.derivative(Y, Ypred) * activation_function.derivative(Z) # [dJ/dZ^L == dJ/dYpred . dYpred/dZ^L]\n",
    "        # obs.: Ypred == A^L == g(Z^L), thus dYpred/dZ^L == dA^L/dZ^L == g'(Z^L)\n",
    "        #       [dJ/dZ^L == dJ/dYpred . dYpred/dZ^L == dJ/dA^L . dA^L/dZ^L]\n",
    "\n",
    "class CrossEntropy(CostFunction):\n",
    "    def __call__(self, Y, Ypred, eps=1e-9):\n",
    "        return np.mean( -(Y * np.log(Ypred+eps)).sum(axis=1) )\n",
    "    def derivative(self, Y, Ypred, eps=1e-9):\n",
    "        m = Ypred.shape[0]\n",
    "        return - (Y / (Ypred+eps)) / m\n",
    "    def deltaL(self, Y, Ypred, activation_function, Z):\n",
    "        if isinstance(activation_function, SoftMax):\n",
    "            m = Ypred.shape[0]\n",
    "            # numerically stable\n",
    "            return (Ypred - Y) / m # (SoftMax(Z) - Y) / m\n",
    "        else:\n",
    "            return super().deltaL(Y, Ypred, activation_function, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME test if eps are really necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    ''' The optimizer's optimization policy should be implemented on its update(layers) function '''\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "    def update(self, layers):\n",
    "        ''' Updates the parameters (i.e. weights and biases) for each layer in the layers list '''\n",
    "        raise NotImplementedError\n",
    "\n",
    "class GradientDescent(Optimizer):\n",
    "    def __init__(self, learning_rate):\n",
    "        super().__init__(learning_rate)\n",
    "    def update(self, layers):\n",
    "        for layer in layers[1:]:\n",
    "            layer.W -= self.learning_rate * layer.dW\n",
    "            layer.b -= self.learning_rate * layer.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    ''' A.shape == (n_examples, output_size)\n",
    "        Z.shape == (n_examples, output_size)\n",
    "        W.shape == (input_size, output_size)\n",
    "        b.shape == (output_size, )\n",
    "        X.shape == (n_examples, input_size)\n",
    "        obs.:\n",
    "            input_size == prev_layer.output_size\n",
    "            output_size == next_layer.input_size\n",
    "    '''\n",
    "    def __init__(self, output_size, activation_function):\n",
    "        if activation_function != None:\n",
    "            # obs.: the activation_function should only be None for the network's input layer\n",
    "            assert(isinstance(activation_function, ActivationFunction)), \"Invalid object type for activation_function\"\n",
    "        \n",
    "        self.input_size = None\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # activation function\n",
    "        self.g = activation_function # g_prime == activation_function.derivative\n",
    "        \n",
    "        # activation values\n",
    "        self.A = None # self.A == self.g(self.Z)\n",
    "        self.Z = None # prev_layer.A @ self.W + self.b\n",
    "        \n",
    "        # output value of the previous layer\n",
    "        self.X = None # == prev_layer.A\n",
    "        self.dX = None\n",
    "        \n",
    "        # parameters (weights and biases)\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def init(self, input_size, weight_initialization):\n",
    "        ''' Sets the layer's input_size and initializes its weights and biases '''\n",
    "        self.input_size = input_size\n",
    "        if weight_initialization == 'xavier':\n",
    "            stddev = np.sqrt(1 / self.input_size)\n",
    "            self.W = stddev * np.random.randn(self.input_size, self.output_size)\n",
    "            self.b = np.random.randn(self.output_size, )\n",
    "        elif weight_initialization == 'xavier_avg':\n",
    "            stddev = np.sqrt(2 / (self.input_size + self.output_size))\n",
    "            self.W = stddev * np.random.randn(self.input_size, self.output_size)\n",
    "            self.b = np.random.randn(self.output_size, )\n",
    "        elif weight_initialization == 'rand_-1_to_1':\n",
    "            self.W = 2 * np.random.randn(self.input_size, self.output_size) - 1\n",
    "            self.b = 2 * np.random.randn(self.output_size, ) - 1\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid weight_initialization value: '{weight_initialization}'\")\n",
    "    \n",
    "    @property\n",
    "    def params_count(self):\n",
    "        return self.W.size + self.b.size\n",
    "    \n",
    "    # receives the activation values of the previous layer (i.e. this layer's input)\n",
    "    # returns the activation values of the current layer (i.e. next layer's input)\n",
    "    def feedforward(self, X):\n",
    "        ''' X.shape == (n_examples, self.input_size) '''\n",
    "        assert(X.shape[1] == self.input_size)\n",
    "        self.X = X\n",
    "        # (n_examples, output_size) = (n_examples, input_size) @ (input_size, output_size) + (output_size, )\n",
    "        self.Z = self.X @ self.W + self.b\n",
    "        self.A = self.g(self.Z)\n",
    "        return self.A\n",
    "    \n",
    "    # receives the derivative of the cost function w.r.t. the Z value of the current layer [dJ/dZ = dJ/dA . dA/dZ]\n",
    "    # returns the derivative of the cost function w.r.t. the A value of the previous layer [dJ/dX = dJ/dZ . dZ/dX]\n",
    "    # obs.: the A value of the previous layer is this layer's input value X\n",
    "    def backprop(self, dZ):\n",
    "        ''' dZ.shape == (n_examples, self.output_size)\n",
    "        \n",
    "            Note that only calling backprop doesn't actually update the layer parameters\n",
    "        '''\n",
    "        assert(dZ.shape[1] == self.output_size)\n",
    "        # (input_size, output_size) = (input_size, n_examples)  @ (n_examples, output_size)\n",
    "        # (output_size, )           = (n_examples, output_size).sum(axis=0)\n",
    "        # (n_examples, input_size)  = (n_examples, output_size) @ (output_size, input_size), input_size==prev_layer.output_size\n",
    "        self.dW = (self.X).T @ dZ              # [dJ/dW = dJ/dZ . dZ/dX]\n",
    "        self.db = dZ.sum(axis=0)               # [dJ/db = dJ/dZ . dZ/db]\n",
    "        self.dX = dZ @ (self.W).T              # [dJ/dX = dJ/dZ . dZ/dX]\n",
    "        return self.dX\n",
    "        # note that dJ/dX is dJ/dA for the previous layer (since this layer's input X is the previous layer's A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputationalGraph:\n",
    "    def __init__(self, layers, cost_function, optimizer, weight_initialization='xavier'):\n",
    "        assert(isinstance(cost_function, CostFunction)), \"Invalid object type for cost_function\"\n",
    "        \n",
    "        self.J = cost_function # cost_function(Y, Ypred)\n",
    "        # obs.: cost_function.derivative is the derivative of J w.r.t. the last layer's activation values [dJ/dYpred]\n",
    "        #       Ypred == self.layers[-1].A, thus [dJ/dYpred == dJ/dA^L]\n",
    "        #\n",
    "        #       cost_function.deltaL is the derivative of J w.r.t. the last layer's Z values [dJ/dZ^L]\n",
    "        #       Z^L == self.layers[-1].Z, thus [dJ/dZ^L == dJ/dA^L . dA^L/dZ^L == dJ/dYpred . dYpred/dZ^L]\n",
    "        \n",
    "        self.optimizer = optimizer # obs.: the learning rate is set on the optimizer object\n",
    "        \n",
    "        self.layers = []\n",
    "        # obs.: we don't call init for the input layer since we set it's activation values manually\n",
    "        layers[0].input_size = layers[0].output_size\n",
    "        self.layers.append(layers[0]) # input layer\n",
    "        for l in range(1, len(layers)):            \n",
    "            # sets the layer's input_size as the last layer's output_size and initializes its weights and biases\n",
    "            layers[l].init(input_size=layers[l-1].output_size, weight_initialization=weight_initialization)            \n",
    "            self.layers.append(layers[l]) # adds the initialized layer to the network\n",
    "        \n",
    "        self.history = { \"loss\": [], \"loss_val\": [], \"acc\": [], \"acc_val\": [] }\n",
    "    \n",
    "    # note that we use zero-based indexing here, so\n",
    "    # the 1st layer is self.layers[0] and the last is self.layers[len(self.layers) - 1]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        ''' X.shape == (n_examples, self.layers[0].input_size) '''\n",
    "        assert(X.shape[1] == self.layers[0].output_size) # self.layers[0].input_size == self.layers[0].output_size\n",
    "        activation = X # network's input\n",
    "        for l in range(1, len(self.layers)):\n",
    "            Z = activation @ self.layers[l].W + self.layers[l].b\n",
    "            activation = self.layers[l].g(Z)\n",
    "        return activation # network's output (Ypred)\n",
    "    \n",
    "    def feedforward(self, X):\n",
    "        ''' X.shape     == (n_examples, self.layers[0].input_size) '''\n",
    "        assert(X.shape[1] == self.layers[0].output_size) # self.layers[0].input_size == self.layers[0].output_size\n",
    "        self.layers[0].A = X # input\n",
    "        for l in range(1, len(self.layers)):\n",
    "            self.layers[l].feedforward(self.layers[l-1].A)\n",
    "        Ypred = self.layers[-1].A # output\n",
    "        return Ypred\n",
    "    \n",
    "    def backprop(self, X, Y, Ypred):\n",
    "        ''' X.shape     == (n_examples, self.layers[0].input_size)\n",
    "            Y.shape     == (n_examples, self.layers[-1].output_size)\n",
    "            Ypred.shape == (n_examples, self.layers[-1].output_size)\n",
    "            where Ypred is the result of feedforward(X)\n",
    "            \n",
    "            Note that only calling backprop doesn't actually update the network parameters\n",
    "        '''\n",
    "        assert(X.shape[0] == Y.shape[0])\n",
    "        assert(X.shape[1] == self.layers[0].output_size) # self.layers[0].input_size == self.layers[0].output_size\n",
    "        assert(Y.shape[1] == self.layers[-1].output_size)\n",
    "        assert(Ypred.shape == Y.shape)\n",
    "        \n",
    "        delta = self.J.deltaL(Y, Ypred, self.layers[-1].g, self.layers[-1].Z) # delta^L == [dJ/dZ^L]\n",
    "        self.layers[-1].backprop(dZ=delta)\n",
    "        for l in reversed(range(1, len(self.layers) - 1)):\n",
    "            # [dJ/dZ^l == dJ/dA^l . dA^l/dZ^l], note that dJ/dA^l is dJ/dX^{l+1}\n",
    "            delta = self.layers[l+1].dX * self.layers[l].g.derivative(self.layers[l].Z) # delta^l == [dJ/dZ^l]\n",
    "            self.layers[l].backprop(dZ=delta)\n",
    "        \n",
    "        # obs.: we don't backpropagate the input layer since we \n",
    "        #       manually set it's activation values A to the network's input X\n",
    "    \n",
    "    def __shuffle_X_Y(self, X, Y):\n",
    "        m = X.shape[0] # == Y.shape[0]\n",
    "        p = np.random.permutation(m)\n",
    "        return X[p], Y[p]\n",
    "    \n",
    "    def __get_batches(self, X, Y, batch_size, shuffled):\n",
    "        m = X.shape[0] # == Y.shape[0]\n",
    "        n_batches = m // batch_size\n",
    "        if shuffled:\n",
    "            X, Y = self.__shuffle_X_Y(X, Y)\n",
    "        return zip(np.array_split(X, n_batches), np.array_split(Y, n_batches))\n",
    "    \n",
    "    # test data\n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        ''' X_test.shape == (n_test_samples, self.layers[0].input_size)\n",
    "            Y_test.shape == (n_test_samples, self.layers[-1].output_size)\n",
    "        '''\n",
    "        assert(X_test.shape[0] == Y_test.shape[0])\n",
    "        assert(X_test.shape[1] == self.layers[0].output_size) # self.layers[0].input_size == self.layers[0].output_size\n",
    "        assert(Y_test.shape[1] == self.layers[-1].output_size)\n",
    "        \n",
    "        # loss/cost value for the training set\n",
    "        Ypred = self.predict(X_test) # same as self.feedforward(X_test) but doesn't change the 'cached' values\n",
    "        cost = self.J(Y_test, Ypred)\n",
    "        \n",
    "        # calculates the values not as one-hot encoded row vectors\n",
    "        target = np.argmax(Y_test, axis=1)\n",
    "        prediction = np.argmax(Ypred, axis=1)\n",
    "        accuracy = (prediction == target).mean()\n",
    "\n",
    "        return cost, accuracy\n",
    "    \n",
    "    # training and validation data\n",
    "    def train(self, X, Y, X_val, Y_val, n_epochs, batch_size, verbose=True):\n",
    "        ''' X.shape == (n_training_samples, self.layers[0].input_size)\n",
    "            Y.shape == (n_training_samples, self.layers[-1].output_size)\n",
    "            \n",
    "            X_val.shape == (n_validation_samples, self.layers[0].input_size)\n",
    "            Y_val.shape == (n_validation_samples, self.layers[-1].output_size)\n",
    "            \n",
    "            For each iteration we'll have:\n",
    "              n_examples = batch_size\n",
    "              batch_X.shape == (n_examples, self.layers[0].input_size)\n",
    "              batch_Y.shape == (n_examples, self.layers[-1].output_size)\n",
    "            Thus, each epoch has ceil(n_training_samples / batch_size) iterations\n",
    "            obs.: batch_X and batch_Y are rows of X and Y, and after each iteration (i.e. after going through\n",
    "                  each batch) we update our network parameters (weights and biases)\n",
    "            \n",
    "            If n_training_samples is not divisible by batch_size the last training batch will be smaller\n",
    "        '''\n",
    "        assert(X.shape[0] == Y.shape[0])\n",
    "        assert(X.shape[1] == self.layers[0].output_size) # self.layers[0].input_size == self.layers[0].output_size\n",
    "        assert(Y.shape[1] == self.layers[-1].output_size)\n",
    "        assert(X_val.shape[0] == Y_val.shape[0])\n",
    "        assert(X_val.shape[1] == self.layers[0].output_size) # self.layers[0].input_size == self.layers[0].output_size\n",
    "        assert(Y_val.shape[1] == self.layers[-1].output_size)\n",
    "        \n",
    "        n_training_samples = X.shape[0]\n",
    "        batches_per_epoch = int(np.ceil(n_training_samples / batch_size)) # equal to the number of iterations per epoch\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            if verbose:\n",
    "                start_time = time()\n",
    "                batch_number = 1\n",
    "                \n",
    "            for batch_X, batch_Y in self.__get_batches(X, Y, batch_size, shuffled=True):\n",
    "                # calculates the predicted target values for this batch (with the current network parameters)\n",
    "                batch_Ypred = self.feedforward(batch_X)\n",
    "                \n",
    "                # sets the values of dW and db, used to then update the network parameters\n",
    "                self.backprop(batch_X, batch_Y, batch_Ypred)\n",
    "                \n",
    "                # updates each layer's parameters (i.e. weights and biases) with some flavor of gradient descent\n",
    "                self.optimizer.update(self.layers)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"batch ({batch_number}/{batches_per_epoch})\", end='\\r')\n",
    "                    batch_number += 1\n",
    "            \n",
    "            # calculate the loss/cost value for this epoch\n",
    "            epoch_cost, epoch_accuracy = self.evaluate(X, Y) # training set\n",
    "            epoch_cost_val, epoch_accuracy_val = self.evaluate(X_val, Y_val) # validation set\n",
    "            self.history[\"loss\"].append(epoch_cost)\n",
    "            self.history[\"loss_val\"].append(epoch_cost_val)\n",
    "            self.history[\"acc\"].append(epoch_accuracy)\n",
    "            self.history[\"acc_val\"].append(epoch_accuracy_val)\n",
    "            if verbose:\n",
    "                print(f\"epoch ({epoch+1}/{n_epochs}) \"\n",
    "                      f\"loss: {epoch_cost:.4f}, loss_val: {epoch_cost_val:.4f} | \"\n",
    "                      f\"acc: {epoch_accuracy:.2f}, acc_val: {epoch_accuracy_val:.2f} | \"\n",
    "                      f\"Δt: {(time() - start_time):.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Perform Multinomial Logistic Regression (i.e., softmax regression)\n",
    "It is a generalization of Logistic Regression to the case where we want to handle multiple classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SPsGc8lu5uj7"
   },
   "source": [
    "## Neural Network\n",
    "\n",
    "Being $m$ the number of samples in a batch, from a layer $k-1$ to a layer $k$ we have:\n",
    "- Weights $\\mathbb{W}^{(k)} \\in \\mathbb{R}^{n_{k-1} \\times n_k}$\n",
    "- Biases $\\mathbf{b}^{(k)} \\in \\mathbb{R}^{n_k}$\n",
    "- Activations $\\mathbb{A}^{(k)} = g_k(\\mathbb{Z}^{(k)}) \\in \\mathbb{R}^{m \\times n_k}$, where $g_k(\\mathbb{Z}^{(k)})$ is the activation function of the $k^{\\text{th}}$ layer and $\\mathbb{Z}^{(k)} = \\mathbb{A}^{(k-1)} \\mathbb{W}^{(k)} + \\mathbf{b}^{(k)}$\n",
    "\n",
    "(Xavier initialization: [[1]](https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/))\n",
    "\n",
    "For the first layer, the activation is the input itself: $\\mathbb{A}^{(1)} = \\mathbb{X} \\in \\mathbb{R}^{m \\times n_1}$, where $n_1$ is the input size (3072)  \n",
    "For the middle layers ($2 \\leq k < L$), the activation function is the sigmoid: $\\mathbb{A}^{(k)} = g_k(\\mathbb{Z}^{(k)}) = sigmoid(\\mathbb{Z}^{(k)})$  \n",
    "For the last layer, we have the predicted value with softmax activation: $\\mathbb{A}^{(L)} = g_k(\\mathbb{Z}^{(L)}) = softmax(\\mathbb{Z}^{(L)}) \\in \\mathbb{R}^{m \\times n_L}$, where $n_L$ is the output size (10)  \n",
    "(i.e. the hypothesis function $a^{(L)} = h_{W, b}(x) = y_{\\text{pred}} \\approx y$)\n",
    "\n",
    "obs.: the number of layers $L$ comes from: $1$ input layer + $1$ output layer + $L-2$ hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
