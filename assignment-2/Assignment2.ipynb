{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libs and loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# plt.rcParams['figure.figsize'] = [6, 6]\n",
    "sns.set() # apply the seaborn defaults to plotted figures (e.g. theme, scaling, color palette), instead of matplotlib's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Computational Graph definition and implementation\n",
    "\n",
    "We will define a Computational Graph [[ref]](https://colah.github.io/posts/2015-08-Backprop/) where each node either represents a value (i.e. what will later be a **layer** in a neural network) or a computation (i.e. an operation like the **activation** or **cost** functions in a neural network).\n",
    "\n",
    "With it we can then represent both an artificial neural network, aswell as a softmax regressor (which performs Multinomial Logistic Regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from time import time\n",
    "\n",
    "RANDOM_SEED = 886"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction:\n",
    "    ''' An ActivationFunction is applied to Z to get the output A, \n",
    "        but its derivative expects the value A, not Z (!):\n",
    "\n",
    "        A == __call__(Z) and derivative(A) == derivative(__call__(Z)), \n",
    "        calling derivative(Z) will often yield WRONG results\n",
    "    '''\n",
    "    def __call__(self, Z):\n",
    "        ''' Z.shape=(n_examples, layer_output_size) '''\n",
    "        raise NotImplementedError    \n",
    "    def derivative(self, A):\n",
    "        ''' A.shape=(n_examples, layer_output_size) '''\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Sigmoid(ActivationFunction):\n",
    "    def __call__(self, Z, clip=500):\n",
    "        Z = np.clip(Z, -clip, clip) # numerical stability\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    def derivative(self, A):\n",
    "        return A * (1 - A) # Sigmoid(Z) * (1 - Sigmoid(Z))\n",
    "\n",
    "class ReLU(ActivationFunction):\n",
    "    def __call__(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "    def derivative(self, A):\n",
    "        return np.where(A > 0, 1, 0)\n",
    "\n",
    "class SoftMax(ActivationFunction):\n",
    "    def __call__(self, Z):\n",
    "        exp = np.exp(Z - Z.max(axis=1, keepdims=True))\n",
    "        return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "    def derivative(self, A, eps=1e-9):\n",
    "        Ypred = A+eps\n",
    "        return Ypred * (1 - Ypred) # SoftMax(Z) * (1 - SoftMax(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostFunction:\n",
    "    ''' A CostFunction is applied to Y (the target values) and Ypred (the predicted values) to get a scalar output\n",
    "        Its derivative w.r.t. Ypred also expects Y and Ypred, but returns a tensor of shape (n_examples, last_layer_output_size)\n",
    "        \n",
    "        obs.: Ypred is the last layer's activation values: last_layer.A == last_layer.g(last.layer.Z), \n",
    "              i.e. last_layer is the output layer of the network\n",
    "    '''\n",
    "    def __call__(self, Y, Ypred):\n",
    "        ''' Y.shape == Ypred.shape == (n_examples, last_layer_output_size) '''\n",
    "        raise NotImplementedError # [J(Y, Ypred) == J(Y, A^L)]\n",
    "    def derivative(self, Y, Ypred):\n",
    "        ''' Y.shape == Ypred.shape == (n_examples, last_layer_output_size) '''\n",
    "        raise NotImplementedError # [dJ/dYpred == dJ/dA^L]\n",
    "    def deltaL(self, Y, Ypred, activation_function):\n",
    "        ''' Y.shape == Ypred.shape == (n_examples, last_layer_output_size) \n",
    "\n",
    "            activation_function should be the last layer's g function, thus Ypred == activation_function(Z)\n",
    "            and the value returned is the delta for the output layer of the network (delta^L == dJ/dZ^L)\n",
    "        '''\n",
    "        return self.derivative(Y, Ypred) * activation_function.derivative(A=Ypred) # [dJ/dZ^L == dJ/dYpred . dYpred/dZ^L]\n",
    "        # obs.: Ypred == A^L == g(Z^L), thus dYpred/dZ^L == dA^L/dZ^L == g'(Z^L)\n",
    "        #       [dJ/dZ^L == dJ/dYpred . dYpred/dZ^L == dJ/dA^L . dA^L/dZ^L]\n",
    "\n",
    "class CrossEntropy(CostFunction):\n",
    "    def __call__(self, Y, Ypred, eps=1e-9):\n",
    "        return np.mean( -(Y * np.log(Ypred+eps)).sum(axis=1) )\n",
    "    def derivative(self, Y, Ypred, eps=1e-9):\n",
    "        m = Ypred.shape[0]\n",
    "        return - (Y / (Ypred+eps)) / m\n",
    "    def deltaL(self, Y, Ypred, activation_function):\n",
    "        if isinstance(activation_function, SoftMax):\n",
    "            m = Ypred.shape[0]\n",
    "            # numerically stable\n",
    "            return (Ypred - Y) / m # (SoftMax(Z) - Y) / m\n",
    "        else:\n",
    "            return super().deltaL(Y, Ypred, activation_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    ''' The optimizer's optimization policy should be implemented on its update(layers) function '''\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "    def update(self, layers):\n",
    "        ''' Updates the parameters (i.e. weights and biases) for each layer in the layers list '''\n",
    "        raise NotImplementedError\n",
    "    def init(self, layers):\n",
    "        ''' Performs initializations that require knowledge about the layers list '''\n",
    "        pass\n",
    "\n",
    "class GradientDescent(Optimizer):\n",
    "    def __init__(self, learning_rate):\n",
    "        super().__init__(learning_rate)\n",
    "    def update(self, layers):\n",
    "        for layer in layers[1:]:\n",
    "            layer.W -= self.learning_rate * layer.dW\n",
    "            layer.b -= self.learning_rate * layer.db\n",
    "\n",
    "class Momentum(Optimizer):\n",
    "    def __init__(self, learning_rate, momentum=0.9):\n",
    "        super().__init__(learning_rate)\n",
    "        self.momentum = momentum\n",
    "        self.last_dW = None # stores the last value of dW for each layer\n",
    "        self.last_db = None # stores the last value of db for each layer\n",
    "    def update(self, layers):\n",
    "        for l in range(1, len(layers)):\n",
    "            layer = layers[l]\n",
    "            self.last_dW[l] = (self.momentum * self.last_dW[l]) + (self.learning_rate * layer.dW)\n",
    "            self.last_db[l] = (self.momentum * self.last_db[l]) + (self.learning_rate * layer.db)\n",
    "            layer.W -= self.last_dW[l]\n",
    "            layer.b -= self.last_db[l]\n",
    "    def init(self, layers):\n",
    "        self.last_dW = [0 for _ in len(layers)]\n",
    "        self.last_db = [0 for _ in len(layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    ''' A.shape == (n_examples, output_size)\n",
    "        Z.shape == (n_examples, output_size)\n",
    "        W.shape == (input_size, output_size)\n",
    "        b.shape == (output_size, )\n",
    "        X.shape == (n_examples, input_size)\n",
    "        obs.:\n",
    "            input_size == prev_layer.output_size\n",
    "            output_size == next_layer.input_size\n",
    "    '''\n",
    "    def __init__(self, output_size, activation_function, name=\"\"):\n",
    "        if activation_function != None:\n",
    "            # obs.: the activation_function should only be None for the network's input layer\n",
    "            assert(isinstance(activation_function, ActivationFunction)), \"Invalid object type for activation_function\"\n",
    "        \n",
    "        self.name = name\n",
    "        \n",
    "        self.input_size = None\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # activation function\n",
    "        self.g = activation_function # g_prime == activation_function.derivative\n",
    "        \n",
    "        # activation values\n",
    "        self.A = None # self.A == self.g(self.Z)\n",
    "        self.Z = None # prev_layer.A @ self.W + self.b\n",
    "        \n",
    "        # output value of the previous layer\n",
    "        self.X = None # == prev_layer.A\n",
    "        self.dX = None\n",
    "        \n",
    "        # parameters (weights and biases)\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def init(self, input_size, weight_initialization):\n",
    "        ''' Sets the layer's input_size and initializes its weights and biases '''\n",
    "        self.input_size = input_size\n",
    "        if weight_initialization == 'xavier':\n",
    "            stddev = np.sqrt(1 / self.input_size)\n",
    "            self.W = stddev * np.random.randn(self.input_size, self.output_size)\n",
    "            self.b = np.random.randn(self.output_size, )\n",
    "        elif weight_initialization == 'xavier_avg':\n",
    "            stddev = np.sqrt(2 / (self.input_size + self.output_size))\n",
    "            self.W = stddev * np.random.randn(self.input_size, self.output_size)\n",
    "            self.b = np.random.randn(self.output_size, )\n",
    "        elif weight_initialization == 'rand_-1_to_1':\n",
    "            self.W = 2 * np.random.randn(self.input_size, self.output_size) - 1\n",
    "            self.b = 2 * np.random.randn(self.output_size, ) - 1\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid weight_initialization value: '{weight_initialization}'\")\n",
    "    \n",
    "    @property\n",
    "    def params_count(self):\n",
    "        count = 0\n",
    "        if self.W is not None:\n",
    "            count += self.W.size\n",
    "        if self.b is not None:\n",
    "            count += self.b.size\n",
    "        return count\n",
    "    \n",
    "    # receives the activation values of the previous layer (i.e. this layer's input)\n",
    "    # returns the activation values of the current layer (i.e. next layer's input)\n",
    "    def feedforward(self, X):\n",
    "        ''' X.shape == (n_examples, self.input_size) '''\n",
    "        assert(X.shape[1] == self.input_size)\n",
    "        self.X = X\n",
    "        # (n_examples, output_size) = (n_examples, input_size) @ (input_size, output_size) + (output_size, )\n",
    "        self.Z = self.X @ self.W + self.b\n",
    "        self.A = self.g(self.Z)\n",
    "        return self.A\n",
    "    \n",
    "    # receives the derivative of the cost function w.r.t. the Z value of the current layer [dJ/dZ = dJ/dA . dA/dZ]\n",
    "    # returns the derivative of the cost function w.r.t. the A value of the previous layer [dJ/dX = dJ/dZ . dZ/dX]\n",
    "    # obs.: the A value of the previous layer is this layer's input value X\n",
    "    def backprop(self, dZ):\n",
    "        ''' dZ.shape == (n_examples, self.output_size)\n",
    "        \n",
    "            Note that only calling backprop doesn't actually update the layer parameters\n",
    "        '''\n",
    "        assert(dZ.shape[1] == self.output_size)\n",
    "        # (input_size, output_size) = (input_size, n_examples)  @ (n_examples, output_size)\n",
    "        # (output_size, )           = (n_examples, output_size).sum(axis=0)\n",
    "        # (n_examples, input_size)  = (n_examples, output_size) @ (output_size, input_size), input_size==prev_layer.output_size\n",
    "        self.dW = (self.X).T @ dZ              # [dJ/dW = dJ/dZ . dZ/dX]\n",
    "        self.db = dZ.sum(axis=0)               # [dJ/db = dJ/dZ . dZ/db]\n",
    "        self.dX = dZ @ (self.W).T              # [dJ/dX = dJ/dZ . dZ/dX]\n",
    "        return self.dX\n",
    "        # note that dJ/dX is dJ/dA for the previous layer (since this layer's input X is the previous layer's A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputationalGraph:\n",
    "    def __init__(self, layers, cost_function, optimizer, weight_initialization='xavier', name=\"\"):\n",
    "        assert(isinstance(cost_function, CostFunction)), \"Invalid object type for cost_function\"\n",
    "        \n",
    "        self.name = name\n",
    "        \n",
    "        self.J = cost_function # cost_function(Y, Ypred)\n",
    "        # obs.: cost_function.derivative is the derivative of J w.r.t. the last layer's activation values [dJ/dYpred]\n",
    "        #       Ypred == self.layers[-1].A, thus [dJ/dYpred == dJ/dA^L]\n",
    "        #\n",
    "        #       cost_function.deltaL is the derivative of J w.r.t. the last layer's Z values [dJ/dZ^L]\n",
    "        #       Z^L == self.layers[-1].Z, thus [dJ/dZ^L == dJ/dA^L . dA^L/dZ^L == dJ/dYpred . dYpred/dZ^L]\n",
    "        \n",
    "        self.optimizer = optimizer # obs.: the learning rate is set on the optimizer object\n",
    "        \n",
    "        self.layers = []\n",
    "        # obs.: we don't call init for the input layer since we set it's activation values manually\n",
    "        layers[0].input_size = layers[0].output_size\n",
    "        self.layers.append(layers[0]) # input layer\n",
    "        for l in range(1, len(layers)):            \n",
    "            # sets the layer's input_size as the last layer's output_size and initializes its weights and biases\n",
    "            layers[l].init(input_size=layers[l-1].output_size, weight_initialization=weight_initialization)            \n",
    "            self.layers.append(layers[l]) # adds the initialized layer to the network\n",
    "        \n",
    "        self.optimizer.init(self.layers) # performs initializations that require knowledge about the layers list\n",
    "        \n",
    "        self.history = { \"loss\": [], \"loss_val\": [], \"acc\": [], \"acc_val\": [], \"lr\": self.optimizer.learning_rate }\n",
    "    \n",
    "    def __str__(self):\n",
    "        to_str = self.name + \"\\n\"\n",
    "        to_str += \"[name?] Layer (input_size, output_size) params_count\\n\"\n",
    "        to_str += \"----------------------------------------------------\\n\"\n",
    "        for l in range(len(self.layers)):\n",
    "            layer = self.layers[l]\n",
    "            to_str += f\"{'' if layer.name == '' else '['+layer.name+'] '}Layer_{l} \"\n",
    "            to_str += f\"({layer.input_size}, {layer.output_size}) {layer.params_count}\\n\"\n",
    "        return to_str\n",
    "    \n",
    "    # note that we use zero-based indexing here, so\n",
    "    # the 1st layer is self.layers[0] and the last is self.layers[len(self.layers) - 1]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        ''' X.shape == (n_examples, self.layers[0].input_size) '''\n",
    "        assert(X.shape[1] == self.layers[0].output_size) # self.layers[0].input_size == self.layers[0].output_size\n",
    "        activation = X # network's input\n",
    "        for l in range(1, len(self.layers)):\n",
    "            Z = activation @ self.layers[l].W + self.layers[l].b\n",
    "            activation = self.layers[l].g(Z)\n",
    "        return activation # network's output (Ypred)\n",
    "    \n",
    "    def feedforward(self, X):\n",
    "        ''' X.shape     == (n_examples, self.layers[0].input_size) '''\n",
    "        assert(X.shape[1] == self.layers[0].output_size) # self.layers[0].input_size == self.layers[0].output_size\n",
    "        self.layers[0].A = X # input\n",
    "        for l in range(1, len(self.layers)):\n",
    "            self.layers[l].feedforward(self.layers[l-1].A)\n",
    "        Ypred = self.layers[-1].A # output\n",
    "        return Ypred\n",
    "    \n",
    "    def backprop(self, X, Y, Ypred):\n",
    "        ''' X.shape     == (n_examples, self.layers[0].input_size)\n",
    "            Y.shape     == (n_examples, self.layers[-1].output_size)\n",
    "            Ypred.shape == (n_examples, self.layers[-1].output_size)\n",
    "            where Ypred is the result of feedforward(X)\n",
    "            \n",
    "            Note that only calling backprop doesn't actually update the network parameters\n",
    "        '''\n",
    "        assert(X.shape[0] == Y.shape[0])\n",
    "        assert(X.shape[1] == self.layers[0].output_size) # self.layers[0].input_size == self.layers[0].output_size\n",
    "        assert(Y.shape[1] == self.layers[-1].output_size)\n",
    "        assert(Ypred.shape == Y.shape)\n",
    "        \n",
    "        delta = self.J.deltaL(Y, Ypred, self.layers[-1].g) # delta^L == [dJ/dZ^L]\n",
    "        self.layers[-1].backprop(dZ=delta)\n",
    "        for l in reversed(range(1, len(self.layers) - 1)):\n",
    "            # [dJ/dZ^l == dJ/dA^l . dA^l/dZ^l], note that dJ/dA^l is dJ/dX^{l+1}\n",
    "            delta = self.layers[l+1].dX * self.layers[l].g.derivative(self.layers[l].A) # delta^l == [dJ/dZ^l]\n",
    "            self.layers[l].backprop(dZ=delta)\n",
    "        \n",
    "        # obs.: we don't backpropagate the input layer since we \n",
    "        #       manually set it's activation values A to the network's input X\n",
    "    \n",
    "    def __shuffle_X_Y(self, X, Y):\n",
    "        m = X.shape[0] # == Y.shape[0]\n",
    "        p = np.random.permutation(m)\n",
    "        return X[p], Y[p]\n",
    "    \n",
    "    def __get_batches(self, X, Y, batch_size, shuffled):\n",
    "        m = X.shape[0] # == Y.shape[0]\n",
    "        n_batches = m // batch_size\n",
    "        if shuffled:\n",
    "            X, Y = self.__shuffle_X_Y(X, Y)\n",
    "        return zip(np.array_split(X, n_batches), np.array_split(Y, n_batches))\n",
    "    \n",
    "    # test data\n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        ''' X_test.shape == (n_test_samples, self.layers[0].input_size)\n",
    "            Y_test.shape == (n_test_samples, self.layers[-1].output_size)\n",
    "        '''\n",
    "        assert(X_test.shape[0] == Y_test.shape[0])\n",
    "        assert(X_test.shape[1] == self.layers[0].output_size) # self.layers[0].input_size == self.layers[0].output_size\n",
    "        assert(Y_test.shape[1] == self.layers[-1].output_size)\n",
    "        \n",
    "        # loss/cost value for the training set\n",
    "        Ypred = self.predict(X_test) # same as self.feedforward(X_test) but doesn't change the 'cached' values\n",
    "        cost = self.J(Y_test, Ypred)\n",
    "        \n",
    "        # calculates the values not as one-hot encoded row vectors\n",
    "        target = np.argmax(Y_test, axis=1)\n",
    "        prediction = np.argmax(Ypred, axis=1)\n",
    "        accuracy = (prediction == target).mean()\n",
    "\n",
    "        return cost, accuracy\n",
    "    \n",
    "    # training and validation data\n",
    "    def train(self, X, Y, X_val, Y_val, n_epochs, batch_size, verbose=True):\n",
    "        ''' X.shape == (n_training_samples, self.layers[0].input_size)\n",
    "            Y.shape == (n_training_samples, self.layers[-1].output_size)\n",
    "            \n",
    "            X_val.shape == (n_validation_samples, self.layers[0].input_size)\n",
    "            Y_val.shape == (n_validation_samples, self.layers[-1].output_size)\n",
    "            \n",
    "            For each iteration we'll have:\n",
    "              n_examples = batch_size\n",
    "              batch_X.shape == (n_examples, self.layers[0].input_size)\n",
    "              batch_Y.shape == (n_examples, self.layers[-1].output_size)\n",
    "            Thus, each epoch has ceil(n_training_samples / batch_size) iterations\n",
    "            obs.: batch_X and batch_Y are rows of X and Y, and after each iteration (i.e. after going through\n",
    "                  each batch) we update our network parameters (weights and biases)\n",
    "            \n",
    "            If n_training_samples is not divisible by batch_size the last training batch will be smaller\n",
    "        '''\n",
    "        assert(X.shape[0] == Y.shape[0])\n",
    "        assert(X.shape[1] == self.layers[0].output_size) # self.layers[0].input_size == self.layers[0].output_size\n",
    "        assert(Y.shape[1] == self.layers[-1].output_size)\n",
    "        assert(X_val.shape[0] == Y_val.shape[0])\n",
    "        assert(X_val.shape[1] == self.layers[0].output_size) # self.layers[0].input_size == self.layers[0].output_size\n",
    "        assert(Y_val.shape[1] == self.layers[-1].output_size)\n",
    "        \n",
    "        n_training_samples = X.shape[0]\n",
    "        batches_per_epoch = int(np.ceil(n_training_samples / batch_size)) # equal to the number of iterations per epoch\n",
    "        \n",
    "        if verbose:\n",
    "            self.history = { \"loss\": [], \"loss_val\": [], \"acc\": [], \"acc_val\": [], \"lr\": self.optimizer.learning_rate }\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            if verbose:\n",
    "                start_time = time()\n",
    "                batch_number = 1\n",
    "                \n",
    "            for batch_X, batch_Y in self.__get_batches(X, Y, batch_size, shuffled=True):\n",
    "                # calculates the predicted target values for this batch (with the current network parameters)\n",
    "                batch_Ypred = self.feedforward(batch_X)\n",
    "                \n",
    "                # sets the values of dW and db, used to then update the network parameters\n",
    "                self.backprop(batch_X, batch_Y, batch_Ypred)\n",
    "                \n",
    "                # updates each layer's parameters (i.e. weights and biases) with some flavor of gradient descent\n",
    "                self.optimizer.update(self.layers)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"batch ({batch_number}/{batches_per_epoch})\", end='\\r')\n",
    "                    batch_number += 1\n",
    "            \n",
    "            # calculate the loss/cost value for this epoch\n",
    "            epoch_cost, epoch_accuracy = self.evaluate(X, Y) # training set\n",
    "            epoch_cost_val, epoch_accuracy_val = self.evaluate(X_val, Y_val) # validation set\n",
    "            self.history[\"loss\"].append(epoch_cost)\n",
    "            self.history[\"loss_val\"].append(epoch_cost_val)\n",
    "            self.history[\"acc\"].append(epoch_accuracy)\n",
    "            self.history[\"acc_val\"].append(epoch_accuracy_val)\n",
    "            if verbose:\n",
    "                print(f\"epoch ({epoch+1}/{n_epochs}) \"\n",
    "                      f\"loss: {epoch_cost:.4f}, loss_val: {epoch_cost_val:.4f} | \"\n",
    "                      f\"acc: {epoch_accuracy:.4f}, acc_val: {epoch_accuracy_val:.4f} | \"\n",
    "                      f\"Δt: {(time() - start_time):.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** you should have the .npz files in the directory listed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# confirm that you have test.npz, train.npz and val.npz\n",
    "!ls mini_cinic10/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = os.path.join(\"mini_cinic10\")\n",
    "\n",
    "train_data = np.load(os.path.join(PATH_TO_DATA, \"train.npz\"))\n",
    "val_data   = np.load(os.path.join(PATH_TO_DATA, \"val.npz\"))\n",
    "# test_data  = np.load(os.path.join(PATH_TO_DATA, \"test.npz\")) # assume this doesn't exist (we'll come back to it at the end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = train_data['xs'], train_data['ys']\n",
    "print(xs.shape, xs.dtype) # 3072 = 3 * 1024 = 3 * (32 * 32)\n",
    "print(ys.shape, ys.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xs_val, ys_val = val_data['xs'], val_data['ys']\n",
    "print(xs_val.shape, xs_val.dtype) # 3072 = 3 * 1024 = 3 * (32 * 32)\n",
    "print(ys_val.shape, ys_val.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = IMG_HEIGHT = 32\n",
    "IMG_SHAPE = (IMG_HEIGHT, IMG_WIDTH, 3) # RGB images\n",
    "IMG_FLAT_SHAPE = (IMG_HEIGHT*IMG_WIDTH*3, )\n",
    "\n",
    "# classes_and_names.csv\n",
    "CLASS_NAME = {\n",
    "    0: \"airplane\",\n",
    "    1: \"automobile\",\n",
    "    2: \"bird\",\n",
    "    3: \"cat\",\n",
    "    4: \"deer\",\n",
    "    5: \"dog\",\n",
    "    6: \"frog\",\n",
    "    7: \"horse\",\n",
    "    8: \"ship\",\n",
    "    9: \"truck\",\n",
    "}\n",
    "CLASS_COUNT = len(CLASS_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unflatten(flat_img, img_shape=IMG_SHAPE):\n",
    "    ''' Converts a flattened image back into a 3-layer RGB matrix representation '''\n",
    "    return flat_img.reshape(img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img_index, xs, ys, figsize=None):\n",
    "    ''' Shows the image represented by the img_index-th row of xs '''\n",
    "    if figsize != None:\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "    plt.imshow(unflatten(xs[img_index]))\n",
    "    plt.title(f'idx #{img_index} ({CLASS_NAME[ys[img_index]]})')\n",
    "    plt.axis(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(0, xs, ys, [1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Normalize xs and one-hot encode ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"xs: mean={xs.mean():>8.4f}, stddev={xs.std():>7.4f}\")\n",
    "\n",
    "# NOTE that we must use stats from train data to normalize the val and test sets aswell\n",
    "mean, std = xs.mean(), xs.std()\n",
    "X = (xs - mean) / std\n",
    "\n",
    "print(f\"X:  mean={X.mean():>8.4f}, stddev={X.std():>7.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = (xs_val - mean) / std\n",
    "print(f\"xs_val: mean={xs_val.mean():>8.4f}, stddev={xs_val.std():>7.4f}\")\n",
    "print(f\"X_val:  mean={X_val.mean():>8.4f}, stddev={X_val.std():>7.4f}\") # mean should be close to 0 and stddev close to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode(ys):\n",
    "    n_examples, *_ = ys.shape\n",
    "    onehot = np.zeros(shape=(n_examples, CLASS_COUNT))\n",
    "    onehot[np.arange(n_examples), ys] = 1\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = onehot_encode(ys)\n",
    "print(ys.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_val = onehot_encode(ys_val)\n",
    "print(ys_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Add helper functions for running Computational Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, n_epochs, batch_size=None, title_prefix=\"\"):\n",
    "    plot_title = \"\" if title_prefix == \"\" else (title_prefix + \"\\n\")\n",
    "    plot_title += \"Train history ({n_epochs} epochs with α = {learning_rate}\".format(\n",
    "        n_epochs=n_epochs, \n",
    "        learning_rate=history['lr'])\n",
    "    plot_title += \")\" if batch_size == None else \" and {batch_size} batch size)\".format(\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    # leave only \"loss\" and \"loss_val\" for plotting\n",
    "    loss_data = pd.DataFrame({k: v for k, v in history.items() if (k == \"loss\" or k == \"loss_val\")})\n",
    "    loss_data.index += 1 # start counting the epochs at 1, not 0\n",
    "\n",
    "    lineplot = sns.lineplot(data=loss_data)\n",
    "    lineplot.set_title(plot_title)\n",
    "    lineplot.set_xlabel(\"epoch\")\n",
    "    lineplot.set_ylabel(\"Loss J(θ)\")\n",
    "    plt.show()\n",
    "\n",
    "    # leave only \"acc\" and \"acc_val\" for plotting\n",
    "    acc_data = pd.DataFrame({k: v for k, v in history.items() if (k == \"acc\" or k == \"acc_val\")})\n",
    "    acc_data.index += 1 # start counting the epochs at 1, not 0\n",
    "    lineplot = sns.lineplot(data=acc_data)\n",
    "    lineplot.set_title(plot_title)\n",
    "    lineplot.set_xlabel(\"epoch\")\n",
    "    lineplot.set_ylabel(\"Accuracy\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_computational_graph(computational_graph, X, Y, X_val, Y_val, n_epochs, batch_size, verbose=True, plot=True):\n",
    "    start = time()\n",
    "    print(\"Starting to train...\")\n",
    "    computational_graph.train(\n",
    "        X, Y,\n",
    "        X_val, Y_val,\n",
    "        n_epochs,\n",
    "        batch_size,\n",
    "        verbose\n",
    "    )\n",
    "    end = time()\n",
    "    print(f\"\\nDone.\\nTraining took {(end - start):.2f}s\")\n",
    "    if plot:\n",
    "        plot_history(computational_graph.history, n_epochs, batch_size, title_prefix=computational_graph.name)\n",
    "    return computational_graph.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 24\n",
    "batch_size = 80\n",
    "\n",
    "nn_lr = 0.2\n",
    "regressor_lr = 5e-3\n",
    "\n",
    "hidden_layer_size = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perform Multinomial Logistic Regression (i.e. softmax regression)\n",
    "It is a generalization of Logistic Regression to the case where we want to handle multiple classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_regressor = ComputationalGraph(\n",
    "    name=\"Softmax Regressor\",\n",
    "    cost_function=CrossEntropy(),\n",
    "    optimizer=GradientDescent(regressor_lr),\n",
    "    weight_initialization='xavier',\n",
    "    layers=[\n",
    "        Layer(IMG_FLAT_SHAPE[0], None, name=\"Input\"), Layer(CLASS_COUNT, SoftMax(), name=\"Output\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(softmax_regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "softmax_regressor_history = train_computational_graph(\n",
    "    softmax_regressor,\n",
    "    X=X, X_val=X_val,\n",
    "    Y=Y, Y_val=Y_val,\n",
    "    n_epochs=n_epochs,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SPsGc8lu5uj7"
   },
   "source": [
    "## 3.  Move on to Neural Networks, using one or two hidden layers\n",
    "\n",
    "Being $m$ the number of samples in a batch, from a layer $k-1$ to a layer $k$ we have:\n",
    "- Weights $\\mathbb{W}^{(k)} \\in \\mathbb{R}^{n_{k-1} \\times n_k}$\n",
    "- Biases $\\mathbf{b}^{(k)} \\in \\mathbb{R}^{n_k}$\n",
    "- Activations $\\mathbb{A}^{(k)} = g_k(\\mathbb{Z}^{(k)}) \\in \\mathbb{R}^{m \\times n_k}$, where $g_k(\\mathbb{Z}^{(k)})$ is the activation function of the $k^{\\text{th}}$ layer and $\\mathbb{Z}^{(k)} = \\mathbb{A}^{(k-1)} \\mathbb{W}^{(k)} + \\mathbf{b}^{(k)}$\n",
    "\n",
    "(Xavier initialization: [[1]](https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/))\n",
    "\n",
    "For the first layer, the activation is the input itself: $\\mathbb{A}^{(1)} = \\mathbb{X} \\in \\mathbb{R}^{m \\times n_1}$, where $n_1$ is the input size (3072)  \n",
    "For the middle layers ($2 \\leq k < L$), the activation function is the sigmoid: $\\mathbb{A}^{(k)} = g_k(\\mathbb{Z}^{(k)}) = sigmoid(\\mathbb{Z}^{(k)})$  \n",
    "For the last layer, we have the predicted value with softmax activation: $\\mathbb{A}^{(L)} = g_k(\\mathbb{Z}^{(L)}) = softmax(\\mathbb{Z}^{(L)}) \\in \\mathbb{R}^{m \\times n_L}$, where $n_L$ is the output size (10)  \n",
    "(i.e. the hypothesis function $a^{(L)} = h_{W, b}(x) = y_{\\text{pred}} \\approx y$)\n",
    "\n",
    "obs.: the number of layers $L$ comes from: $1$ input layer + $1$ output layer + $L-2$ hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = ComputationalGraph(\n",
    "    name=\"Neural Network\",\n",
    "    cost_function=CrossEntropy(),\n",
    "    optimizer=GradientDescent(nn_lr),\n",
    "    weight_initialization='xavier',\n",
    "    layers=[\n",
    "        Layer(IMG_FLAT_SHAPE[0], None, name=\"Input\"), \n",
    "        Layer(hidden_layer_size, Sigmoid(), name=\"Hidden\"), \n",
    "        Layer(CLASS_COUNT, SoftMax(), name=\"Output\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_history = train_computational_graph(\n",
    "    nn,\n",
    "    X=X, X_val=X_val,\n",
    "    Y=Y, Y_val=Y_val,\n",
    "    n_epochs=n_epochs,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare results with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Dense, Input, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "#callbacks to be used during training\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_keras_history(history, n_epochs, learning_rate, batch_size=None, title_prefix=\"\"):\n",
    "    plot_title = \"Keras' \" + (\"\" if title_prefix == \"\" else title_prefix) + \"\\n\"\n",
    "    plot_title += \"Train history ({n_epochs} epochs with α = {learning_rate}\".format(\n",
    "        n_epochs=n_epochs, \n",
    "        learning_rate=learning_rate)\n",
    "    plot_title += \")\" if batch_size == None else \" and {batch_size} batch size)\".format(\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    # leave only \"loss\" and \"val_loss\" for plotting\n",
    "    loss_data = pd.DataFrame({k: v for k, v in history.items() if (k == \"loss\" or k == \"val_loss\")})\n",
    "    loss_data.index += 1 # start counting the epochs at 1, not 0\n",
    "\n",
    "    lineplot = sns.lineplot(data=loss_data)\n",
    "    lineplot.set_title(plot_title)\n",
    "    lineplot.set_xlabel(\"epoch\")\n",
    "    lineplot.set_ylabel(\"Loss J(θ)\")\n",
    "    plt.show()\n",
    "\n",
    "    # leave only \"accuracy\" and \"val_accuracy\" for plotting\n",
    "    acc_data = pd.DataFrame({k: v for k, v in history.items() if (k == \"accuracy\" or k == \"val_accuracy\")})\n",
    "    acc_data.index += 1 # start counting the epochs at 1, not 0\n",
    "    lineplot = sns.lineplot(data=acc_data)\n",
    "    lineplot.set_title(plot_title)\n",
    "    lineplot.set_xlabel(\"epoch\")\n",
    "    lineplot.set_ylabel(\"Accuracy\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Compare softmax regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__input  = Input(shape=IMG_FLAT_SHAPE)\n",
    "__output = Dense(CLASS_COUNT, activation='softmax')(__input)\n",
    "\n",
    "keras_softmax_regressor = Model(inputs=__input, outputs=__output)\n",
    "keras_softmax_regressor.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=SGD(learning_rate=regressor_lr, momentum=0.0),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_softmax_regressor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keras_softmax_regressor_history = keras_softmax_regressor.fit(\n",
    "            x=X, y=Y,\n",
    "            validation_data=(X_val, Y_val),\n",
    "            epochs=n_epochs,\n",
    "            batch_size=batch_size\n",
    "          ).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_keras_history(keras_softmax_regressor_history, n_epochs, regressor_lr, batch_size, \"Softmax Regressor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Compare neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__input  = Input(shape=IMG_FLAT_SHAPE) # 3072\n",
    "__hidden = Dense(hidden_layer_size, activation='sigmoid')(__input)\n",
    "__output = Dense(CLASS_COUNT, activation='softmax')(__hidden) # 10\n",
    "\n",
    "keras_nn = Model(inputs=__input, outputs=__output)\n",
    "keras_nn.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=SGD(learning_rate=nn_lr, momentum=0.0),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_nn_history = keras_nn.fit(\n",
    "            x=X, y=Y,\n",
    "            validation_data=(X_val, Y_val),\n",
    "            epochs=n_epochs,\n",
    "            batch_size=batch_size\n",
    "          ).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_keras_history(keras_nn_history, n_epochs, nn_lr, batch_size, \"Neural Network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implement two update rules used to optimize Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
